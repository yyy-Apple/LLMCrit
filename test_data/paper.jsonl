{"introduction": "Accurate and rigorous uncertainty estimation is key for reliable machine learning models in safety-critical domains. It quantifies the confidence of machine learning models, thus allowing them to validate knowledgeable predictions corresponding to correct/wrong predictions, flag predictions on unknown input domains corresponding to anomaly or Out-of-Distribution detection, or detect natural shifts of the data facilitating real-time model maintenance (Filos et al., 2019;Malinin et al., 2021;Ovadia et al., 2019). Specifically, a reliable model can handle all these failure modes with high-quality estimates of aleatoric and epistemic uncertainty (Gal, 2016). These two levels of uncertainty allow a model to account for both irreducible data uncertainty (e.g. a fair dice's chance of 1/6 for each face) and uncertainty due to the lack of knowledge about unseen data (e.g. input features differing significantly from training data or a covariate shift) respectively. Aleatoric and epistemic uncertainty levels can eventually be combined into an overall predictive uncertainty (Gal, 2016) Traditional neural networks are not readily applicable in safety-critical domains as they show overconfident prediction, in particular on data that is different from training data (Guo et al., 2017;Lakshminarayanan et al., 2017). To mitigate this problem, an important model family for uncertainty estimation directly predicts the parameters of a conjugate prior distribution on the predicted target probability distribution, thus accounting for the different levels of uncertainty. These models are efficient as they only require a single forward pass for target and uncertainty prediction. Most of those models focus on classification and thus predict parameters of a Dirichlet distribution (Bilo\u0161 et Zhao et al., 2020b). However, only two works (Amini et al., 2020;Malinin et al., 2020a) have focused on regression by learning parameters of a Normal Inverse-Gamma (NIG) distribution as conjugate prior. Hence, all these models are limited to a single task (e.g. either classification or regression). Some approaches even require out-of-distribution (OOD) data at training time (Malinin & Gales, 2018;2019) which is an unrealistic assumption in many real-world applications where anomalies are a priori diverse, rare or unknown.\nOur contribution. We propose Natural Posterior Network (NatPN) as a new approach parametrizing conjugate prior distributions for versatile uncertainty estimation. NatPN is motivated from both the theoretical and practical perspective. (1) NatPN can estimate predictive uncertainty for any task described by the general group of exponential family distributions contrary to existing approaches from this family of models. Notably, this encompasses very common tasks such as classification, regression and count prediction which can be described with Categorical, Normal and Poisson distributions, respectively. (2) In theory, NatPN is based on a new unified exponential family framework which performs an input-dependent Bayesian update. For every input, it predicts the parameters of the posterior over the target exponential family distribution. We show that this Bayesian update is guaranteed to predict high uncertainty far from training data. (3) In practice, NatPN requires no OOD data for training, only adds a single normalizing flow density to the last predictor layer and provides fast uncertainty estimation in a single forward pass. Our extensive experiments showcase the high performances of NatPN for various criteria (accuracy, calibration, OOD and shift detection) and tasks (classification, regression and count prediction). We illustrate the accurate aleatoric and predictive uncertainty predictions of NatPN on two toy examples for classification and regression in Fig. 1. None of the conjugate prior related works have similar theoretical and practical properties. However, it is unclear what new insights this approach provides compared to existing methods."}
{"introduction": "Reinforcement learning (RL) has been successfully applied to many challenging domains including games (Mnih et al., 2015;2016) and robot control (Schulman et al., 2015;Fujimoto et al., 2018;Haarnoja et al., 2018). Advanced RL methods often employ policy regularization motivated by, e.g., boosting exploration (Haarnoja et al., 2018) or safe policy improvement (Schulman et al., 2015). While Shannon entropy is often used as a policy regularizer (Ziebart et al., 2008), Geist et al. (2019) recently proposed a theoretical foundation of regularized Markov decision processes (MDPs)-a framework that uses strongly convex functions as policy regularizers. Here, one crucial advantage is that an optimal policy is shown to uniquely exist, whereas multiple optimal policies may exist in the absence of policy regularization.\nMeanwhile, since RL requires a given or known reward function (which can often involve non-trivial reward engineering), Inverse Reinforcement Learning (IRL) (Russell, 1998;Ng et al., 2000)-the problem of acquiring a reward function that promotes expert-like behavior-is more generally adopted in practical scenarios like robotic manipulation (Finn et al., 2016b), autonomous driving (Sharifzadeh et al., 2016;Wu et al., 2020) and clinical motion analysis (Li et al., 2018). In these scenarios, defining a reward function beforehand is particularly challenging and IRL is simply more pragmatic. However, complications with IRL in unregularized MDPs relate to the issue of degeneracy, where any constant function can rationalize the expert's behavior (Ng et al., 2000).\nFortunately, Geist et al. (2019) show that IRL in regularized MDPs-regularized IRL-does not contain such degenerate solutions due to the uniqueness of the optimal policy for regularized MDPs. Despite this, no tractable solutions of regularized IRL-other than maximum-Shannon-entropy IRL (MaxEntIRL) (Ziebart et al., 2008;Ziebart, 2010;Ho & Ermon, 2016;Finn et al., 2016a;Fu et al., 2018)-have been proposed.\nIn Geist et al. (2019), solutions for regularized IRL were introduced. However, they are generally intractable since they require a closed-form relation between the policy and optimal value function and the knowledge on model dynamics. Furthermore, practical algorithms for solving regularized IRL problems have not yet been proposed. We summarize our contributions as follows. Unlike the solutions in Geist et al. (2019), we propose tractable solutions for regularized IRL problems that can be derived from policy regularization and its gradient in discrete control problems (Section 3.1). We additionally show that our solutions are tractable for Tsallis entropy regularization with multi-variate Gaussian policies in continuous control problems (Section 3.2). We devise Regularized Adversarial Inverse Reinforcement Learning (RAIRL), a practical sample-based method for policy imitation and reward learning in regularized MDPs, which generalizes adversarial IRL (AIRL, Fu et al. (2018)) (Section 4). Finally, we empirically validate our RAIRL method on both discrete and continuous control tasks, evaluating RAIRL via episodic scores and from divergence minimization perspective (Ke et al., 2019;Ghasemipour et al., 2019;Dadashi et al., 2020) (Section 5)."}
{"introduction": "Artificial neural networks have been increasingly used to study biological neural circuits. In particular, recent work in vision demonstrated that convolutional neural networks (CNNs) trained to perform visual object classification provide state-of-the-art models that match neural responses along various stages of visual processing (Yamins et al., 2014;Khaligh-Razavi & Kriegeskorte, 2014;Yamins & DiCarlo, 2016;Cadieu et al., 2014;G\u00fc\u00e7l\u00fc & van Gerven, 2015;Kriegeskorte, 2015). Recurrent neural networks (RNNs) trained on cognitive tasks have also been used to account for neural response characteristics in various domains (Mante et al., 2013;Sussillo et al., 2015;Song et al., 2016;Cueva & Wei, 2018;Banino et al., 2018;Remington et al., 2018;Wang et al., 2018;Orhan & Ma, 2019;Yang et al., 2019). We address these challenges using the brain's internal compass -the head direction system, a system that has accumulated substantial amounts of functional and structural data over the past few decades in rodents and fruit flies (Taube et al., 1990a;b;Turner-Evans et al., 2017;Green et al., 2017;Seelig & Jayaraman, 2015;Stone et al., 2017;Lin et al., 2013;Finkelstein et al., 2015;Wolff et al., 2015;Green & Maimon, 2018). We trained RNNs to perform a simple angular velocity (AV) integration task (Etienne & Jeffery, 2004) and asked whether the anatomical and functional features that have emerged as a result of stochastic gradient descent bear similarities to biological networks sculpted by long evolutionary time. By leveraging existing knowledge of the biological head direction (HD) systems, we demonstrate that RNNs exhibit striking similarities in both structure and function. Our results suggest that goal-driven training of artificial neural networks provide a framework to study neural systems at the level of both neural activity and anatomical organization."}
{"introduction": "Vision Transformer (Chu et al., 2021b;d'Ascoli et al., 2021;Dosovitskiy et al., 2021;Guo et al., 2021;Han et al., 2020;Khan et al., 2021;Touvron et al., 2020;Wang et al., 2021b;Wu et al., 2021;Xu et al., 2021;Yuan et al., 2021b) has shown promising performance in ImageNet classification. The improved variants, Local Vision Transformer (Chu et al., 2021a;Liu et al., 2021b;Vaswani et al., 2021), adopt the local attention mechanism, which partitions the image space into a set of small windows, and conducts the attention over the windows simultaneously. Local attention leads to great improvement in memory and computation efficiency and makes the extension to downstream tasks easier and more efficient, such as object detection and semantic segmentation.\nWe further present the empirical verification for the connection. We take the recently-developed Local Vision Transformer, Swin Transformer (Liu et al., 2021b), as an example, and study the empirical performance of local attention and (dynamic) depth-wise convolution in the same training settings as Swin Transformer. We replace the local attention layer with the (dynamic) depth-wise convolution layer, keeping the overall structure unchanged.\nThe results show that the (dynamic) depth-wise convolution-based approaches achieve comparable or slightly higher performance for ImageNet classification and two downstream tasks, COCO object detection and ADE semantic segmentation, and (dynamic) depth-wise convolution takes lower computation complexity. The ablation studies imply that weight sharing and dynamic weight improves the model capability. Specifically, (i) for Swin Transformer, weight sharing across channels is beneficial mainly for reducing the parameter (attention weight) complexity, and the attention-based dynamic weight scheme is advantageous in learning instance-specific weights and block-translation equivalent representations; (ii) for depth-wise convolution, weight sharing across positions is beneficial for reducing the parameter complexity as well as learning translation equivalent representations, and the linear projection-based dynamic weight scheme learns instance-specific weights."}
{"introduction": "Input-gradients, or gradients of outputs w.r.t. inputs, are commonly used for the interpretation of deep neural networks (Simonyan et al., 2013). We consider the connections made between softmax-based discriminative classifiers and generative models (Bridle, 1990;Grathwohl et al., 2020), made by viewing the logits of standard classifiers as un-normalized log-densities. This connection reveals an alternate interpretation of input-gradients, as representing the log-gradients of a class-conditional density model which is implicit within standard softmax-based deep models, which we shall call the implicit density model. This compels us to consider the following hypothesis: perhaps input-gradients are highly structured because this implicit density model is aligned with the 'ground truth' class-conditional data distribution? The core of this paper is dedicated to testing the validity of this hypothesis, whether or not input-gradients do become more structured and explanatory if this alignment increases and vice versa.\nFor the purpose of validating this hypothesis, we require mechanisms to increase or decrease the alignment between the implicit density model and the data distribution. To this end, we consider a generative modelling approach called score-matching, which reduces the density modelling problem to that of local geometric regularization. Hence by using score-matching, we are able to view commonly used geometric regularizers in deep learning as density modelling methods. In practice, the score-matching objective is known for being computationally expensive and unstable to train (Song & Ermon, 2019;Kingma & LeCun, 2010). To this end, we also introduce approximations and regularizers which allow us to use score-matching on practical large-scale discriminative models.\nThis work is broadly connected to the literature around unreliability of saliency methods. While most such works consider how the explanations for nearly identical images can be arbitrarily different (Dombrowski et al., 2019;Subramanya et al., 2019;Zhang et al., 2020;Ghorbani et al., 2019), our work considers how one may change the model itself to yield arbitrary explanations without affecting discriminative performance. This is similar to Heo et al. (2019) who show this experimentally, whereas we provide an analytical reason for why this happens relating to the shift-invariance of softmax.\nThe rest of the paper is organized as follows. We show in \u00a7 2 that it is trivial to manipulate input-gradients of standard classifiers using the shift-invariance of softmax without affecting the discriminative model. In \u00a7 3 we state our main hypothesis and describe the details of score-matching, present a tractable approximation for the same that eliminates the need for expensive Hessian computations. \u00a7 4 revisits other interpretability tools from a density modelling perspective. Finally, \u00a7 5 presents experimental evidence for the validity of the hypothesis that improved alignment between the implicit density model and the data distribution can improve the structure and explanatory nature of input-gradients."}
{"introduction": "Deep learning has shown high performance in several tasks such as image recognition, speech recognition, and natural language processing. In particular, convolutional neural networks (CNNs) and dilated CNNs have been quite effective in tasks involving high-dimensional data (van den Oord et al., 2016;He et al., 2016;Simonyan & Zisserman, 2015;Yoon, 2014). The theoretical nature of deep learning has been thoroughly studied and is well understood. The function approximation and estimation capabilities of deep learning have been analyzed extensively in past research. Although the derived rates of convergence are optimal, these studies assumed that the dimensionality of inputs is fixed and much smaller than the sample size. \n\nIn terms of infinite dimensional inputs, there have been already several studies on approximation and estimation errors for non-deep-learning methods. For example, so called hyperbolic cross approximation has been considered to approximate a function in a tensor product space with support on [0, 1] \u221e (D\u0169ng & Griebel, 2016) and a polynomial order approximation is possible for functions with mixed smoothness. Ingster & Stepanova (2011) analyzed a Gaussian white noise model with an infinite dimensional input and showed that the estimation accuracy for signals on infinite dimensional anisotropic Sobolev spaces depends on the reciprocal sum of the smoothness per axis (see also Ingster & Stepanova (2006); Ingster & Suslina (2007); Ingster & Stepanova (2009)). \n\nIn this study, we analyze the approximation and estimation accuracy in a setting where the input is infinite dimensional. We assume that the true function has mixed and anisotropic smoothness, that is, the function has different smoothness toward different coordinate similarly to D\u0169ng & Griebel (2016); Ingster & Stepanova (2011). The intuition behind this setting is as follows: Considering a function which takes an image as an input, an image can be decomposed into different frequency components and usually a function of images has less sensitivity on the high frequency components and more dependent on the low frequency components, which can be formulated as non-uniform smoothness toward each coordinate direction. By considering such a setting, we can show that the rate of convergence can avoid the curse of dimensionality and be of polynomial order. \n\nLet \u03bb be the uniform probability measure on [0, 1] and let \u03bb \u221e be the product measure of \u03bb on [0, 1]\u221e. Let PX be a probability measure defined on the measurable space ([0, 1]\u221e, B([0, 1]\u221e)). Then, suppose that there exists a true function fo: [0, 1]\u221e \u2192 R, and consider the following nonparametric regression problem with an infinite dimensional input:\n\ny = fo(X) + \u03be,\n\nwhere X is a random variable taking its value on [0, 1]\u221e and obeys the distribution PX introduced above, and \u03be is a observation noise generated from N(0, \u03c32) (a normal distribution with mean 0 and variance \u03c32 > 0). Let P be the joint distribution of X and Y obeying the regression model.\n\nWhat we investigate in the following is (i) how efficiently we can approximate the true function fo by a neural network, and (ii) how accurately deep learning can estimate the true function fo from n observations Dn = (Xi, yi)ni=1 where (Xi, yi)ni=1 are i.i.d. observations from the model. As a performance measure, we employ the mean squared error f - fo 2L2(PX) which can be seen as the excess risk of the predictive error."}
{"introduction": "Markov chain Monte Carlo (MCMC) methods like Langevin dynamics generate samples from the posterior that converge asymptotically. However, MCMC methods require many iterations and converge slowly.\n\nWe propose an amortization method for Langevin dynamics called amortized Langevin dynamics (ALD). In ALD, datapoint-wise MCMC iterations are replaced with updates of an inference model that maps observations into latent variables. This amortization enables simultaneous sampling from posteriors over massive datasets. When inference is performed for new test data, the trained inference model can be used as initialization of MCMC to improve the mixing.\n\nWe experimentally show that the ALD can accurately perform sampling from posteriors without datapoint-wise iterations. Furthermore, we demonstrate its applicability to the training of deep generative models called Langevin autoencoders (LAEs). LAEs can be easily extended for more flexible generative modeling by combining them with contrastive divergence learning. We refer to this extension of LAEs as contrastive Langevin autoencoders (CLAEs). We experimentally show that our LAEs and CLAEs can generate sharper images than existing explicit generative models, such as VAEs. Moreover, we report their performance of unsupervised anomaly detection."}
{"introduction": "Estimation of quantities defined by the stationary distribution of a Markov chain lies at the heart of many scientific and engineering problems. A Markov chain is a stochastic model that undergoes transitions between states based on fixed probabilities. The long-run proportion of time spent in each state is described by the stationary distribution, which satisfies detailed balance and does not change over time. There are many techniques for analyzing Markov chains, including solving linear systems, eigenvalue analysis, and Monte Carlo methods. The PageRank algorithm, originally developed by Larry Page and Sergey Brin, leverages a random walk on the web graph to rank web pages. Markov chain Monte Carlo methods use Markov chains to sample from complex probability distributions for Bayesian inference. Queueing theory studies queues and waiting times using Markov chains. Reinforcement learning also relies heavily on Markov decision processes.\n\nHow can we estimate stationary distribution quantities in challenging off-line settings?\n\nSome existing methods for estimating stationary distributions require interacting with the environment or knowing the transition probabilities. But often this information is not available, like when using previously collected data. For example, evaluating a new policy using past experience in reinforcement learning. Or estimating PageRank from a sample of the web graph."}
{"introduction": "Principal component analysis is a technique in machine learning. It is well known that the PCA solution of the d-dimensional dataset X \u2208 R n\u00d7d is given by the eigenvectors of X X or equivalently, the right singular vectors of X. Impractically, the cost of computing the full SVD scales with O(min{nd 2 , n 2 d})-time and O(nd)-space (Shamir, 2015; Tang, 2019). For moderately sized data, randomized methods can be used (Halko et al., 2011). Beyond this, stochastic-or online-methods based on Oja's rule (Oja, 1982) or power iterations (Rutishauser, 1971) are common. Another option is to use streaming k-PCA algorithms such as Frequent Directions (FD) (Ghashami et al., 2016) or Oja's algorithm 2 (Allen-Zhu and Li, 2017) with storage complexity O(kd). Sampling or sketching methods also scale well, but again, focus on the top-k subspace (Sarlos, 2006; Cohen et al., 2017; Feldman et al., 2020).\n\nIn contrast to these approaches, we view each principal component (equivalently eigenvector) as a player in a game whose objective is to maximize their own local utility function in controlled competition with other vectors. The proposed utility gradients are interpretable as a combination of Oja's rule and a generalized Gram-Schmidt process. We make the following contributions:\n\n- A novel formulation of PCA as finding the Nash equilibrium of a suitable game,\n\n- A sequential, globally convergent algorithm for approximating the Nash on full-batch data, 1 After learning the top-k subspace V \u2208 R d\u00d7k , the rotation can be recovered via an SVD of XV. 2 FD approximates the top-k subspace; Oja's algorithm approximates the top-k eigenvectors.\n\n- A decentralized algorithm with experiments demonstrating the approach as competitive with modern streaming k-PCA algorithms on synthetic and real data, \n\n- In demonstration of the scaling of the approach, we compute the top-32 principal components of the matrix of RESNET-200 activations on the IMAGENET dataset (n \u2248 10 6 , d \u2248 20 \u2022 10 6 )."}
{"introduction": "A fundamental component in deep convolutional neural networks is the pooling operation, which replaces the output of convolutions with local summaries of nearby points and is usually implemented by maximum or average operations. State-of-the-art architectures alternate convolutions, which extrapolate local patterns irrespective of the specific location on the input signal, and pooling, which lets the ensuing convolutions capture aggregated patterns. Pooling allows to learn abstract representations in deeper layers of the network by discarding information that is superfluous for the task, and keeps model complexity under control by limiting the growth of intermediate features. Graph neural networks (GNNs) are a class of deep learning methods that operate on graph structured data. They extend convolutional neural networks to non-Euclidean domains such as graphs and manifolds. Graph convolutional networks (GCNs) generalize convolutional neural networks (CNNs) to graph data by replacing the linear convolutional filters with filters that operate on graphs and replace regular grid-like data with arbitrarily structured graphs. The convolution operation in GCNs enables extracting local features centered around each node and its neighbors. By stacking multiple convolutional layers, GCNs can learn highly nonlinear functions to perform transductive node classification and graph-level regression. However, developing pooling strategies for GNNs has lagged behind newer message-passing operations like graph convolutions. This is because it is difficult to define a pooled version of the original graph that can support the pooled signal while maintaining structural information. Common pooling techniques like average pooling lack flexibility as they do not extract local structural patterns. Pre-computing coarsened graphs is another approach but ignores task objectives and node features. \nIn this paper, we propose a differentiable pooling operation implemented as a neural network layer, which can be seamlessly combined with other MP layers. The parameters in the pooling layer are learned by combining the task-specific loss with an unsupervised regularization term, which optimizes a continuous relaxation of the normalized minCUT objective. The minCUT identifies dense graph components, where the nodes features become locally homogeneous after the message-passing. By gradually aggregating these components, the GNN learns to distil global properties from the graph. The proposed minCUT pooling operator (minCUTpool) yields partitions that 1) cluster together nodes which have similar features and are strongly connected on the graph, and 2) take into account the objective of the downstream task."}
{"introduction": "Quantifying the relationship between two variables is a common challenge across many fields. Mutual information measures the dependence between two random variables X and Y and is defined as:\n\nIt is difficult to estimate mutual information with limited samples. Existing methods like k-NN Kraskov et al. (2004) and KDE Khan et al. (2007) estimate mutual information by approximating probability densities. However, these do not perform well as dimension or dependence between variables increases. \n\n1. We propose new variational lower bounds on mutual information based on nonextensive statistical mechanics.\n2. We review generalized logarithm and exponential functions, define a generalized partition function, and use them to balance estimator variance and bias.  \n3. We outperform previous estimators in capturing correlation trends and dimensionality by using different generalizations."}
{"introduction": "Reinforcement learning (RL) is concerned with building agents that can learn to act so as to maximize reward through trial-and-error interaction with the environment. There are several reasons why it can be useful for an agent to learn about multiple ways of behaving, i.e., learn about multiple policies. The agent may want to achieve multiple tasks (or subgoals) in a lifelong learning setting and may learn a separate policy for each task, reusing them as needed when tasks reoccur. The agent may have a hierarchical architecture in which many policies are learned at a lower level while an upper level policy learns to combine them in useful ways, such as to accelerate learning on a single task or to transfer efficiently to a new task. Learning about multiple policies in the form of options (Sutton et al., 1999a) can be a good way to achieve temporal abstraction; again this can be used to quickly plan good policies for new tasks.  \n\nIn this paper, we aim to explore different approaches to policy learning and composition. There are two components to policy learning: (i) what policies should be in the set, and (ii) how to compose a policy to be used on a new task from the policies in the set. To answer (ii), we propose the concept of a set improving policy (SIP). Given any set of n policies, a SIP is any composition of these policies whose performance is at least as good as, and generally better than, that of all of the constituent policies in the set. We present two policy composition (or improvement) operators that lead to a SIP. The first is called set-max policy (SMP). Given a distribution over states, a SMP chooses from n policies the one that leads to the highest expected value. The second SIP operator is generalized policy improvement (Barreto et al., 2017, GPI). Given a set of n policies and their associated action-value functions, GPI is a natural extension of regular policy improvement in which the agent acts greedily in each state with respect to the maximum over the set of action-values functions. Although SMP provides weaker guarantees than GPI (we will show this below), it is more amenable to analysis and thus we will use it exclusively for our theoretical results. However, since SMP's performance serve as a lower bound to GPI's, the results we derive for the former also apply to the latter. In our illustrative experiments we will show this result empirically. \n\nRelated Work. The proposed approach has interesting connections with hierarchical RL (HRL) (Sutton et al., 1999b;Dietterich, 2000). We can think of SMP (and GPI) as a higher-level policy-selection mechanism that is fixed a priori. Under this interpretation, the problem we are solving can be seen as the definition and discovery of lower-level policies that will lead to a robust hierarchical agent.\nThere are interesting parallels between robustness and diversity. For example, diverse stock portfolios have less risk. In robust least squares (El Ghaoui & Lebret, 1997;Xu et al., 2009), the goal is to find a solution that will perform well with respect to (w.r.t) data perturbations. This leads to a min-max formulation, and there are known equivalences between solving a robust (min-max) problem and the diversity of the solution (via regularization) (Xu & Mannor, 2012). Our work is also related to robust Markov decision processes (MDPs) (Nilim & El Ghaoui, 2005), but our focus is on a different aspect of the problem. While in robust MDPs the uncertainty is w.r.t the dynamics of the environment, here we focus on uncertainty w.r.t the reward and assume that the dynamics are fixed. More importantly, we are interested in the hierarchical aspect of the problem -how to discover and compose a set of policies. In contrast, solutions to robust MDPs are typically composed of a single policy.\nIn Apprenticeship Learning (AL; Abbeel & Ng, 2004) the goal is also to solve a min-max problem in which the agent is expected to perform as well as an expert w.r.t any reward. If we ignore the expert, AL algorithms can be used to find a single policy that performs well w.r.t any reward. The solution to this problem (when there is no expert) is the policy whose SFs have the smallest possible norm. When the SFs are in the simplex (as in tabular MDPs) the vector with the smallest 2 norm puts equal probabilities on its coordinates, and is therefore \"diverse\" (making an equivalence between the robust min-max formulation and the diversity perspective). In that sense, our problem can be seen as a modified AL setup where: (a) no expert demonstrations are available (b) the agent is allowed to observe the reward at test time, and (c) the goal is to learn a set of constituent policies."}
{"introduction": "The deep learning community is undergoing a transition from neural architecture to automatically designed neural architecture (Zoph & Le, 2017;Pham et al., 2018;Real et al., 2019;Dong & Yang, 2019b;Liu et al., 2019). These NAS-generated architectures have shown promising results in many domains, such as image recognition (Zoph & Le, 2017;Pham et al., 2018;Real et al., 2019), sequence modeling (Pham et al., 2018;Dong & Yang, 2019b;Liu et al., 2019), etc. While these NAS methods are methodically designed and show promising improvements, many setups in their algorithms are different. \n(1) Different search space is utilized, e.g., different macro skeletons of the whole architecture (Zoph et al., 2018;Tan et al., 2019) and a different operation set for the micro cell within the skeleton (Pham et al., 2018), etc. (2) After a good architecture is selected, various strategies can be employed to train this architecture and report the performance, e.g., different data augmentation (Ghiasi et al., 2018;Zhang et al., 2018), different regularization (Zoph et al., 2018), different scheduler (Loshchilov & Hutter, 2017), and different selections of hyper-parameters (Liu et al., 2018;Dong & Yang, 2019a).\n(3) The validation set for testing the performance of the selected architecture is not split in the same way (Liu et al., 2019;Pham et al., 2018). These discrepancies make it difficult to conclude their contributions.\nIn response, NAS-Bench-201 with a fixed cell search space, inspired from the search space used in the most popular neural cell-based searching algorithms (Zoph et al., 2018;Liu et al., 2019). Architecture search is transformed into the problem of searching a good cell. The size of the search space is related to the number of nodes defined for the DAG and the size of the operation set. In NAS-Bench-201, we choose 4 nodes and 5 representative operation candidates for the operation set, which generates a total search space of 15,625 cells/architectures. Each architecture is trained multiple times on three different datasets. (Liu et al., 2018;Zoph & Le, 2017) so that researchers can target on the essence of NAS, i.e., search algorithm. Another benefit is that the validation time for NAS largely decreases when testing in NAS-Bench-201, which provides a computational power friendly environment for more participations in NAS.\n(3) It provides results of each architecture on multiple datasets. The model transferability can be thoroughly evaluated for most NAS algorithms. (4) In NAS-Bench-201, we provide systematic analysis of the proposed search space. We also evaluate 10 recent advanced NAS algorithms including reinforcement learning (RL)-based methods, evolutionary strategy (ES)-based methods, differentiable-based methods, etc. We hope our empirical analysis can bring some insights to the future designs of NAS algorithms."}
{"introduction": "Learning disentangled representation from the data generated from variation factors gives interpretable insight on real-world applications such as face recognition, self-driving and explainable healthcare. The notion of disentangled representations was theoretically proposed in (Bengio et al. (2013)). One conceptually agreed definition is that the disentangled representation comprises a number of latent factors, with each factor controlling an interpretable aspect of the generated data (Bengio et al. (2013)). For example, in flower images, disentangled latent factors might control variations in color, shape, and background. Disentangled representations promise several advantages: better generalization ability (Higgins et al. (2017)), increased interpretability (Adel et al. (2018)), and faster learning on downstream tasks such as reasoning (van Steenkiste et al. (2019)).\nDespite the recent growth of the field, most of the works of unsupervised disentanglement learning rely on the assumption that the generative factors are independent. One line of these works includes variants of Variational AutoEncoder(VAE) (Kingma & Welling (2013)). They directly minimize the total correlation of the features (Higgins et al. (2016); Chen et al. (2018); Kim & Mnih (2018); Kumar et al. (2018)). Another kind of disentanglement learning models, GAN-based models (Chen et al. (2016); Jeon et al. (2018); Lin et al. (2020)) are also restricted by independence assumption since they randomly sample the latent representation in the data generation process. However, independence assumption is often violated in real-world scenarios. For the images used for object classification, factors such as texture and color are confounded by the species of objects (e.g. stripes and black/white are correlated since they co-occur in the images of zebras). There are also recent works which use self-supervised learning to boost disentanglement (Zhu et al. (2020); Lin et al. (2020)). They propose to achieve an additional self-supervision information by self-supervised learning assuming independence assumption is satisfied.\nThe disentanglement performance of algorithms proposed under independence assumption may reduce when generative factors are correlated as in the work Tr\u00e4uble et al. (2020) shows. In this paper, we consider the unsupervised learning of disentanglement in a general setting which the generative factors of the data may be correlated. We development an intervention-based framework to address this problem. In particular, we define a random intervention operation which assigns a sampled value to one selected feature of the learnt image representation. Random intervention operation allows us to obtain an adjusted image representation which satisfies the selected feature is independent of the rest fixed features. To measure and improve disentanglement, we propose a novel metric by an elaborated downstream image translation task. A well disentangled representation may result in an relatively easy translation task and this translation task provides self-supervision information for our model. We prove the effectiveness of our novel metric experimentally and it correlates well with existing ground-truth-required metrics.\nOur main contributions can be summarized as follows: 1) We address the unsupervised learning of disentanglement under a general setting that the independence assumption may be violated; 2) We propose an end-to-end framework to tackle the disentanglement, meanwhile we propose a novel metric and prove it is consistent with existed ground-truth-required metrics experimentally; 3) We evaluate our framework on benchmark datasets under independent/correlated factors assumption and compare the quality of disentanglement with baselines. The results show that our model outperforms baselines given correlated data. For the experiments on real-world dataset without ground truth factors, our model extracts semantic factors compared to baselines."}
{"introduction": "Class imbalance presents challenges for training deep neural networks. A natural approach in attempt to bypass this class-imbalance problem is to re-balance the training objective artificially in class-wise with respect to their numbers of samples. Two of such methods are representative: (a) \"re-weighting\" the given loss function by a factor inversely proportional to the sample frequency in class-wise (Huang et al., 2016;Khan et al., 2017), and (b) \"re-sampling\" the given dataset so that the expected sampling distribution during training can be balanced, either by \"over-sampling\" the minority classes (Japkowicz, 2000;Cui et al., 2018) or \"under-sampling\" the majority classes (He & Garcia, 2008).\n\nThe methods on this line, however, usually result in harsh over-fitting to minority classes, since in essence, they cannot handle the lack of information on minority data. Several attempts have been made to alleviate this over-fitting issue: Cui et al. (2019) proposed the concept of \"effective number\" of samples as alternative weights in the re-weighting method. In the context of re-sampling, on the other hand, SMOTE (Chawla et al., 2002) is a widely-used variant of the over-sampling method that mitigates the over-fitting via data augmentation, but generally this direction has not been much explored recently. Cao et al. (2019) found that both re-weighting and re-sampling can be much more effective when applied at the later stage of training, in case of neural networks.\n\nAnother line of the research attempts to prevent the over-fitting with a new regularization scheme that minority classes are more regularized, where the margin-based approaches generally suit well as a form of data-dependent regularizer (Zhang et al., 2017;Dong et al., 2018;Khan et al., 2019;Cao et al., 2019). There have also been works that view the class-imbalance problem in the framework of active learning (Ertekin et al., 2007;Attenberg & Ertekin, 2013) or meta-learning (Wang et al., 2017;Ren et al., 2018;Shu et al., 2019;Liu et al., 2019).  \n\nContribution. In this paper, we revisit the over-sampling framework and propose a new way of generating minority samples, coined Adversarial Minority Over-sampling (AMO). In contrast to other over-sampling methods, e.g. SMOTE (Chawla et al., 2002) that applies data augmentation to minority samples to mitigate the over-fitting issue, we attempt to generate minority samples in a completely different way: AMO does not use the existing minority samples for synthesis, but use adversarial examples (Szegedy et al., 2014;Goodfellow et al., 2015) of non-minority samples made from another, baseline classifier (potentially, over-fitted to minority classes) independently trained using the given imbalanced dataset. This motivation leads us to a very counter-intuitive method at a first glance: it results in labeling minority class on an adversarial example of a majority class at last. Our key finding is that, this method actually can be very effective on learning generalizable features in the imbalanced learning: it does not overly use the minority samples, and leverages the richer information of the majority samples simultaneously.\n\nOur minority over-sampling method consists of three components to improve the sampling quality. First, we propose an optimization objective for generating synthetic samples, so that a majority input can be translated into a synthetic minority sample via optimizing it, while not affecting the performance of the majority class (even the sample is labeled to the minority class). Second, we design a sample rejection criteria based on the observation that generation from more majority class is more preferable. Third, based on the proposed rejection criteria, we suggest an optimal distribution for sampling the initial seed points of the generation.\n\nWe evaluate our method on various imbalanced classification problems, including synthetically imbalanced CIFAR-10/100 (Krizhevsky, 2009), and real-world imbalanced datasets including Twitter dataset (Gimpel et al., 2011) and Reuters dataset (Lewis et al., 2004) in natural language processing. Despite its simplicity, our method of adversarial minority over-sampling significantly improves the balanced test accuracy compared to previous re-sampling or re-weighting methods across all the tested datasets. These results even surpass the results from state-of-the-art margin-based method (LDAM; Cao et al. 2019). We also highlight that our method is fairly orthogonal to the regularization-based methods, by showing that joint training of our method with LDAM could further improve the balanced test accuracy as well.\n\nDespite the great generalization ability of DNNs, they are known to be susceptible to adversarial examples, which makes it difficult to deploy them in real-world safety-critical applications (Szegedy et al., 2014;Goodfellow et al., 2015). The broad existence of adversarial examples in DNNs is still a mysterious phenomenon (Gilmer et al., 2019;Galloway et al., 2019;Ilyas et al., 2019), and we think our results can be of independent interest to shed new insight on understanding their property."}
{"introduction": "Persistence diagrams have been successfully used to analyse problems ranging from financial crashes (Gidea & Katz, 2018) to protein binding (Kovacev-Nikolic et al., 2014), but the non-Hilbertian nature of the space of persistence diagrams means it is difficult to directly use persistence diagrams for machine learning. In order to better integrate diagrams into machine learning workflows, efforts have been made to map them into a more manageable form; primarily through embeddings into finite feature vectors, functional summaries, or by defining a positive-definite kernel on diagram space. In all cases, this explicitly or implicitly embeds diagrams into a Hilbert space which deforms the metric structure, potentially losing important information. With the exception of Topological Autoencoders, techniques to integrate these persistence-based summaries as topological regularisers and loss functions currently require prior knowledge about the correct topology of the dataset, which is clearly not feasible in most scenarios.\nAgainst this background, we give an algorithm to perform Fuzzy c-Means (FCM) clustering (Bezdek, 1980) directly on collections of persistence diagrams, giving an important unsupervised learning algorithm and enabling learning from persistence diagrams without deforming the metric structure. We perform the convergence analysis for our algorithm, giving the same guarantees as traditional FCM clustering: that every convergent subsequence of iterates tends to a local minimum or saddle point. We demonstrate the value of our fuzzy clustering algorithm by using it to cluster datasets that benefit from both the topological and fuzzy nature of our algorithm. We apply our technique in two settings: lattice structures in materials science and the decision boundaries of CNNs. A key property for machine learning in materials science has been identified as \"invariance to the basis symmetries of physics [...] rotation, reflection, translation\" (Schmidt et al., 2019). Geometric clustering algorithms do not have this invariance, but persistence diagrams do, making them ideally suited for this application; we can cluster transformed lattice structure datasets where geometric equivalents fail. In addition to this, our probabilistic membership values allow us to rank the top-k most likely lattices assigned to a cluster. This is particularly important in materials science, as further investigation requires expensive laboratory time and expertise. Our second application is inspired by Ramamurthy et al. (2019), who show that models perform better on tasks if they have topologically similar decision boundaries. We use our algorithm to cluster models and tasks by the persistence diagrams of their decision boundaries. Not only is our algorithm able to successfully cluster models to the correct task, based just on the topology of its decision boundary, but we show that higher membership values imply better performance on unseen tasks."}
{"introduction": "A paradigm for research and development in deep learning involves the introduction of novel network architectures followed by experimental validation on a selection of tasks. The methodology has undoubtedly generated significant advances in the field. However, it is hampered by the fact that the full capabilities of a candidate model may be obscured by difficulties in the training procedure. It is often possible to overcome such difficulties by carefully selecting the optimizer, batch size, learning rate schedule, initialization scheme, or other hyperparameters. However, the standard strategies for searching for good values of these hyperparameters are not guaranteed to succeed, especially if the trainable configurations are constrained to a low-dimensional subspace of hyperparameter space, which can render random search, grid search, and even Bayesian hyperparameter selection methods unsuccessful.  \nIn this work, we argue that for long sequence tasks, the trainable configurations of initialization hyperparameters for LSTMs and GRUs lie in just such a subspace, which we characterize theoretically. In particular, we identify precise conditions on the hyperparameters governing the initial weight and bias distributions that are necessary to ensure trainability. These conditions derive from the observation that in order for a network to be trainable, (a) signals from the relevant parts of the input sequence must be able to propagate all the way to the loss function and (b) the gradients must be stable (i.e., they must not explode or vanish exponentially). As shown in Figure 1, training of recurrent networks with standard initialization on certain tasks begins to fail as the sequence length increases and signal propagation becomes harder to achieve. However, as we shall show, a suitably-chosen initialization scheme can dramatically improve trainability on such tasks.\nWe study the effect of the initialization hyperparameters on signal propagation for a very broad class of recurrent architectures, which includes as special cases many state-of-the-art RNN cells, including the GRU (Cho et al. (2014)), the LSTM (Hochreiter & Schmidhuber (1997)), and the peephole LSTM (Gers et al. (2002)). The analysis is based on the mean field theory of signal propagation developed in a line of prior work (Schoenholz et al. (2016); Xiao et al. (2018); Chen et al. (2018); Yang et al. (2019)), as well as the concept of dynamical isometry (Saxe et al. (2013); Pennington Test accuracy for peephole LSTM trained to classify sequences of MNIST digits after 8000 iterations. As the sequence length increases, the network is no longer trainable with standard initialization, but still trainable using critical initialization. et al. (2017; 2018)) that is necessary for stable gradient backpropagation and which was shown to be crucial for training simpler RNN architectures (Chen et al. (2018)). We perform a number of experiments to corroborate the results of the calculations and use them to motivate initialization schemes that outperform standard initialization approaches on a number of long sequence tasks."}
{"introduction": "Deep regression networks, in which a continuous output is predicted for a given input, are traditionally trained by minimizing squared/absolute error of output labels, which we refer to as direct regression. However, there is a significant gap in accuracy between direct regression and recent task-specialized approaches for regression problems including head pose estimation, age estimation, and facial landmark estimation. Given the increasing importance of deep regression networks, developing generic approaches to improving their accuracy is desirable.\n\nIn this work, we propose binary-encoded labels (BEL) which improves accuracy by generalizing application of binary classification to regression. In BEL, a target label is quantized and converted to a binary code of length M, and M binary classifiers are then used to learn these binary-encoded labels. An encoding function is introduced to convert the target label to a binary code, and a decoding function is introduced to decode the output of binary classifiers to a real-valued prediction. BEL allows using an adjustable number of binary classifiers depending upon the quantization, encoding, and decoding functions. BEL opens possible avenues to improve the accuracy of regression problems with a large design space spanning quantization, encoding, decoding, and loss functions. \nWe focus on the encoding and decoding functions and theoretically study the relations between the absolute error of label and binary classifiers' errors for sample encoding and decoding functions. This analysis demonstrates the impact of binary classifiers' error distribution over the numeric range of target labels on the suitability of different encoding and decoding functions. Based on our analysis and empirically observed binary classifiers' error distribution, we propose properties of suitable encoding functions for regression and explore various encoding functions on a wide range of tasks. We also propose an expected correlation-based decoding function for regression that can effectively reduce the quantization error introduced by the use of classification.\n\nA deep regression network consists of a feature extractor and a regressor and is trained end-to-end. A regressor is typically the last fully connected layer with one output logit for direct regression. Our proposed regression approach (BEL) can be combined with off-the-shelf task-specific feature extractors by increasing the regressor's output logits. Further, we find that the correlation between multiple binary classifiers' outputs can be exploited to reduce the size of the feature vector and consequently reduce the number of parameters in the regressor. We explore the use of different decoding functions for training loss formulation and evaluate binary cross-entropy, cross-entropy, and squared/absolute error loss functions for BEL. We evaluate BEL on four complex regression problems: head pose estimation, facial landmark detection, age estimation, and end-to-end autonomous driving. We make the following contributions in this work:\n\n- We propose binary-encoded labels for regression and introduce a general framework and a taxonomy for the design aspects of regression by binary classification. We propose desirable properties of encoding and decoding functions suitable for regression problems.\n\n- We present a series of suitable encoding, decoding, and loss functions for regression with BEL. We present an end-to-end learning approach and regression layer architecture for BEL. We combine BEL with task-specific feature extractors for four tasks and evaluate multiple encoding, decoding, and loss functions. BEL outperforms direct regression for all the problems and specialized approaches for several tasks. \n\n- We theoretically and empirically demonstrate the effect of different design parameters on the accuracy, how it varies across different tasks, datasets, and network architectures, and provide preliminary insights and motivation for further study."}
{"introduction": "Mutual information (MI) is an ubiquitous measure of dependency between a pair of random variables, and is one of the corner stones of information theory. In machine learning, the information maximization principle for learning representation from unlabeled data through self-supervision (Bell & Sejnowski, 1995) motivated the development of many MI estimators and applications (Hjelm et al., 2019;Noroozi & Favaro, 2016;Kolesnikov et al., 2019;Doersch et al., 2015;van den Oord et al., 2018b;Hu et al., 2017). The information bottleneck (Tishby et al., 1999;Kolchinsky et al., 2017) is another principle that triggered recent interest in mutual information estimation. MI is also used to understand the information flow in neural networks, in learning clusters (Krause et al., 2010) and in regularizing the training of Generative Adversarial Networks (GANs) (Chen et al., 2016).\nIn many of those machine learning applications and other scientific fields, one has to estimate MI given samples from the joint distribution of high dimensional random variables. This is a challenging problem and many methods have been devised to address it. Since MI is defined as the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginals, one can leverage non parametric estimators of f-divergences (Nguyen et al., 2008;Nowozin et al., 2016;Sriperumbudur et al., 2009). Specifically of interest to us is the Donsker-Varadhan (DV) representation of the KL divergence (Donsker & Varadhan, 1976) that was used recently with neural networks estimators (Belghazi et al., 2018;Poole et al., 2019). Other approaches to estimating the MI are through finding lower bounds using variational Bayesian methods (Alemi et al., 2016;2017;Barber & Agakov, 2003;Blei et al., 2017), as well as through geometric methods like binning (Kraskov et al., 2004).\nIn this paper we propose a new estimator of MI that can be used in direct MI maximization or as a regularizer, thanks to its unbiased gradients. Our starting point is the DV lower bound of the KL divergence that we represent equivalently via a joint optimization that we call \u03b7-DV on a witness function f and an auxiliary variable \u03b7 in Section 2. In Section 3, we show that when the witness function f is learned in a Reproducing Kernel Hilbert Space (RKHS) the \u03b7-DV problem is jointly convex in both f and \u03b7. The dual of this problem sheds the light on this estimator as a constrained ratio estimation where \u03b7 plays the role of a Lagrange multiplier that ensures proper normalization of the likelihood ratio. We also show how the witness function can be estimated as a neural network akin to Belghazi et al. (2018). We specify our estimator for MI in Section 4, and show how it compares to alternatives in the literature (Nguyen et al., 2008;Belghazi et al., 2018;Poole et al., 2019). The experiments are presented in Section 5. On synthetic data, we validate our estimators by estimating MI on Gaussian variables and by regularizing GAN training as in Chen et al. (2016). On real data, we explore our estimator in deep MI maximization for learning representation from unlabeled data.\nFigure 1 shows an overview of all the bounds and related MI estimators discussed in this paper."}
{"introduction": "Federated learning is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data. The goal is to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. This approach emerged as a way to enable machine learning on private datasets kept locally on devices like phones, tablets and laptops.\n\nFederated learning was originally conceptualized in 2016 by Google researchers including Brendan McMahan and Daniel Ramage. The researchers recognized that the increasing capabilities of mobile devices coupled with privacy concerns around storing personal data in the cloud necessitated a new paradigm for machine learning where algorithms are trained locally on devices using decentralized data and only model updates are aggregated to benefit from global data. \n\nThe federated learning process works by training local models on each device using the local data samples available. These local models are then shared with a centralized server, where they are aggregated into a global model using techniques like Federated Averaging (FedAvg). The global model is then shared back with the devices and the process repeats until the model converges. As data remains local, this protects user privacy while allowing models to learn from a larger global dataset.\n\nKey benefits of federated learning include preserving privacy, reducing transmission costs, and availability of localized data. Challenges include statistical heterogeneity, systems heterogeneity, limited communication, and privacy constraints. Overall, federated learning is an important evolution in distributed machine learning and enables models to be trained on sensitive decentralized datasets at global scale.\n\n\nNotation We adopt the following notation in this paper: \n\u2022 For random variable \u03bet, we use \u039en,T and Dn,T to denote the set of random variables and distributions, respectively: \n\u2022 For a decentralized network with n nodes, we use W \u2208 Rn\u00d7n to present the confusion matrix, where Wij \u2265 0 is the weight that node i sends to node j (i, j \u2208 [n]). Niout = {j|Wij > 0} and Niin = {j|Wji > 0} are also used for denoting the sets of in neighbors of and out neighbors of node i respectively.\n\u2022 Norm \u2016\u2027\u2016 denotes the 2 norm \u2016\u2027\u20162 by default."}
{"introduction": "Learning disentangled representation from the data generated from variation factors gives interpretable insight on real-world applications such as face recognition, self-driving and explainable healthcare. The notion of disentangled representations was theoretically proposed in (Bengio et al. (2013)). One conceptually agreed definition is that the disentangled representation comprises a number of latent factors, with each factor controlling an interpretable aspect of the generated data (Bengio et al. (2013)). For example, in flower images, disentangled latent factors might control variations in color, shape, and background. Disentangled representations promise several advantages: better generalization ability (Higgins et al. (2017)), increased interpretability (Adel et al. (2018)), and faster learning on downstream tasks such as reasoning (van Steenkiste et al. (2019)). Variation factors refer to the different ways in which the data can change or vary. For example, in images of faces, some variation factors could be lighting, pose, facial expression, etc. Being able to disentangle these factors is useful for many applications. Lighting could be separated from identity, allowing face recognition to be invariant to lighting conditions. Pose could be separated from facial expression, allowing better understanding of emotions.  \n\nIn this paper, we consider the unsupervised learning of disentanglement in a general setting which the generative factors of the data may be correlated. We development an intervention-based framework to address this problem. In particular, we define a random intervention operation which assigns a sampled value to one selected feature of the learnt image representation. Random intervention operation allows us to obtain an adjusted image representation which satisfies the selected feature is independent of the rest fixed features. To measure and improve disentanglement, we propose a novel metric by an elaborated downstream image translation task. A well disentangled representation may result in an relatively easy translation task and this translation task provides self-supervision information for our model. We prove the effectiveness of our novel metric experimentally and it correlates well with existing ground-truth-required metrics.\nOur main contributions can be summarized as follows: 1) We address the unsupervised learning of disentanglement under a general setting that the independence assumption may be violated; 2) We propose an end-to-end framework to tackle the disentanglement, meanwhile we propose a novel metric and prove it is consistent with existed ground-truth-required metrics experimentally; 3) We evaluate our framework on benchmark datasets under independent/correlated factors assumption and compare the quality of disentanglement with baselines. The results show that our model outperforms baselines given correlated data. For the experiments on real-world dataset without ground truth factors, our model extracts semantic factors compared to baselines."}
{"introduction": "The parameter and floating point operation (FLOP) efficiency of sparse neural networks is now well demonstrated on a variety of problems (Han et al., 2015;Srinivas et al., 2017). Some work has even shown inference time speedups are possible on Recurrent Neural Networks (RNNs) (Kalchbrenner et al., 2018) and Convolutional Neural Networks (ConvNets) (Park et al., 2016). Currently, the most accurate sparse models are obtained with techniques that require, at a minimum, the cost of training a dense model in terms of memory and FLOPs (Zhu & Gupta, 2018;Guo et al., 2016), and sometimes significantly more (Molchanov et al., 2017). This paradigm has two main limitations:\n1. The maximum size of sparse models is limited to the largest dense model that can be trained. Even if sparse models are more parameter efficient, we can't use pruning to train models that are larger and more accurate than the largest possible dense models.\n2. It is inefficient. Large amounts of computation must be performed for parameters that are zero valued or that will be zero during inference.\nAdditionally, it remains unknown if the performance of the current best pruning algorithms are an upper bound on the quality of sparse models. Gale et al. (2019) found that three different dense-tosparse training algorithms all achieve about the same sparsity / accuracy trade-off. However, this is far from conclusive proof that no better performance is possible. In this work we show the surprising result that dynamic sparse training, which includes the method we introduce below, can find more accurate models than the current best approaches to pruning initially dense networks. Importantly, our method does not change the FLOPs required to execute the model during training, allowing one to decide on a specific inference cost prior to training.\nThe Lottery Ticket Hypothesis (Frankle & Carbin, 2019) hypothesized that if we can find a sparse neural network with iterative pruning, then we can train that sparse network from scratch, to the same level of accuracy, by starting from the original initial conditions. In this paper we introduce a new method for training sparse models without the need of a \"lucky\" initialization; for this reason, we call our method \"The Rigged Lottery\" or RigL * . We show that this method is:\n\u2022 Memory efficient: It requires memory only proportional to the size of the sparse model. It never requires storing quantities that are the size of the dense model. This is in contrast to Dettmers & Zettlemoyer (2019) which requires storing the momentum for all parameters, even those that are zero valued.\n\u2022 Computationally efficient: The amount of computation required to train the model is proportional to the number of nonzero parameters in the model.\n\u2022 Accurate: The performance achieved by the method matches and sometimes exceeds the performance of pruning based approaches.\nOur method works by infrequently using instantaneous gradient information to inform a re-wiring of the network. We show that this allows the optimization to escape local minima where it would otherwise become trapped if the sparsity pattern were to remain static. Crucially, as long as the full gradient information is needed less than every 1 1-sparsity iterations, then the overall work remains proportional to the model sparsity."}
{"introduction": "Learning to rank applies supervised or semi-supervised machine learning to construct ranking models for information retrieval problems. Learning to rank is a technique in information retrieval where a query is given along with a number of search results, which must be ranked by their relevance to the query. In learning to rank, there are typically three approaches: the pointwise, pairwise, and listwise approaches (Liu, 2011). The pointwise approach assigns an importance score to each pair of query and search result. The pairwise approach discerns which search result is more relevant for a certain query and a pair of search results. The listwise approach outputs the ranks for all search results given a specific query, therefore being the most general.  \nFor learning to rank, neural networks are known to enjoy success. Generally in such models, neural networks are applied to model the ranking probabilities with the features of queries and search results as the input. For instance, RankNet (Burges et al., 2005) applies a neural network to calculate a probability for any search result being more relevant compared to another. Each pair of query and search result is combined into a feature vector, which is the input of the neural network, and a ranking priority score is the output. Another approach learns the matching mechanism between the query and the search result, which is particularly suitable for image retrieval. Usually the mechanism is represented by a similarity matrix which outputs a bilinear form as the ranking priority score; for instance, such a structure is applied in Severyn & Moschitti (2015).\nWe postulate that it could be beneficial to apply multiple embeddings of the queries and search results to a learning to rank model. It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011). From such an approach, the randomness of the architecture of a single neural network can be effectively reduced. For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al. (2015). This is due to the fact that it is relatively hard to judge different models a priori. However, we have seen no literature on designing a mechanism to incorporate different embeddings for ranking. We hypothesize that applying multiple embeddings to a ranking neural network can improve the accuracy not only in terms of \"averaging out\" the error, but it can also provide a more robust solution compared to applying a single embedding.  \nFor learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features. It incorporates different embeddings with weights changing over time, derived from a recurrent neural network (RNN) structure. Thus, it can help us better summarize information from the query and search results. We also apply a decoder mechanism to rank all the search results, which provides a flexible list-wise ranking approach that can be applied to both image retrieval and text querying. Our model has the following contributions: (1) it applies the attention mechanism to listwise learning to rank problems, which we think is novel in the learning to rank literature; (2) it takes different embeddings of queries and search results into account, incorporating them with the attention mechanism; (3) double attention mechanisms are applied to both queries and search results.\nSection 2 reviews RankNet, similarity matching, and the attention mechanism in details. Section 3 constructs the attention-based deep net for ranking, and discusses how to calibrate the model. Section 4 demonstrates the performance of our model on image retrieval and text querying data sets. Section 5 discusses about potential future research and concludes the paper."}
{"introduction": "Recurrent neural networks (RNN), like LSTM (Hochreiter & Schmidhuber, 1997), have become popular methods for time series forecasting due to their end-to-end training, the ease of incorporating exogenous covariates, and their automatic feature extraction abilities, which are the hallmarks of deep learning. Forecasting outputs can either be points or probability distributions, in which case the forecasts typically come with uncertainty bounds.\n\nFinally, individual time series, in many cases, are statistically dependent on each other, and models need the capacity to adapt to this in order to improve forecast accuracy (Tsay, 2014). For example, to model the demand for a retail article, it is important to not only model its sales dependent on its own past sales, but also to take into account the effect of interacting articles, which can lead to cannibalization effects in the case of article competition. As another example, consider traffic flow in a network of streets as measured by occupancy sensors. A disruption on one particular street will also ripple to occupancy sensors of nearby streets-a univariate model would arguably not be able to account for these effects.\n\nIn this work, we propose end-to-end trainable autoregressive deep learning architectures for probabilistic forecasting that explicitly models multivariate time series and their temporal dynamics by employing a normalizing flow, like the Masked Autoregressive Flow (Papamakarios et al., 2017) or Real NVP (Dinh et al., 2017). These models are able to scale to thousands of interacting time series, we show that they are able to learn ground-truth dependency structure on toy data and we establish new state-of-the-art results on diverse real world data sets by comparing to competitive baselines. Additionally, these methods adapt to a broad class of underlying data distribution on account of using a normalizing flow and our Transformer based model is highly efficient due to the parallel nature of attention layers while training.\n\nThe paper first provides some background context in Section 2. We cover related work in Section 3. Section 4 introduces our model and the experiments are detailed in Section 5. We conclude with some discussion in Section 6. The Appendix contains details of the datasets, additional metrics and exploratory plots of forecast intervals as well as details of our model."}
{"introduction": "There has been interest in metalearning (e.g. Maclaurin et al., 2015;Andrychowicz et al., 2016;Finn et al., 2017). This is due to the methods meta-learning provides, amongst other things, for producing models that perform well beyond the confines of a single task, outside the constraints of a static dataset, or simply with greater data efficiency or sample complexity. The overarching aim of this paper is to purport some notion of ownership or precedence in any sense over existing efforts by virtue of having proposed a unifying formulation. Rather, in pointing out similarities under such unification, it provides theoretical and practical tools for facilitating further research in this exciting domain."}
{"introduction": "The field of natural language processing has seen great advances in recent years, enabled by the introduction of deep learning techniques. As early as the 1950s, researchers began exploring computational methods for understanding language, but progress was slow due to the limitations of symbolic approaches. The rise of statistical NLP in the 1990s led to more robust models, but they still could not reach human-level language understanding.\n\nA major breakthrough came with the development of word embeddings in 2013, allowing words to be represented as dense vectors capturing semantic meaning. This enabled neural networks to achieve far superior results on NLP tasks. Further advances came in 2017 with the introduction of the transformer architecture by Vaswani et al. The self-attention mechanism underpinning transformers proved immensely effective for sequential data like text.\n\nWhen transformer-based models are pretrained on large amounts of text through unsupervised objectives like masked language modeling, they develop rich contextual representations of language. This technique reached its apotheosis in 2018 with the release of BERT by Devlin et al. at Google AI. BERT obtained state-of-the-art results on a wide variety of NLP tasks, outperforming all previous approaches. Multilingual versions of BERT like mBERT showed the potential to learn unified representations across multiple languages.\n\nFollowing BERT, even more powerful pretrained transformers were developed by organizations like Google, Facebook, Microsoft, and NVIDIA. Generative pretrained transformers like GPT-3 demonstrated the ability to perform zero-shot transfer, few-shot learning, and natural language generation at an unprecedented scale. The entire NLP community embraced this paradigm shift, with transformers becoming ubiquitous for natural language tasks.\n\nPre-trained Transformers (Vaswani et al., 2017; Devlin et al., 2019) have lead to state-of-the-art results on a wide range of NLP tasks, for example, named entity recognition, relation extraction and question answering, often approaching human inter-rater agreement (Joshi et al., 2020a). These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons (Wu & Dredze, 2019; Pires et al., 2019). In this paper, we propose an alternative strategy for model selection in a zero-shot setting. Our approach, dubbed Learned Model Selection (LMS), learns a function that scores the compatibility between a fine-tuned multilingual transformer, and a target language. The compatibility score is calculated based on features of the multilingual model's learned representations and the target language. A model's features are based on its own internal representations; this is done by aggregating representations over an unlabeled target language text corpus. These model-specific features capture information about how the cross-lingual representations transfer to the target language after fine-tuning on source language data. In addition to modelspecific representations, we also make use of learned language embeddings from the lang2vec package (Malaviya et al., 2017), 2 which have been shown to encode typological information, for example, whether a language has prepositions or postpositions. To measure compatibility between Figure 1: An illustration of our approach to select the best model for zero-shot cross-lingual transfer. (a) Prior works select the best model using source language development data. (b) LMS: A learned function scores fine-tuned models based on their hidden layer representations when encoding unlabeled target language data. a multilingual model's fine-tuned representations and a target language, the model-and languagespecific representations are combined in a bilinear layer. Parameters of the scoring function are optimized to minimize a pairwise ranking loss on a set of held-out models, where the gold ranking is calculated using standard performance metrics, such as accuracy or F 1 , on a set of pivot languages (not including the target language). Our method assumes training data in English, and small amounts of annotated data in one or more pivot languages (not the target language). This corresponds to a scenario where a new multilingual NLP task needs to be quickly applied to a new language. LMS does not rely on any annotated data in the target language, yet it is effective in learning to predict whether fine-tuned multilingual representations are a good match.\n\nIn experiments on five well-studied NLP tasks (part of speech tagging, named entity recognition, question answering, relation extraction and event argument role labeling), we find LMS consistently selects models with better target-language performance than those chosen using English dev data. Appendix A.5 demonstrates that our framework supports multi-task learning, which can be helpful in settings where some target-language annotations are available, but not for the desired task. Finally, we show that LMS generalizes to both mBERT and XLM-RoBERTa in Appendix A.4."}
{"introduction": "Reinforcement Learning (RL) can be described as the computational approach to learning from interaction. In RL, an agent acts in an environment and receives observations including a numerical reward. The reward is usually a function of the current state (of the environment) and the action taken by the agent. The goal of the agent is to learn how to act, i.e which control strategy (or policy) to adopt in specific situations in order to achieve a long term goal (maximizing the long term expected reward). RL problems are typically modelled as Markov Decision Processes (MDPs).\nThis general formulation allows us to describe a large variety of tasks of practical interest across diverse fields such as game-playing, robotics and traffic control (Mnih et al., 2015;Levine et al., 2016;Ma et al., 2018;Arel et al., 2010). With the rise of deep learning, RL has seen great success in the recent years with artificial agents now outperforming humans in increasingly challenging tasks (Graepel, 2016;Silver et al., 2017). However, these agents exhibit weak-AI behaviors, being highly task specific and with limited ability to generalise across similar tasks. While some tasks may be very different from one another (e.g., learning how to drive vs learning to master the game of chess), others do not differ much (e.g., taking the train to work vs taking the train home). In the latter case, it is clearly desirable for the agent to be able to leverage the knowledge acquired while solving one task to speed up the solving of other similar tasks. This ability for autonomous agent to re-use previous knowledge is known as meta-learning (Mehta et al., 2008;Schmidhuber et al., 1996). The key challenge is for the agent to adapt or generalize to new tasks and new environments that have never been encountered during training time.\n\nThe successor representation (Dayan, 1993) decouples the environment from the reward in the value function computation, in such a way that one remain fixed should the other change. It is thus a natural tool to consider for achieving meta-RL. In fact, several recent works have adopted the successor features approach for meta-RL across tasks that share common dynamics (Barreto et al., 2017;2018;Borsa et al., 2019). These studies are highly promising as they show the ability for autonomous systems to transfer knowledge across tasks. However, they exhibit two major limitations: (i) the learning of the successor features (meta-training) is expensive and (ii) the meta-testing is not sample-efficient, especially for tasks that do not share a common or similar optimal policy. The reason for this is that the learned successor features are heavily dependent on the on-policy experience, requiring a re-training phase for each individual task Lehnert et al. (2017).\nIn this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. The overall goal is to find the optimal balance between datatraining and data-testing, by differentiating the data used in the meta-training with respect to the data used to train the policy. We propose state2vec, an efficient yet reliable framework for learning the successor features. We are interested in learning features that capture the underlying geometry of the state space. In particular, we seek the following properties for the features: (i) to be learned from data rather than handcraftedto avoid structural bias, see Madjiheurem & Toni (2019); (ii) to be low-dimensionalto ensure a fast adaptation during meta-testing; (iii) to be geometry-aware rather than task-awareto generalize across optimal policies. To learn such features, we extend the well known node2vec algorithm (Grover & Leskovec, 2016) to infer graph embeddings capturing temporal dependencies. In other words, state2vec encodes states in low-dimensional embeddings, defining the similarity of states based on the discounted future transitions. Moreover, to ensure offpolicy meta-training, we impose that the data used for training is fully exploratory and dependent on any specific task (it is reward agnostic). This allows us to use the same representation without any retraining of the features to solve tasks with varying reward functions. In the meta-testing phase, the agent will need to simply learn a task-aware coefficient vector to derive a value function approximation. The dimensionality of the coefficient vector is imposed by the embedding dimension, which we constraint to be low to favor sample-efficiency in the meta-testing. We show experimentally that state2vec captures with high accuracy the structural geometry of the environment while remaining reward agnostic. The experiments also support the intuition that off-policy state2vec representations are robust low dimensional basis functions that allow to approximate well the value function."}
{"introduction": "Humans have a remarkable ability to learn motor skills by mimicking behaviors of agents that look and act very differently from them. For example, developmental psychologists has shown that 18-month-old children are able to infer the intentions and imitate behaviors of adults (Meltzoff, 1995). Imitation is not easy: children likely need to infer correspondences between their observations and their internal representations, which effectively aligns the two domains. Learning such a cross-domain correspondence is particularly valuable for robotics and control. For example, in imitation learning, if we want robots to imitate the motor skills of humans (or robots with different morphologies), we need to find the correspondence in both visual observations and morphology dynamics. Similarly, when transferring a policy trained in simulation to a real robot, we, again, need to align visual inputs and physics parameters across different environments.\nTo align the skills across different domains, several prior approaches have proposed learning invariant feature representations across the domains (Gupta et al., 2017;Sermanet et al., 2018). Policies or visual representations are trained to be invariant to the changes which are irrelevant to the downstream task, while maintaining useful information for cross-domain alignment. However, these methods require paired and aligned trajectories, usually collected by pre-trained policies or human labeling, which is often too expensive to collect for real-world learning problems. Additionally, invariance is a rather strong constraint, and might not be universally suitable. The reason is that different invariances might be beneficial for different downstream tasks, which has been recently studied in self-supervised visual representation learning (Tian et al., 2020).  Instead of learning invariances, an emerging line of research focuses on finding correspondences by learning to translate between two different domains with unpaired data (Zhu et al., 2017;Bansal et al., 2018). While this translation technique has shown encouraging results in imitation learning (Smith et al., 2019) and sim-to-real transfer (Hoffman et al., 2017;James et al., 2019), it is limited to finding correspondences only in the visual observation space. However, in real-world applications, besides visual observations, the physics parameters and morphology dynamics between two domains are also often misaligned. Hence, solely learning with passive visual correspondences, one is unable to reason about the effects of dynamics. We must go beyond the image space and explicitly incorporate dynamics information to truly extend correspondence learning to aligning behaviors.\nIn this paper, we take the first steps toward learning correspondences which can align behaviors on a range of domains including different modalities (vision vs. agent state), different physical parameters (friction and mass), and different morphologies. Importantly, we use unpaired and unaligned data from the two domains to learn the correspondences. Specifically, we propose to find observation correspondences and action correspondences at the same time using dynamics cycle-consistency. Our dynamics cycles chain the observations and actions across time and domains together. The consistency in the dynamics cycle indicates consistent translation and prediction results. The input data to our learning algorithm takes the form of 3-element tuples from both domains: current state, action and the next state. Figure 1(a) exemplifies our model, which is a 4-cycle chain containing the observations of one domain (x t , x t+1 ) (real robot in Figure 1(a)) at two time steps, and another domain (y t , y t+1 ) (simulation in Figure 1(a)). To form a cycle, we learn a domain translator G : x t \u2192 y t to translate images to states and a predictive forward dynamics model in state space F : y t \u00d7u t \u2192 y t+1 where u t represents the action taken at time t, and a t is the corresponding action in the real robot domain. The forward model in the real robot domain is not necessary in our framework. The training signal is: given observations in time t, the future prediction in time t + 1 should be consistent under the consistent action taken across two domains, namely dynamics cycle-consistency.\nWe explore applications both in simulation and with a real robot. In simulation, we adopt multiple tasks in the MuJoCo (Todorov et al., 2012) physics engine, and show that our model can find correspondence and align two domains across different modalities, physical parameters (Figure 1(b)), and morphologies (Figure 1(c)). Given the alignment, we can transfer a reinforcement learning (RL) policy trained in one domain directly to another domain without further optimizing the RL objective. For our real robot experiments, we use the xArm Robot (Figure 1(a)). Given only uncalibrated monocular videos of the xArm performing random actions, our method learns correspondences between the real robot and simulated robot without any paired data. At test time, given a video of the robot arm executing a smooth trajectory, we can generate the same trajectory in simulation."}
{"introduction": "Weight-sharing neural architecture search (NAS) methods have achieved state-of-the-art performance while requiring computation training of just a single shared-weights network (Pham et al., 2018;Li and Talwalkar, 2019;Liu et al., 2019). However, weight-sharing remains poorly understood. In this work, we present a novel perspective on weight-sharing NAS motivated by the key observation that these methods subsume the architecture hyperparameters as another set of learned parameters of the shared-weights network, in effect extending the hypothesis class. An important ramification of this insight is that weight-sharing is not NAS-specific and can be used to tune hyperparameters corresponding to parameterized feature maps of the input data. We refer this larger subset of hyperparameter optimization problems as architecture search, and we study the following two questions associated with weight-sharing applied to the architecture search problem:\n\nThe field of neural architecture search traces its origins back to the early 1990s with some of the foundational work done by Schmidhuber and others. In those early days, researchers focused on simple feedforward neural networks trained with backpropagation and experimented with evolutionary algorithms and random search to find good architectures. The field saw a resurgence in the 2010s with the rise of deep learning, as researchers proposed new methods like reinforcement learning and neuroevolution to search over complex spaces of deep convolutional and recurrent neural networks. Some key innovations during this period included Zoph and Le's Neural Architecture Search, which used a recurrent network controller to generate architectures, and Real et al.'s AmoebaNet, which introduced the use of evolutionary algorithms. \n\nAround 2017, researchers proposed one-shot weight-sharing approaches like ENAS and DARTS that allowed efficient search by training a single overparameterized network. These methods helped make NAS practical for large-scale problems like designing CNNs and transformers. Recent work has focused on improving the weight-sharing approach, studying its theoretical properties, and expanding NAS to new domains like on-device search and hardware-aware NAS. The field continues to be an active research area today, with open problems around improving search efficiency, enhancing search spaces, and understanding the theoretical underpinnings.\n\n1. How can we efficiently optimize the objective induced by applying weight sharing to architecture search, namely minimizing empirical risk in the joint space of model and architecture parameters? For large structured search spaces that preclude brute force search, a natural approach to architecture search with weight-sharing is to use gradient-based methods to minimize the empirical risk over a continuous relaxation of the discrete space (Liu et al., 2019). Although this has allowed NAS researchers to apply their preferred optimizers to determine architecture weights, it is far from clear that the success of established methods for unconstrained optimization in training neural networks will naturally extend to these constrained and often non-Euclidean environments. As we foresee that architecture search spaces will continue to become more complex and multi-faceted, we argue for and develop a more principled, geometry-aware formulation of the optimization problem. Drawing upon the mirror descent meta-algorithm (Beck and Teboulle, 2003) and successive convex approximation, we give non-asymptotic stationary-point convergence guarantees for the empirical risk minimization (ERM) objective associated with weight-sharing via algorithms that simultaneously connect to the underlying problem structure and handle the alternating-block nature of the architecture search. Our guarantees inform the design of gradient-based weight-sharing methods by explicitly quantifying the impact of optimizing in the right geometry on convergence rates.\n2. What are the generalization benefits of solving a bilevel optimization for the architecture search problem commonly considered in practice? At its core, the goal of architecture search is to find a configuration that achieves good generalization performance. Consequently, a bilevel objective that optimizes the architecture weights using a separate validation loss is commonly used in practice in lieu of the ERM objective naturally induced by weight sharing (Pham et al., 2018;Liu et al., 2019;Cai et al., 2019). The learning aspects of this approach have generally been studied in settings with much stronger control over the model complexity (Kearns et al., 1997). We provide generalization guarantees for this objective over structured hypothesis spaces associated with a finite set of architectures; this leads to meaningful bounds for simple feature map selection problems as well as insightful results for the NAS problem that depend on the size of the space of global optima.\nTo validate our theoretical results, we conduct empirical studies of weight-sharing in two settings:\n(1) shallow feature map selection, i.e., tuning the hyperparameters of kernel classification and NLP featurization pipelines, and (2) CNN neural architecture search. In (1) we demonstrate that weightsharing efficiently optimizes the bilevel objective and achieves low generalization error with respect to the best architecture setting. For (2), motivated by insights from our convergence analysis, we develop a simple exponentiated gradient version of DARTS (Liu et al., 2019) called EDARTS that better exploits the geometry of the optimization problem. We evaluate EDARTS on the design of CNN architectures for CIFAR-10 and demonstrate that EDARTS finds better architectures than DARTS in less than half the time. We also achieve very competitive results relative to state-of-the-art architectures when using an extended evaluation routine.\nRelated Work: Our work on optimization for weight-sharing benefits from the literature on firstorder stochastic optimization (Hazan and Kale, 2014;Beck, 2017) and in particular the mirror descent framework (Beck and Teboulle, 2003). Specifically, we use successive convex approximation (Razaviyayn et al., 2013;Mairal, 2015) to show convergence of alternating minimization and derive geometry-dependent rates comparable to existing work on non-convex stochastic mirror descent (Dang and Lan, 2015;Zhang and He, 2018). Our result generalizes to the constrained, non-Euclidean, and multi-block setting an approach of Agarwal et al. (2019) for obtaining non-convex convergence from strongly convex minimization, which may be of independent interest. Previous optimization results for NAS have generally only shown bounds on auxiliary quantities such as regret that are not well-connected to the learning objective (Noy et al., 2019;Nayman et al., 2019;Carlucci et al., 2019) or have only given monotonic improvement or asymptotic guarantees (Akimoto et al., 2019;Yao et al., 2019). However, due to the generality of mirror descent, the approaches in the middle three papers can be seen as special cases of our analysis. Finally, our analysis of the properties of the bilevel optimization is related to work on model selection (Vuong, 1989;Kearns et al., 1997), but does not consider the configuration parameters as explicit controls on the model complexity. Our learning results are broadly related to hyperparameter optimization, although most work focuses on algorithmic and not statistical questions (Li et al., 2018;Kandasamy et al., 2017)."}
{"introduction": "Differential planning modules (DPM) is potentially useful to achieve this goal, since it learns to map observation to a planning computation for a task, and generates action predictions based on the resulting plan (Tamar et al., 2016;Nardelli et al., 2019;Zhang et al., 2020). Value iteration networks (VIN) (Tamar et al., 2016) is the representative one, which represents value iteration as a convolutional neural network (CNN). Meaningful reward and value maps have been learned along with the useful planning computation, which leads to policies that generalize well to new tasks.\n\nAs shown in Figure 1, vPERL learns meaningful reward and value maps that attends to the resulting region of the agent executing an action, which indicates meaningful planning computation. However, directly applying VIN in Atari domain in the way of supervised learning (Tamar et al., 2016) only learns reward and value maps that attend no specific region, which usually results in no avail."}
{"introduction": "Wisdom of the crowd harnesses the power of aggregated opinion of a diverse group rather than a few individuals. Though initially proposed for mainly aggregating human judgements, this idea has been successfully implemented in the context of machine learning. In particular, ensemble learning was proposed and studied to improve prediction performance by combining several learning models to obtain better results compared to a single one (Dietterich, 2000). The developed ensemble techniques have shown consistent benefits in real-world machine learning applications, evidenced by the Netflix Competition (Bennett et al., 2007) and Kaggle competition. Popular ensemble methods include Boosting (e.g., AdaBoost (Freund & Schapire, 1997)), Bootstrap aggregating (bagging), Stacking (Bishop, 2006), andRandom Forest (Ho, 1995).\n\nThe most popular, as well as simple, way to perform aggregation is via majority voting rule. The classical example is Random Forest, which outputs the majority answer from multiple trained decision trees. Inference methods (Raykar et al., 2010;Zhang et al., 2014;Liu et al., 2012;Zhou et al., 2012;2014) have been applied to perform smarter aggregation that aims to outperform majority-voted answers. These methods often leverage homogeneous assumption of certain hidden models over a large number of data points in order to perform joint inference. Nonetheless, all above methods rely on the assumption that the majority answer is more likely to be correct -this is also true for the more sophisticated inference models, as the inferences will mostly likely initiate based on majority-voted answers (when the algorithm has no prior information). While enjoying this assumption that majority tends to be correct, this claim is questionable in settings where special knowledge is needed to infer the truth, but it is owned by few individuals when they are not widely shared (Chen et al., 2004;Simmons et al., 2010;Prelec et al., 2017). Echoing to the above problem of aggregating human judgements, we face similar challenge when aggregating classifiers' predictions in machine learning. For example, we have a deep learning (Goodfellow et al., 2016) classification model which performs the best among multiple models when used in the ensemble method. For some data point, the classification result of this deep learning model may be the correct minority. In this situation, applying majority voting leads to wrong answers.\nWe aim to complement the literature via studying whether we can aggregate classifiers better than majority voting even when majority opinion is wrong. We also target a method that can operate over each data point separately without assuming homogeneous assumptions across a massive dataset. \nThe question sounds unlikely to resolve at a first look, but we are inspired by the seminal work Bayesian Truth Serum (BTS) (Prelec, 2004;Prelec et al., 2017) which approached this question in the setting of incentivizing and aggregating truthful human judgements. The core idea behind BTS is simple and elegant: the correctness of an answer does not rely on its popularity, but rather whether it is \"surprisingly\" popular or not -here an answer that has a higher posterior (computed from reports of the crowds) than its prior is taken as being \"surprisingly\" popular, and should be considered as the true answer. This argument has a very intuitive Bayesian reasoning: the signal that improves over its prior is more likely to be informative. Prelec et al. (2017) also argued that via eliciting a peer prediction information, which is defined as the fraction of \"how many other people would agree with you\" from each agent, he will be able to construct an informative prior to compare with the majority vote posterior aggregation. BTS operates over each single question separately, without seeing a large number of similar tasks (in order to leverage a certain homogeneity assumption).\nIn this paper, we make a connection between these two seemingly irrelevant topics, and extend the key idea in Bayesian Truth Serum to aggregating classifiers' predictions. The challenge is that we would not be able to elicit a belief from a classifier on \"how many other classifiers would agree with themselves\", which renders the task of computing the prior difficult. We proposed two machine learning aided algorithms to mimic the procedure of reporting the peer prediction information, which we jointly name as Machine Truth Serum (MTS). We firstly propose Heuristic Machine Truth Serum (HMTS). In HMTS, we pair each baseline classifier (an agent) with a regressor model, which is trained to predict the peer prediction information using a processed training dataset. With the predictions from the regressors, we will be able to apply the idea of BTS to decide on whether to adopt the minority as the answer via comparing the prior (computed using the regressor) and the posterior for each label. Then we proposed Discriminative Machine Truth Serum (DMTS). In DMTS, we directly train one classifier to predict whether adopting the minority as the answer or not. As for the training complexity of our algorithm, the training time of HMTS is linear in the number of label classes because of the training of extra regressors. DMTS will only need to train one additional classifier and both the training and the running time are almost the same as the basic majority voting algorithm. Therefore our proposed methods are very practical to implement and run.\nOur contributions summarize as follows: (1) We propose Heuristic Machine Truth Serum (HMTS) and Discriminative Machine Truth Serum (DMTS) to complement ensemble methods, which can detect when minority should be considered the final prediction instead of the majority. (2) Our experiments over 6 binary and 6 multiclass classification real-world datasets reveal promising results of our approach in improving over majority voting. Our proposed methods also outperform popular ensemble algorithms. (3) To pair with our experimental results, we also provide analytical evidences for the correctness of our proposed approaches. (4) Our approaches can be generically applied in ensemble methods to replace simple majority voting rules.\nThe rest of the paper is organized as follows. Section 2 introduces some related works. Section 3 reviews preliminaries and BTS. Section 4 introduces our Machine Truth Serum approaches. Section 5 presents our experimental results. Section 6 concludes our paper."}
{"introduction": "Generative adversarial training (GAT) (Yin et al., 2020) is a recently introduced defense mechanism that could be used for adversarial example detection and robust classification. The defense consists of a committee of detectors (binary discriminators), with each one trained to discriminate natural data of a particular class from adversarial examples perturbed from data of other classes. Like most other work in the area of robust machine learning, the defense is specially designed for defending against norm-constrained adversaries -adversaries that are constrained to perturb the data up to a certain amount as measured by some norm. The defense's robustness is achieved by training each detector model against adversarial examples produced by the norm-constrained PGD attack (Madry et al., 2017).  \n\nExisting work: training and evaluating robust predictive models A detector trained with GAT has strong interpretability -an unbounded attack that maximizes the detector's output results in images that resemble the target class data -this suggests the detector has learned the target class data distribution. However, all previous works (Yin et al., 2020;Tramer et al., 2020) focus on the empirical evaluations of GAT's application to training robust predictive models; a theoretical understanding of why this training method causes the detector to learn the data distribution is missing.\n\nThis work: theoretical understanding, improved training algorithm, and extended applications In order to better understand the GAT method, we first analyze the optimal solutions of the training objective. We start with a maximin formulation (eq. ( 5)) of the objective, and try to connect it with the minimax formulation (eq. ( 1)) that is employed by GANs (Goodfellow et al., 2014). We find that the differences between solutions of these two formulations become immediately clear when we take a game-theory perspective. We then use theoretical analysis and 2D simulations to understand the convergence property of the GAT training algorithm. Building upon these theoretical and experimental insights, we develop an unconstrained GAT algorithm, and apply it to the tasks of generative modeling and out-of-distribution detection. We find the maximin-based generative model to be more stable to train than its minimax counterpart (GANs), and at the same time more flexible as it does not have a fixed generator and can transform arbitrary inputs to the target distribution data, which might be particularly useful for certain applications (e.g., face manipulation). The model trained with the unconstrained GAT algorithm also outperforms several state-of-the-art methods on the task of adversarial out-of-distribution detection. In summary, our key contributions are:\n\n\u2022 We analyze the optimal solutions of the GAT objective and convergence property of the training algorithm. We discuss the implications of these results on improved training of robust predictive models, generative modeling, and out-of-distribution detection. \n\n\u2022 We develop an unconstrained generative adversarial training algorithm. We conduct a comprehensive evaluation of the algorithm's application to image generation and adversarial out-of-distribution detection.\n\n\u2022 Our comparative analysis of the maximin and minimax problem clarifies misconceptions and provides new insights into how they could be utilized to solve different problems."}
{"introduction": "State-of-the art modeling of complex physical systems, such as deforming surfaces and volumes, often employs mesh representations to solve the underlying partial differential equations (PDEs). Mesh-based finite element simulations underpin popular methods in structural mechanics [31,48], aerodynamics [13,34], electromagnetics [32], geophysics [35,39], and acoustics [26]. Meshes also support adaptive representations, which enables optimal use of the resource budget by allocating greater resolution to regions of the simulation domain where strong gradients are expected or more accuracy is required, such as the tip of an airfoil in an aerodynamics simulation. Adaptive meshing enables running simulations at accuracy and resolution levels impossible with regular discretization schemes [8,27] (Figure 3b).\n\nTogether, our method allows us to learn the dynamics of physical systems directly from data, providing only very general biases such as spatial equivariance. We demonstrate that by using mesh-space computation we can reliably model materials with a rest state such as elastics, which are challenging for prediction models [37]. MESHGRAPHNETS outperform particle-and grid-based baselines, and can generalize to more complex dynamics than those on which it was trained."}
{"introduction": "Deep reinforcement learning has achieved remarkable successes in a variety of tasks (Mnih et al., 2015; Morav\u010d\u00edk et al., 2017; Silver et al., 2017; Abreu et al., 2019), but its impressive performance is mirrored by its brittleness and sensitivity to seemingly innocuous design choices (Henderson et al., 2018). In sparse-reward environments in particular, even different random seeds of the same algorithm can attain dramatically different performance outcomes. \n\nThe principal thesis of this paper is that over the course of training, deep RL agents lose some of their capacity to quickly fit new prediction tasks, and in extreme cases this capacity loss prevents the agent entirely from making learning progress. We present an empirical analysis of this phenomenon which considers both the ability of networks to learn new target functions via gradientbased optimization methods, and their ability to linearly disentangle states' feature representations. We find that agents' ability to fit new target functions declines over the course of training in several environments from the Atari suite (Bellemare et al., 2013) and non-stationary reward prediction tasks. We further find that the ability of representations to linearly distinguish different states quickly diminishes in sparse-reward environments, leading to representation collapse, where the feature outputs for every state in the environment inhabit a low-dimensional -or possibly even zero -subspace. \n\nFinally, we propose a simple regularization technique, Initial Feature Regularization (InFeR), to prevent representation collapse by regressing a set of auxiliary outputs towards their value under the network's initial parameters. We show that this regularization scheme enables significant performance improvements in a number of RL tasks.\n\nOne take-away from our results is that agents trained on so-called 'hard exploration' games can attain significant improvements over existing baselines without using smart exploration algorithms, given a suitable representation learning objective. This suggests that the poor performance of deep RL agents in sparse-reward environments is not solely due to inadequate exploration, but rather also in part due to poor representation learning. Investigation into the interplay between representation learning and exploration in sparse-reward settings thus presents a promising direction for future work."}
{"introduction": "Pretraining a model on a large dataset before transferring to a downstream task's training data substantially improves accuracy over training from scratch-for example, pretraining a ResNet-50 on unlabeled ImageNet boosts accuracy on CIFAR-10 from 94% to 98% (Chen et al., 2020a;b). High-stakes applications such as poverty mapping in under-resourced countries (Jean et al., 2016), self-driving cars (Yu et al., 2020), and medical diagnosis (AlBadawy et al., 2018), require models that also generalize to circumstances not seen in the training distribution. In addition to testing on data drawn from the downstream task's training distribution (in-distribution; ID), it is increasingly important to test on data distributions unseen during training (out-of-distribution; OOD). The field of machine learning has seen great advances in recent years, with new models achieving state-of-the-art results on a variety of tasks. However, there are still many open challenges, such as developing models that can generalize well to new distributions. The history of machine learning stretches back to the early days of artificial intelligence research in the 1950s. Influential early work includes Rosenblatt's perceptron algorithm, Widrow and Hoff's Adaline, and Amari's formulation of backpropagation. Machine learning gained popularity in the 1980s and 1990s with the development of neural networks, support vector machines, decision trees, and Bayesian networks. The rise of big data and increased computing power in the 2000s enabled the development of deep learning, which led to breakthroughs in computer vision, natural language processing, and other domains. Exciting new directions in machine learning include generative models, reinforcement learning, graph neural networks, and neuro-symbolic AI.\n\nAfter initializing with a pretrained model, two popular transfer methods are fine-tuning (running gradient descent on all the model parameters), and linear probing (tuning the head but freezing lower layers). In the ID setting it is well known that fine-tuning leads to better accuracy than linear probing (Kornblith et al., 2019;Zhai et al., 2020;He et al., 2020), and even when testing OOD, prior work usually fine-tunes all parameters of their model (Hendrycks et al., 2019a;Miller et al., 2021;Andreassen et al., 2021). Intuitively, fine-tuning all layers of a network can improve pretrained features by adapting them to the specific task, while linear probing freezes these features.\n\nIn this work, we investigate the OOD accuracy of fine-tuning and linear probing and find that surprisingly, fine-tuning can do worse than linear probing in the presence of a large distribution shift. We experiment on ten distribution shift benchmarks (BREEDS Living17, BREEDS Entity30, Do-mainNet, CIFAR \u2192 STL, CIFAR10.1, FMoW Geo-shift, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), initializing with good pretrained features from MoCo-v2 (Chen et al., 2020b) and CLIP (Radford et al., 2021). While both methods offer gains over training from scratch, fine-tuning improves the average ID accuracy relative to linear probing from 83% to 85% but brings down the OOD accuracy from 66% to 59% (Figure 1).\nWhen and why does fine-tuning underperform linear probing? We theoretically consider fine-tuning a two-layer linear network in an overparameterized regression setting where the feature extractor layer has been pretrained to map high-dimensional inputs to useful, lower-dimensional, features. We prove that fine-tuning is worse than linear probing on directions outside the span of the training data when using \"good\" pretrained features. Even with an infinitesimally small learning rate, fine-tuning distorts pretrained features-the features of ID training data are updated while those of OOD data Figure 1: Given a good feature extractor (top-left), a randomly initialized head is added to map features to outputs and we can (a) fine-tune all the model parameters or (b) linear probe, which freezes the feature extractor and trains only the head. We run experiments on ten distribution shifts. Fine-tuning does well when the test example is sampled from the fine-tuning distribution (ID), but can underperform on test examples sampled from OOD distributions (when the distribution shift is large). (c) Our theory indicates that fine-tuning can distort the pretrained feature extractor and lead to poor OOD accuracy, but initializing with a linear probed head can fix this-empirically LP-FT gets better accuracies both ID and OOD. change less. Since the head and feature extractor are simultaneously optimized during fine-tuning to a configuration that works well on ID training data, the head only accomodates the distorted features of ID points and performs poorly (relative to linear probing) on the less changed features of OOD points. Interestingly, we show that this feature distortion issue cannot be simply fixed by early stopping-throughout the entire process of fine-tuning, we never pass through parameters that do well OOD (relative to linear probing). On the other hand, given \"good\" features, linear probing extrapolates better OOD because it preserves pretrained features, but does worse than fine-tuning ID because linear probing cannot adapt the features to the downstream task.\nTechnical challenges. Existing theoretical work on transfer learning focuses on linear probing (Wu et al., 2020;Tripuraneni et al., 2020;Du et al., 2020). In contrast, analyses of fine-tuning is scarce and challenging because it requires understanding the training dynamics, instead of only the loss function and its global minimizers. In fact, fine-tuning and training from scratch optimize the same training loss and only differ in their initializations (pretrained vs random). A mathematical analysis that distinguishes them needs to capture properties of the different minima that these algorithms converge to, a phenomenon that is sometimes theoretically referred to as the implicit regularization effect of initialization (Neyshabur et al., 2014). Accordingly, our analysis reasons about the parameters that gradient methods pass through starting from the pretrained initialization, which is challenging because this is a non-convex optimization problem and there is no known closed form for this trajectory. Two-layer linear networks are widely studied in the literature on implicit regularization (Saxe et al., 2014;Gunasekar et al., 2017;Gidel et al., 2019;Arora et al., 2018). However, they analyze random and often small initializations, which don't capture pretraining.\nAlgorithmic implications. Our theory shows that fine-tuning underpeforms because when trying to fit ID training data with a randomly initialized head, the feature extractor changes significantly for ID examples, making features for ID and OOD examples largely inconsistent. This can be fixed by initializing with a good head that does not need to be updated much during fine-tuning, reducing how much the feature extractor changes. This suggests a simple two-step strategy of first linear probing to find a good head and then full fine-tuning (LP-FT). Empirically, LP-FT outperforms fine-tuning and linear probing, both ID and OOD. Even on CIFAR-10.1 (small distribution shift), where fine-tuning is better for both ID and OOD, we find LP-FT outperforms fine-tuning on both metrics. LP-FT and vanilla fine-tuning use similar amounts of compute because the first step of linear probing is relatively very cheap. Prior work has used LP-FT (Levine et al., 2016;Kanavati & Tsuneki, 2021) (or variants such as layerwise fine-tuning (Howard & Ruder, 2018) or larger learning rates for the head layer (Prabhu et al., 2021))-however it has not been used for robustness / OOD accuracy, and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Note that LP-FT is not meant to be a SOTA method but rather a simple, principled way to get good ID and OOD accuracy-we hope our analysis inspires even better methods for robust fine-tuning.\nEmpirical validation. Finally, we find that fine-tuning fails and LP-FT works, for the reasons predicted by our feature distortion theory: (1) fine-tuning changes the features for ID examples more than for OOD examples, leading to distortions; (2) LP-FT indeed changes both ID and OOD features 10-100\u00d7 less than fine-tuning does; (3) LP-FT gets the best of both worlds, achieving better accuracies than fine-tuning and linear probing both ID and OOD (Figure 1)."}
{"introduction": "A primary goal for AI research is to develop agents that can act optimally in real-world multi-agent interactions. In recent years, progress has been made in developing AI agents for games. In this paper we augment neural policies trained through imitation learning with regret minimization search techniques, and evaluate on the benchmark game of no-press Diplomacy. Diplomacy is a longstanding benchmark for research that features a rich mixture of cooperation and competition. Like previous researchers, we evaluate on the widely played no-press variant of Diplomacy, in which communication can only occur through the actions in the game (i.e., no cheap talk is allowed). Specifically, we begin with a blueprint policy that approximates human play in a dataset of Diplomacy games. We then improve upon the blueprint during play by approximating an equilibrium for the current phase of the game, assuming all players (including our agent) play the blueprint for the remainder of the game. Our agent then plays its part of the computed equilibrium. The equilibrium is computed via regret matching (RM).\nSome prior work has been done on Diplomacy agents. Nevertheless, we show that our agent exceeds the performance of prior agents and for the first time convincingly achieves human-level performance in no-press Diplomacy. Specifically, we show that our agent soundly defeats previous agents, that our agent is far less exploitable than previous agents, that an expert human cannot exploit our agent even in repeated play, and, most importantly, that our agent ranks in the top 2% of human players when playing anonymous games on a popular Diplomacy website."}
{"introduction": "Distributed or federated machine learning, where the data is distributed across multiple workers, has become an increasingly important learning paradigm both due to growing sizes of datasets, as well as data privacy concerns. In such a setting, the workers collaborate to train a single model without directly transmitting their training data (McMahan et al., 2016;Bonawitz et al., 2019;Kairouz et al., 2019). However, by decentralizing the training across a vast number of workers we potentially open ourselves to new security threats. Due to the presence of agents in the network which are actively malicious, or simply due to system and network failures, some workers may disobey the protocols and send arbitrary messages; such workers are also known as Byzantine workers (Lamport et al., 2019). Byzantine robust optimization algorithms attempt to combine the updates received from the workers using robust aggregation rules and ensure that the training is not impacted by the presence of a small number of malicious workers. The Byzantine Generals Problem is a thought experiment meant to illustrate the difficulties of reaching agreement in the presence of communication faults. It was originally formulated in 1982 by Leslie Lamport, Robert Shostak, and Marshall Pease. The scenario imagines a group of generals commanding portions of the Byzantine army camped outside an enemy city. The generals must agree on a common battle plan. However, some of them may be traitors, trying to prevent the loyal generals from reaching agreement. The generals communicate only by messenger. The problem asks how many traitors can the loyal generals tolerate, and still reach an agreement. This scenario illustrates the difficulty of securing a computer network, where messages may fail to reach their destinations or may be corrupted along the way. Just like the Byzantine generals, nodes in a network need to reach consensus despite potentially malicious actors trying to disrupt communication.\n\nWhile this problem has received significant recent attention due to its importance, (Blanchard et al., 2017;Yin et al., 2018;Alistarh et al., 2018;Karimireddy et al., 2021), most of the current approaches assume that the data present on each different worker has identical distribution. This assumption is very unrealistic in practice and heterogeneity is inherent in distributed and federated learning (Kairouz et al., 2019). In this work, we show that existing Byzantine aggregation rules catastrophically fail with very simple attacks (or sometimes even with no attacks) in realistic settings. We carefully examine the causes of these failures, and propose a simple solution which provably solves the Byzantine resilient optimization problem under heterogeneous workers.\nConcretely, our contributions in this work are summarized below \u2022 We show that when the data across workers is heterogeneous, existing aggregation rules fail to converge, even when no Byzantine adversaries are present. We also propose a simple new attack, mimic, which explicitly takes advantage of data heterogeneity and circumvents median-based defenses. Together, these highlight the fragility of existing methods in real world settings. \u2022 We then propose a simple fix -a new bucketing step which can be used before any existing aggregation rule. We introduce a formal notion of a robust aggregator (ARAGG) and prove that existing methods like KRUM, coordinate-wise median (CM), and geometric median aka robust federated averaging (RFA)-though insufficient on their own-become provably robust aggregators when augmented with our bucketing. \u2022 We combine our notion of robust aggregator (ARAGG) with worker momentum to obtain optimal rates for Byzantine robust optimization with matching lower bounds. Unfortunately, our lower bounds imply that convergence to an exact optimum may not be possible due to heterogeneity.\nWe then circumvent this lower bound and show that when heterogeneity is mild (or when the model is overparameterized), we can in fact converge to an exact optimum. This is the first result establishing convergence to the optimum for heterogeneous Byzantine robust optimization. \u2022 Finally, we evaluate the effect of the proposed techniques (bucketing and worker momentum) against known and new attacks showcasing drastic improvement on realistic heterogeneously distributed datasets.\nSetup and notations. Suppose that of the total n workers, the set of good workers is denoted by G \u2286 {1, . . . , n}. Our objective is to minimize\nwhere f i is the loss function on worker i defined over its own (heterogeneous) data distribution \u03be i . The (stochastic) gradient computed by a good worker i \u2208 G over minibatch \u03be i is given as\nThe noise in every stochastic gradient is independent, unbiased with E \u03bei [g i (x, \u03be i )] = \u2207f i (x), and has bounded variance E \u03bei g i (x, \u03be i ) -\u2207f i (x) 2 \u2264 \u03c3 2 . Further, we assume that the data heterogeneity across the workers can be bounded as\nWe write g t i or simply g i instead of g i (x t , \u03be t i ) when there is no ambiguity. Byzantine attack model. The set of Byzantine workers B \u2282 [n] is fixed over time, with the remaining workers G being good, i.e.\n[n] = B G. We write \u03b4 for the fraction of Byzantine workers, |B| =: q \u2264 \u03b4n. The Byzantine workers can deviate arbitrarily from our protocol, sending any update to the server. Further, they can collude and may even know the states of all other workers.\nOur modeling assumes that the practitioner picks a value of \u03b4 \u2208 [0, 0.5). This \u03b4 reflects the level of robustness required. A choice of a large \u03b4 (say near 0.5) would mean that the system is very robust and can tolerate a large fraction of attackers, but the algorithm becomes much more conservative and slow. On the flip side, if the practitioner knows that the the number of Byzantine agents are going to be few, they can pick a small \u03b4 (say 0.05-0.1) ensuring some robustness with almost no impact on convergence. The choice of \u03b4 can also be formulated as how expensive do we want to make an attack? To carry out a succesful attack the attacker would need to control \u03b4 fraction of all workers. We recommend implementations claiming robustness be transparent about their choice of \u03b4."}
{"introduction": "The robustness of machine learning (ML) systems has been challenged by test-time attacks using adversarial examples (Szegedy et al., 2013). These adversarial examples are intentionally manipulated inputs that preserve the essential characteristics of the original inputs, and thus are expected to have the same test outcome as the originals by human standard; yet they severely affect the performance of many ML models across different domains (Moosavi-Dezfooli et al., 2016;Eykholt et al., 2018;Qin et al., 2019). As models in high-stake domains such as system security are also undermined by attacks (Grosse et al., 2017;Rosenberg et al., 2018;Hu & Tan, 2018), robust ML in adversarial test environment becomes an imperative task for the ML community.\n\nIn addition to security threats, another rising concern on ML models is the spurious correlations they could have learned in a biased data set. Ribeiro et al. (2016) show that a highly accurate wolf-vshusky-dog classifier indeed bases its prediction on the presence/absence of snow in the background. A reliable model, in contrast, should be robust to changes of this nature. Although dubbed as semantic perturbation or manipulation (Mohapatra et al., 2020;Bhattad et al., 2019), these changes do not alter the core of the semantics of input data, thus, we still consider them to be semantics-preserving pertaining to the classification task. Since such semantics-preserving changes often resulted in large \ufffd p -norms, they are likely to render the existing \ufffd p -norm based defenses ineffective.\n\nFrom the defense perspective, recent work has started to look beyond \ufffd p -norm constraints, including adversarial training (Grosse et al., 2017;Rosenberg et al., 2019;Lei et al., 2019), verificationloss regularization (Huang et al., 2019) and invariance-induced regularization (Yang et al., 2019). Adversarial training in principle can achieve high robust accuracy when the adversarial example in the training loop maximizes the loss. However, finding such adversarial examples is in general NPhard (Katz et al., 2017), and we show in Sec 4 that it is even PSPACE-hard for semantics-preserving attacks that are considered in this paper. Huang et al. (2019) and Yang et al. (2019) add regularizers that incorporate model robustness as part of the training objective. However, such regularization can not be strictly enforced in training, and neither can the model robustness. These limitations still cause vulnerability to semantics-preserving attacks.\n\nNormalize-and-Predict Learning Framework This paper attempts to overcome the limitations of prior work by introducing a learning framework that guarantees robustness by design. In particular, we target a relational adversary, whose admissible manipulation is specified by a logical relation. A logical relation is a set of input pairs, each of which consists of a source and target of an atomic, semantics-preserving transformation. We consider a strong adversary who can apply an arbitrary number of transformations. Our paper makes the following contribution towards the theoretical understanding of robust ML against relational adversaries:\n\n1. We formally describe admissible adversarial manipulation using logical relations, and characterize the necessary and sufficient conditions for robustness to relational adversaries.\n\n2. We propose normalize-and-predict (hereinafter abbreviated as N&P), a learning framework that first converts each data input to a well-defined and unique normal form and then trains and classifies over the normalized inputs. We show that our framework has guaranteed robustness, and characterize conditions to different levels of robustness-accuracy trade-off.\n\n3. We compare N&P to the popular adversarial training framework, which directly optimizes for accuracy under attacks. We show that N&P has the advantage in terms of explicit robustness guarantee and reduced training complexity, and in certain cases yields the same model accuracy as adversarial training. Motivated by the comparison, we propose a unified framework, which selectively normalizes over relations that tend to preserve the model accuracy and adversarially trains over the rest. Our unified approach gets the benefits from both frameworks.\n\nWe then apply our theoretical findings to malware detection and image classification. For the former, first, we formulate two types of common program transformation -(1) addition of redundant libraries and API calls, and (2) substitution of equivalent API calls -as logical relations. Next, we instantiate our learning framework to these relations, and propose two generic relational adversarial attacks to determine the robustness of a model. Finally, we perform experiments over Sleipnir, a real-world WIN32 malware data set. Regarding image classification, we reused an attack method proposed by the prior work (Hosseini & Poovendran, 2018) -shifting of the hue in the HSV color space -that can be deemed as a specific instantiation of our attack framework. We then compare the accuracy and robustness of ResNet-32 (He et al., 2016), a common image classification model, trained with the unified framework against the standard adversarial training on CIFAR-10 ( Krizhevsky et al., 2009). \n\nThe results we obtained in both tasks show that:\n\n1. Attacks using addition and substitution suffice to evade existing ML malware detectors.\n\n2. Our unified approach using input normalization and adversarial training achieves highest robust accuracy among all baselines in malware detection. The drop in accuracy on clean inputs is small and the computation cost is lower than pure adversarial training.   \n\n3. When trained with the unified learning framework, ResNet-32 achieves similar clean accuracy but significantly higher robust accuracy than adversarial training alone.\n\nFinally, based on our theoretical and empirical results, we conclude that input normalization is vital to robust learning against relational adversaries. We believe techniques that can improve the quality of normalization are promising directions for future work."}
{"introduction": "Language modeling using Transformers (Vaswani et al., 2017) has been hugely successful for applications like translation and text generation. Models like GPT are able to generate news articles and stories given just an abstract (Radford et al., 2018). These models are usually (pre-)trained on a proxy task, such as predicting missing words in the case of BERT (Devlin et al., 2019), before fine tuning the models on more specific (downstream) tasks such as machine translation and questionanswering. These proxy tasks are not reliant on labels, and thus can be trained on large corpora of unlabeled data. Recently, however, we have seen successful demonstrations of language modeling using only self-supervised training without any fine tuning (Brown et al., 2020).\n\nThe field of natural language processing has seen immense growth and progress over the past decade. With the advent of deep learning, neural network architectures like recurrent neural networks and convolutional neural networks have achieved state-of-the-art results on many NLP tasks. Word embeddings like word2vec and GloVe have enabled powerful semantic representations of words and phrases. Meanwhile, large pretrained language models like ELMo, BERT and GPT-2 have set benchmarks on question answering, textual entailment, sentiment analysis, and more. The success of transfer learning has allowed models trained on huge datasets to be fine-tuned for specialized tasks with limited labeled data. Trends like attention mechanisms, Transformer architectures, and self-supervised learning have pushed the field rapidly forward.\n\nIn this work, we extend this line of thought and demonstrate that purely self-supervised training can even lead to mathematical reasoning abilities. This represents a major departure from prior work in deep learning for mathematics, which has focused on learning directly on logical reasoning tasks, such as predicting the proof steps or premises or assignments. These approaches require labeled data, which is hard to come by and typically very limited in size. In contrast, our language modeling approach to mathematics allows us to train on unlabeled mathematical expressions. We start with the HOList dataset (Bansal et al., 2019), which spans a wide range of mathematical topics, including topology, multivariate calculus, real and complex analysis, geometric algebra, and measure theory, formalized in the HOL Light proof assistant (Harrison, 1996).\nWe find that training a language model on all mathematical expressions in this dataset leads to surprisingly strong mathematical reasoning capabilities. We believe that this opens the door to different kinds of neural theorem provers, which do not only search through a well-defined search space of tactics and premises, but which are capable to generating their own lemmas and could even come up with a new Ansatz requiring a creative substitution.\nFor self-supervised training on mathematical expressions, we propose a novel skip-tree task, which is a specialization of the skip-sequence task that respects the tree structure of expressions. We show that models trained on the skip-tree task significantly outperform those trained on the skip-sequence task, which is the state of the art for sequence to sequence models for natural language.\nMost previous works that apply sequence-to-sequence models to logics have focused on specific logical tasks in supervised training settings (e.g. Piotrowski and Urban (2020a)). In contrast, we train language models on a self-supervised proxy task that does not require labeled data and can thus be applied to almost any source of mathematical expressions. Lample and Charton (2020) use a Transformer model for symbolic integration. They train their model directly on the the task to produce the integral of a given expression. To generate training data, their approach needs a classical algorithm to compute the derivative of the expressions. Finkbeiner et al. (2020) explore the generalization properties of the Transformer architecture predicting the solutions to SAT formulas and temporal logic, but require a data generator that can solve formulas, which is currently not feasible for higher-order logic. Piotrowski et al. (2019) train RNNs on individual logical reasoning steps, such as substitutions, using a dataset of rewrites on polynomials extracted from Prover9. Wang et al. (2018) translate between synthetic descriptions in natural language and formal mathematics on a dataset generated with Mizar.\nSelf-supervised training techniques for formal mathematics have received much less attention. Wang et al. (2020) apply recent self-supervised translation techniques by Lample et al. (2018) to align formal and informal statements. Very recently, Li et al. (2020) and Polu and Sutskever (2020) applied language modeling to proofs of formal mathematics. In contrast, this work focuses on measuring reasoning abilities on mathematical statements (not necessarily proofs) achieved through self-supervised training only. Independently from our work, Urban and Jakub\u016fv (2020) presented initial experiments on applying self-supervised language modeling to formal mathematics in order to produce conjectures. However, they only evaluate the learned models through the truth of the produced conjectures, while we also consider several reasoning tasks and measure the usefulness of conjectures. Earlier methods to produce conjectures were limited in scope. For example, Piotrowski and Urban (2020b) propose a method to predict the next literal in an automated theorem prover using recurrent neural networks after supervised training. Prior to that Gauthier et al. (2016) relied only on statistical approaches to produce conjectures. Applying natural language techniques to formal mathematics has a long history. Already in 2004, Cairns (2004) applied information retrieval based on latent semantics to improve over search for keywords, and Urban (2004) formulated the intention to learn from large amounts of data in formalized mathematics.\nTransformer models for program understanding have focused on providing inductive biases in the architecture (Shiv and Quirk, 2019;Hellendoorn et al., 2020), whereas this work suggests to use a modified language modeling proxy task."}
{"introduction": "The record-breaking performance of modern deep neural networks (DNNs) comes at a prohibitive training cost due to the required massive training data and parameters, limiting the development of the highly demanded DNN-powered intelligent solutions for numerous applications (Liu et al., 2018;Wu et al., 2018). As an illustration, training ResNet-50 involves 10\n18 FLOPs (floating-point operations) and can take 14 days on one state-of-the-art (SOTA) GPU (You et al., 2020b). Meanwhile, the large DNN training costs have raised increasing financial and environmental concerns. For example, it is estimated that training one DNN can cost more than $10K US dollars and emit carbon as high as a car's lifetime emissions. In parallel, recent DNN advances have fueled a tremendous need for intelligent edge devices, many of which require on-device in-situ learning to ensure the accuracy under dynamic real-world environments, where there is a mismatch between the devices' limited resources and the prohibitive training costs (Wang et al., 2019b;Li et al., 2020;You et al., 2020a).\nTo address the aforementioned challenges, extensive research efforts have been devoted to developing efficient DNN training techniques. Among them, low-precision training has gained significant attention as it can largely boost the training time/energy efficiency (Jacob et al., 2018;Wang et al., 2018a;Sun et al., 2019). For instance, GPUs can now perform mixed-precision DNN training with 16-bit IEEE Half-Precision floating-point formats (Micikevicius et al., 2017b). Despite their promise, existing low-precision works have not yet fully explored the opportunity of leveraging recent findings in understanding DNN training. In particular, existing works mostly fix the model precision during the whole training process, i.e., adopt a static quantization strategy, while recent works in DNN training optimization suggest dynamic hyper-parameters along DNNs' training trajectory. For example, the first deep neural network was developed in the 1950s called the perceptron. It was limited to single layer networks. In the 1980s, research explored multi-layer neural networks with the backpropagation algorithm. However, these early neural networks were limited by the available computational power at the time. In the 1990s and 2000s, researchers developed convolutional neural networks for image recognition tasks. The AlexNet architecture in 2012 sparked renewed interest in deep learning by outperforming other approaches on the ImageNet competition. From 2012 to now, deep learning techniques have rapidly advanced with new architectures, optimization techniques, and applications. Deep learning is now utilized in computer vision, natural language processing, speech recognition, robotics, and many other domains."}
{"introduction": "Reinforcement learning (RL) searches for a policy that maximizes the expected, cumulative reward. In fully observed Markov decision processes (MDPs), this maximization always has a deterministic policy as a solution. Maximum entropy reinforcement learning (MaxEnt RL) is a modification of the RL objective that further adds an entropy term to the objective. This additional entropy term causes MaxEnt RL to seek policies that (1) are stochastic, and (2) have non-zero probability of sampling every action. MaxEnt RL can equivalently be viewed as probability matching between trajectories visited by the policy and a distribution defined by exponentiating the reward (See Section 2). The field of reinforcement learning has its origins in animal psychology, control theory, operations research, information theory and statistics. Early work in cybernetics and control theory in the 1950s and 60s contributed core ideas such as the perception-action cycle and the explore/exploit dilemma. In the 1970s and 80s, reinforcement learning started to take shape as a field in its own right, building on foundational work in animal learning psychology. Key concepts such as the Bellman equation, dynamic programming and temporal difference learning emerged during this period. The field gained significant momentum in the 1990s with important theoretical developments in Markov decision processes, function approximation and policy gradient methods. Recent years have seen remarkable progress in deep reinforcement learning, with algorithms able to master complex environments like Atari games and Go through self-play. However, fundamental challenges remain in scaling up to more complex real-world tasks, exploring safely and efficiently, transferring learned skills to new environments, and building agents that can learn with minimal reward signals. The long history of reinforcement learning draws on diverse intellectual traditions, and the field continues to rapidly evolve by integrating new ideas and perspectives.\n\nMaxEnt RL has appealing connections to probabilistic inference (Dayan & Hinton, 1997;Neumann et al., 2011;Todorov, 2007;Kappen, 2005;Toussaint, 2009;Rawlik et al., 2013;Theodorou et al., 2010;Ziebart, 2010), prompting a renewed interest in recent years (Haarnoja et al., 2018b;Abdolmaleki et al., 2018;Levine, 2018). MaxEnt RL can also be viewed as using Thompson sampling (Thompson, 1933) to collect trajectories, where the posterior belief is given by the exponentiated return. Empirically, MaxEnt RL algorithms achieve good performance on a number of simulated (Haarnoja et al., 2018b) and real-world (Haarnoja et al., 2018a;Singh et al., 2019) control tasks, and can be more robust to perturbations (Haarnoja et al., 2018c).\nThere is empirical evidence that behavior similar MaxEnt RL is used by animals in the natural world. While standard reinforcement learning is often used as a model for decision making (Scott, 2004;Liu & Todorov, 2007;Todorov & Jordan, 2002), many animals, including humans, do not consistently make decisions that maximize expected utility. Rather, they engage in probability matching, choosing actions with probability proportional to how much utility that action will provide. Examples include ants (Lamb & Ollason, 1993), bees (Greggers & Menzel, 1993), fish (Bitterman et al., 1958), ducks (Harper, 1982), pigeons (Bullock & Bitterman, 1962;Graf et al., 1964), and humans, where it has been documented so extensively that Vulkan (2000) wrote a survey of surveys of the field. This effect has been observed not just in individuals, but also in the collective behavior of groups of animals (see Stephens & Krebs (1986)), where it is often described as obtaining the ideal free distribution. Probability matching is not merely a reflection of youth or ignorance. Empirically, more intelligent creatures are more likely to engage in probability matching. For example, in a comparison of Yale students and rats, Gallistel (1990) found that the students nearly always performed probability matching, while rats almost always chose the maximizing strategy. Similarly, older children and adults engage in probability matching more frequently than young children (Stevenson & Odom, 1964;Weir, 1964). While prior work has offered a number of explanations of probability matching (Vulkan, 2000;Gaissmaier & Schooler, 2008;Wozny et al., 2010;Sakai & Fukai, 2008), its root cause remains an open problem.\n\nThe empirical success of MaxEnt RL algorithms on RL problems is surprising, as MaxEnt RL optimizes a different objective than standard RL. The solution to every MaxEnt RL problem is stochastic, while deterministic policies can always be used to solve standard RL problems (Puterman, 2014). While RL can be motivated from the axioms of utility theory (Russell & Norvig, 2016), MaxEnt RL has no such fundamental motivation. It remains an open question as to whether the standard MaxEnt RL objective actually optimizes some well-defined notion of risk or regret that would account for its observed empirical benefits. This paper studies this problem, and aims to answer the following question: What aspects of maximum entropy reinforcement learning might we investigate further? Answering this question is a first step towards understanding the empirical success of MaxEnt RL algorithms, and our analysis will suggest that MaxEnt RL might be applicable to problems typically considered to be much more complex than standard RL.\nIn this paper, we show that MaxEnt RL provides the optimal control solution in settings with uncertainty and variability in the reward function. More precisely, we show that MaxEnt RL is equivalent to two more challenging problems: (1) regret minimization in a meta-POMDP, and (2) robust-reward control. The first setting, the meta-POMDP, is a partially observed MDP where the reward depends on an unobserved portion of the state, and where multiple episodes in the original MDP correspond to a single extended trial in the meta-POMDP. While seemingly Byzantine, this type of problem setting arises in a number of real-world settings discussed in Section 3. Optimal policies for the meta-POMDP must explore at test-time, behavior that cannot result from maximizing expected utility. In the second setting, robust-reward control, we consider an adversary that chooses some aspects of the reward function. Intuitively, we expect stochastic policies to be most robust because they are harder to exploit, as we formalize in Section 5. Even if the agent will eventually be deployed in a setting without adversaries, the adversarial objective bounds the worst-case performance of that agent. Our result in this setting can be viewed as an extension of prior work connecting the principle of maximum entropy to two-player games (Ziebart et al., 2011;Gr\u00fcnwald et al., 2004). While both robust-reward control and regret minimization in a meta-POMDP are natural problems that arise in many real-world scenarios, neither is an expected utility maximization problem, so we cannot expected optimal control to solve these problems. In contrast, we show that MaxEnt RL provides solutions to both. In summary, our analysis suggests that the empirical benefits of MaxEnt RL arise by implicitly solving control problems with variability in the reward."}
{"introduction": "As machine learning models become widespread in automated decision making systems, apart from the efficiency and accuracy of the prediction, their potential social consequence also gains increasing attention. To date, there is ample evidence that machine learning models have resulted in discrimination against certain groups of individuals under many circumstances, for instance, the discrimination in ad delivery when searching for names that can be predictive of the race of individual (Sweeney, 2013); the gender discrimination in job-related ads push (Datta et al., 2015); stereotypes associated with gender in word embeddings (Bolukbasi et al., 2016); the bias against certain ethnicities in the assessment of recidivism risk (Angwin et al., 2016).\n\n\nFocusing on the Equalized-Odds criterion, the attainability has not been comprehensively analyzed. Actually, as we illustrate in this paper, Equalized Odds is not always attainable for regression and even classification tasks, if we use deterministic prediction functions. This calls for alternative definitions in the same spirit as Equalized Odds that can always be achieved under various circumstances. Under mild assumptions, for binary classification we show that if randomized prediction is taken into consideration, one can always derive a non-trivial Equalized Odds classifier. Considering the optimality of performance under fairness constraint(s), when exploiting all available features, we show that the predictor derived via an in-processing approach would always outperform the one derived via a post-processing approach (unconstrained optimization followed by a post-processing step)."}
{"introduction": "Natural organisms benefit from the fact that their sensory inputs and action outputs are all organized in the same space, that is, the physical universe. This consistency makes it easy to apply the same predictive functions across diverse settings. Deep multi-task learning (Deep MTL) has shown a similar ability to adapt knowledge across tasks whose observed variables are embedded in a shared space. Examples include vision, where the input for all tasks (photograph, drawing, or otherwise) is pixels arranged in a 2D plane (Zhang et al., 2014;Misra et al., 2016;Rebuffi et al., 2017); natural language (Collobert & Weston, 2008;Luong et al., 2016;Hashimoto et al., 2017), speech processing (Seltzer & Droppo, 2013;Huang et al., 2015), and genomics (Alipanahi et al., 2015), which exploit the 1D structure of text, waveforms, and nucleotide sequences; and video game-playing (Jaderberg et al., 2017;Teh et al., 2017), where interactions are organized across space and time. Yet, many real-world prediction tasks have no such spatial organization; their input and output variables are simply labeled values, e.g., the height of a tree, the cost of a haircut, or the score on a standardized test. To make matters worse, these sets of variables are often disjoint across a set of tasks. These challenges have led the MTL community to avoid such tasks, despite the fact that general knowledge about how to make good predictions can arise from solving seemingly \"unrelated\" tasks (Mahmud & Ray, 2008;Mahmud, 2009;Meyerson & Miikkulainen, 2019). This paper proposes a solution: Learn all variable locations in a shared space, while simultaneously training the prediction model itself (Figure 1). To illustrate this idea, Figure 1a gives an example of four tasks whose variable values are measured at different locations in the same underlying 2D embedding space. The shape of each marker (i.e., \u2022, , , ) denotes the task to which that variable belongs; white markers denote input variable, black markers denote output variables, and the background coloring indicates the variable values in the entire embedding space when the current sample is drawn. As a concrete example, the color could indicate the air temperature at each point in a geographical region at a given moment in time, and each marker the location of a temperature sensor (however, note that the embedding space is generally more abstract). Figure 1b-c shows a model that can be applied to any task in this universe, using the \u2022 task as an example: (b) The function f encodes the value of each observed variable x i given its 2D location z i \u2208 R 2 , and these encodings are aggregated by elementwise addition ; (c) The function g decodes the aggregated encoding to a prediction for y j at its location z j . In general, the embedded locations z are not known a priori, but they can be learned alongside f and g by gradient descent."}
{"introduction": "In recent years, deep neural networks have produced state-of-the-art results on a variety of important supervised learning tasks. Active learning, a learning protocol where labels can be requested by the algorithm in a sequential, feedback-driven fashion, is a promising approach for minimizing labeling effort. Active learning algorithms aim to identify and label only maximally-informative samples, so that a high-performing classifier can be trained with minimal labeling effort. As such, a robust active learning algorithm for deep neural networks may expand the domains in which these models are applicable. \n\nHow should we design an active learning algorithm for deep neural networks? Theory for active learning provides little guidance for these highly expressive models. One option is to use the network's uncertainty to inform a query strategy, but this can create issues in a batch setting. Remedying this, we could select diverse samples, but this might choose uninformative points. For these reasons, methods that exploit uncertainty or diversity alone do not consistently work well across models, batch sizes, or datasets. Further, hyperparameter changes in active learning provoke substantial labeling inefficiency. Active learning algorithms thus need to perform consistently given fixed hyperparameters.\n\nWe show that BADGE performs well across experiments with different architectures, batch sizes, and datasets. We begin by introducing notation and the setting, followed by a description of BADGE and experiments.\n\nWhile some work has been done in this area, there remain opportunities for further research. This paper presents an algorithm that exhibits strong performance across varied experimental conditions. However, it does not situate itself explicitly within the context of limitations in previous studies. The unique contributions of the research are also not clearly articulated. Nonetheless, the method represents a solid approach for active learning with deep neural networks. Further analysis would be needed to fully understand how it compares to and builds upon prior work, and the precise insights it provides."}
{"introduction": "Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013;Pennington et al., 2014) and sentences (Kiros et al., 2015;Conneau et al., 2017). More recently, deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018;Devlin et al., 2018) which have enabled state-of-the-art results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful for a variety of problems right out of the box. These include Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a).\nThere are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016;Conneau et al., 2017;Schwenk & Douze, 2017;Subramanian et al., 2018), Transformers (Cer et al., 2018;Reimers & Gurevych, 2019), and averaging models (Wieting et al., 2016a;Arora et al., 2017) have found success for learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence prediction (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017;Schwenk & Douze, 2017;Schwenk, 2018;Artetxe & Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) as well as additional tasks like constituency parsing (Subramanian et al., 2018). Surprisingly, despite ample testing of more powerful architectures, the best performing models for many sentence embedding tasks related to semantic similarity often use simple architectures that are mostly agnostic to the interactions between words. For instance, some of the top performing techniques use word embedding averaging (Wieting et al., 2016a), character n-grams (Wieting et al., 2016b), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013;Wieting & Gimpel, 2018) or bilingual data (Wieting et al., 2019b). The inclusion of latent variables in these models has also been explored (Chen et al., 2019).\nIntuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or language-specific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our knowledge, this has not not been explored in prior work. Specifically, we propose a deep generative model that is encouraged to perform source separation on parallel sentences, isolating what they have in common in a latent semantic embedding and explaining what is left over with language-specific latent vectors. At test time, we use inference networks (Kingma & Welling, 2013) for approximating the model's posterior on the semantic and source-separated latent variables to encode monolingual sentences. Finally, since our model and training objective are generative, our approach does not require knowledge of the distance metrics to be used during evaluation,1 and it has the additional property of being able to generate text.\nIn experiments, we evaluate our probabilistic source-separation approach on a standard suite of STS evaluations. We demonstrate that the proposed approach is effective, most notably allowing the learning of high-capacity deep transformer architectures (Vaswani et al., 2017) while still generalizing to new domains, significantly outperforming a variety of state-of-the-art baselines. Further, we conduct a thorough analysis by identifying subsets of the STS evaluation where simple word overlap is not able to accurately assess semantic similarity. On these most difficult instances, we find that our approach yields the largest gains, indicating that our system is modeling interactions between words to good effect. We also find that our model better handles cross-lingual semantic similarity than multilingual translation baseline approaches, indicating that stripping away language-specific information allows for better comparisons between sentences from different languages.\nFinally, we analyze our model to uncover what information was captured by the source separation into the semantic and language-specific variables and the relationship between this encoded information and language distance to English. We find that the language-specific variables tend to explain more superficial or language-specific properties such as overall sentence length, amount and location of punctuation, and the gender of articles (if gender is present in the language), but semantic and syntactic information is more concentrated in the shared semantic variables, matching our intuition. Language distance has an effect as well, where languages that share common structures with English put more information into the semantic variables, while more distant languages put more information into the language-specific variables. Lastly, we show outputs generated from our model that exhibit its ability to do a type of style transfer."}
{"introduction": "Attribution, the identification of the input features most salient to a model prediction, is an increasingly important requirement for neural networks. It is a core part of model interpretability, which is valuable as a research tool and design aid, but also increasingly as an output requirement in its own right for uses as diverse as medical imaging (Singh et al., 2020) to loan applications (Bhatt et al., 2020). A \"right to explanation\" of machine decisions is even provided in the European Union's General Data Protection Regulation (Goodman & Flaxman, 2017).\n\nGradients of predictions with respect to model inputs can be calculated using backpropagation and have been used for feature attribution (\"Vanilla Gradient\") (Erhan et al., 2009;Simonyan et al., 2014;Yosinski et al., 2015). The gradients indicate which features are most sensitive to a perturbation, causing the largest change in prediction outcome. Empirically, this method often does highlight areas relevant to prediction, but it may also miss important areas due to the nonlinearity of the model and saturation of the gradients (Sundararajan et al., 2016). \n\nThe method of Integrated Gradients (Sundarararajan et al., 2016;2017) overcomes these issues by generating attributions based on the theory of Shapley values (Sundararajan et al., 2017). The method integrates the gradients as the input varies linearly between a baseline and the final input of interest:\n\nIntegrated Gradients(F, x , x, i)\n\nwhere F (\u2022) denotes the prediction of the model, x is the input vector of interest, x is the baseline input vector, and i indexes to the feature of interest. A numerical approximation is normally used to calculate the integral in practice. The baseline represents a missing or neutral input, a concept required by the Shapley values theory, but shared with several other attribution methods (Covert et al., 2020).\n\nNote on notation It will be convenient to imply an equivalence between attribution methods which take a baseline, \u03c6(F, x , x, i), and those which do not \u03c6(F, x, i). A baseline free method can be adapted to take a baseline by \u03c6 B (F, x , x, i) = \u03c6(F, x, i) -\u03c6(F, x , i)."}
{"introduction": "Addressing noise in training set labels is an important problem in supervised learning that has been studied for decades across many subfields of machine learning and artificial intelligence. The first investigations into noisy labels date back to the 1960s with early work by Chow on optimizing classification in the presence of incorrectly labeled training examples. Since then, researchers have explored noisy labels in statistical learning theory, computational learning theory, Bayesian analysis, and more recently, deep learning. \n\nIn the 1980s and 90s, theoretical analyses demonstrated that traditional ERM algorithms break down under even small amounts of label noise. This spurred interest in noise-robust loss functions and algorithms. In the 2000s, researchers proposed methods to explicitly model the noise and correct labels. With the rise in popularity of deep learning, label noise has become even more critical to address due to the susceptibility of complex models to memorizing incorrectly labeled examples.\n\nRecent breakthroughs in computer vision and natural language processing have led to larger datasets being generated at scale. However, large-scale data collection inevitably introduces label noise through ambiguity and human or automatic annotation mistakes. Developing noise-resilient methods is crucial for real-world applications.\n\nClassical approaches make simplistic i.i.d. assumptions about label noise, treating it as independent and identically distributed. Methods under this assumption either explicitly model the noise or use regularization. Some results show certain loss functions are inherently robust to i.i.d. noise. \n\nAlthough these methods have guarantees, they often fail in practice due to unrealistic i.i.d. assumptions. Real-world noise is heterogeneous and input-dependent. For example, ambiguous images are more likely mislabeled. Methods robust to general forms of noise are needed to handle real-world challenges. \n\nState-of-the-art techniques use data recalibration, progressively identifying clean data to train on. These achieve excellent empirical performance but lack theoretical justification.\n\nThe PMD noise family captures heterogeneous, feature-dependent noise. Under this model, we propose a theoretically guaranteed data recalibration algorithm that iteratively corrects labels and retrains. Our main theorem proves the label purity improves over iterations, leading the model to eventually converge to the Bayes optimal classifier. Experiments confirm our method outperforms others on CIFAR and real-world datasets.\n\nRelated works either assume i.i.d. noise or lack convergence guarantees. Our method is the first with guaranteed convergence to a well-behaved classifier under general PMD noise. The excessive background detracts from the main contribution and relevance of the work."}
{"introduction": "Few-shot learning, the problem of learning under data scarcity, is an important challenge in deep learning as large number of training instances may not be available in many real-world settings. While the recent advances in meta-learning made it possible to obtain impressive performance on few-shot learning tasks (Hou et al., 2019;Li et al., 2019;Lifchitz et al., 2019), it still remains challenging in cases where we are given very little information (e.g. one-shot learning). In this work, we aim to tackle this problem by proposing a novel confidence-based transductive inference scheme for metric-based meta-learning models. The most challenging problem is that the confidence prediction on the test instances for unseen task should be inevitably unreliable, since the samples come from an unknown distribution. To account for such uncertainties of prediction on an unseen task, we first propose to generate various model and data perturbations, such as random dropping of residual blocks and random augmentations. This randomness helps the model better learn the confidence measure by considering various uncertainties for an unseen task (see Figure 1), and also allows us to take an ensemble over the confidence measures under random perturbations at test time. In order to enhance learning confidence, we further meta-learn the distance metric (or metric) to assign different confidence scores to each query (or test) instance for each class, such that the updated prototypes obtained by confidence-weighted averaging of the queries improve classification of the query samples. This is done by learning a metric length-scale term for each individual instance or a pair of instances. We refer to this transductive inference using meta-learned input-adaptive confidence under various perturbations as Meta-Confidence Transduction (MCT). We validate our transductive inference scheme for metric-based meta-learning models on four benchmark datasets against existing transductive approaches, which shows that the models using meta-learned confidence significantly outperform existing transductive inference methods, and obtain new state-of-the-art results. We further verify the generality of our MCT on semi-supervised learning tasks, where we assign confidence scores to unlabeled data. The results show that MCT outperforms relevant baselines by large margins, which shows the efficacy of our method. Further ablation studies show that both meta-learning of various perturbations and input-adaptive distance metric are crucial in the success of our method in assigning correct confidence to each test sample.\nOur main contributions are as follows:\n\u2022 We propose to meta-learn the confidence with various types of model and data perturbations during meta-learning, such that the meta-learned confidence can better account for uncertainties at unseen tasks. \n\u2022 We further propose to meta-learn an input-adaptive distance metric, which allows to output an accurate and reliable confidence for an unseen test samples that can directly improve upon the transductive inference performance.\n\u2022 We validate our model on four benchmark datasets for few-shot classification and achieve new state-of-the-art results, largely outperforming all baselines. Further experimental validation of our model on semi-supervised few-shot learning also verifies its efficacy."}
{"introduction": "Deep learning (DL) has exploded successfully and is applied to many application domains, such as image recognition and object detection. For Internet of Things (IoT) applications, large neural network models cannot fit into resource-constrained devices. On the other hand, a system designer often tries to find a proper computing platform or a deep learning accelerator (DLA) to execute a DL application with acceptable responsiveness. While several works have been proposed to estimate the delivered performance of a given DL model on a specific computing platform, the estimates from these efforts are not very accurate. \nIn this paper, we propose a deep residual network architecture, called ResPerfNet, to model the performance of DL models running on DL frameworks and DLAs. It is inspired by the prior works which use residual neural networks to solve regression problems. The proposed model can be trained with performance data collected from system configurations to establish a predictor which assists the users in selecting the DL model, the DL framework, and the DLA for their applications. Experiments have been done to show that our approach not only provides accurate performance estimates, but also enables the users to quickly predict the performance of their DL applications executed with various models-framework-accelerator configurations.\nThe remaining of this paper is organized as follows. Section 2 presents the related work. Section 3 describes the architecture of ResPerfNet. Section 4 shows the proposed modeling method. Section 5 elaborates the dataset and training mechanism to train the ResPerfNet models. Section 6 evaluates the efficiency of our approach. Section 7 concludes the paper."}
{"introduction": "Implementing gradient-based optimization in practice requires many choices. These include setting hyperparameters such as learning rate and batch size as well as specifying a data augmentation scheme, a popular set of techniques in which data is augmented (i.e. modified) at every step of optimization. Trained model quality is highly sensitive to these choices. In practice they are made using methods ranging from a simple grid search to Bayesian optimization and reinforcement learning (Cubuk et al., 2019;2020;Ho et al., 2019). Such approaches, while effective, are often ad-hoc and computationally expensive due to the need to handle scheduling, in which optimization hyperparameters and augmentation choices and strengths are chosen to change over the course of optimization.\n\nThis article is a step towards bridging this gap. We provide in \u00a73 a rigorous framework for reinterpreting gradient descent with arbitrary data augmentation as stochastic gradient descent on a time-varying sequence of objectives. This provides a unified language to study traditional stochastic optimization methods such as minibatch SGD together with widely used augmentations such as additive noise (Grandvalet & Canu, 1997), CutOut (DeVries & Taylor, 2017), Mixup (Zhang et al., 2017) and label-preserving transformations (e.g. color jitter, geometric transformations (Simard et al., 2003)). It also opens the door to studying how to schedule and evaluate arbitrary augmentations, an important topic given the recent interest in learned augmentation Cubuk et al. (2019).\nQuantitative results in our framework are difficult to obtain in full generality due to the complex interaction between models and augmentations. To illustrate the utility of our approach and better understand specific augmentations, we present in \u00a73 and \u00a75 results about arbitrary augmentations for overparameterized linear regression and specialize to additive noise and minibatch SGD in \u00a74 and \u00a76. While our results apply directly only to simple quadratic losses, they treat very general augmentations. Treating more complex models is left to future work. Our main contributions are:\n\u2022 In Theorem 5.1, we give sufficient conditions under which gradient descent under any augmentation scheme converges in the setting of overparameterized linear regression. Our result extends classical results of Monro-Robbins type and covers schedules for both learning rate and data augmentation scheme.\n\u2022 We complement the asymptotic results of Theorem 5.1 with quantitative rates of convergence furnished in Theorem 5.2. These rates depend only on the first few moments of the augmented data distribution, underscoring the flexibility of our framework.  \n\u2022 In \u00a74, we analyze additive input noise, a popular augmentation strategy for increasing model robustness. We recover the known fact that it is equivalent to stochastic optimization with 2 -regularization and find criteria in Theorem 4.1 for jointly scheduling the learning rate and noise level to provably recover the minimal norm solution.\n\u2022 In \u00a76, we analyze minibatch SGD, recovering known results about rates of convergence for SGD (Theorem 6.1) and novel results about SGD with noise (Theorem 6.2)."}
{"introduction": "Much of the recent success in building machine learning systems for image classification can be attributed to training deep neural networks on large, humanly annotated datasets, such as Ima-geNet (Deng et al., 2009) or CIFAR-10 (Torralba et al., 2008). However, assembling such datasets is time-consuming and expensive: Both ImageNet and CIFAR-10 were constructed by first searching the web for candidate images and second labeling the candidate images by workers to obtain clean labels instead of the noisy candidate ones. The first step already yields labeled examples, but the accuracy of those automatically collected, candidate labels is low: For example, only about half of the labels of the Tiny Images dataset constructed in the first step of obtaining the CIFAR-10 dataset are correct (Torralba et al., 2008). In the second step, clean labels are obtained by asking workers through a crowdsourcing platform for annotations and aggregating them.\nIn this paper we propose to train directly on the noisy candidate examples, effectively skipping the expensive human labeling step. To make this work, we exploit an intriguing property of large, overparameterized neural networks: If trained with stochastic gradient descent or variants thereof, neural networks fit clean labels significantly faster than noisy ones. This fact is well known, see for example the experiments by Zhang et al. (2017, Fig. 1a) which demonstrated that overparameterized deep networks can fit even randomly shuffled CIFAR-10 labels perfectly. What is now well known is that this effect is sufficiently strong to enable training on candidate labels only. Our idea is that, if neural networks fit clean labels faster than noise, then training them on a set containing clean and wrong labels and early stopping the training resembles training on the clean labels only."}
{"introduction": "A flow-based generative model refers to a deep generative model composed using a set of invertible transformations. While GANs and VAEs remain the two dominant generative models in the community, flow based formulations have continually evolved and now offer competitive performance in applications including audio/speech synthesis Kim et al. (2019;2020), text to speech Miao et al. (2020), photo-realistic image generation Kingma & Dhariwal (2018), and learning cross-domain mappings Mahajan et al. (2020). An important property of such models is the explicit use of a tractable likelihood function, which enables leveraging maximum likelihood principles during training as well as efficient/exact density estimation and sampling. The formulation is invertible by design but this involves higher memory requirements. For example, permitting the bijective mapping to be expressive enough involves increases in the memory footprint Lee et al. (2020); Kim et al. (2019), an issue that is a focus of several recent results Jacobsen et al. ( 2018); Chen et al. (2016). Moreover, in these models we need to calculate the inverse and backpropagate through all invertible transformations during training. Calculating the inverse incurs a multiplicative increase in cost, usually as a function of the feature dimension, relative to the calculation of the likelihood, an issue addressed to some extent in Dinh et al. (2017); Kingma & Dhariwal (2018).\nAt a high level, a flow-based generative model bijectively pushes the data density from a source to a target, i.e., from a known simple distribution to an unknown (may be intractable) data distribution.\nDuring training, we seek to learn this bijective mapping by maximizing the likelihood of the mapped training samples. In the generation step, we need the inverse of this mapping (given such an inverse exists) to map from a sample drawn from the known distribution back to the input (data) space. When the Jacobian of the transformation mapping can be efficiently computed or estimated (e.g., having a lower triangular form), directly optimizing the likelihood of the training samples is possible. However, in training flow-based generative either we must restrict the expressiveness at each layer or fall back on more numerically heavy solutions, see (Chen et al., 2018). Next, we discuss how several existing results may provide a simplification strategy."}
{"introduction": "Deep learning has achieved groundbreaking advances in computer vision, but modern artificial neural networks have largely developed independent of neuroscience research on biological neuronal networks. Many of the core features found in biological neural networks have never been integrated successfully into artificial neural networks, raising serious questions of whether the success of deep learning is relevant to neuroscience research or vice versa.\nThis paper presents an artificial neuronal network incorporating features found in the classic Hodgkin-Huxley equations that relate neuronal membrane potential to the activity of ion channel conductance pathways. A primary feature of this approach is that the values that the neuronal elements of the network can take are range-limited and self-normalizing. In addition, each neuron is classified as either excitatory or inhibitory, an aspect of biological neural networks that until now has never been effectively utilized in artificial neural networks. Here we demonstrate that more biologically accurate networks can learn complex computer vision datasets such as CIFAR-10 competitively with the state-of-the-art just a few years prior. Biological neurons do not compute an affine transform of their inputs. Rather, the synaptic inputs compete against other active conductance pathways in the neuron for the control of the membrane potential and thus the firing state of the neuron. The valence of these different pathways is reflected by the zero-current potentials of the types of channel conductances being regulated, which determines whether the pathway excites or inhibits the neuron. The combination of these conductances into a single membrane potential is analogous to performing a conductance weighted averaging (CWA) of these zero-current potentials.\nThe concept of neuron valence is related to the types of synaptic channels that a neuron regulates in its post-synaptic neurons. Because neurons primarily release one type of neurotransmitter, they functionally fall into one of two classes: Excitatory or Inhibitory neurons.\nIn this paper, CWA similarly uses two valences, (+1) for neurons forming excitatory connections and (-1) for neurons forming inhibitory connections. Importantly, the combining of these pathways into a single neuron using CWA is constrained to produce neuronal outputs that lie somewhere between these two extremes, and thus the output of CWA is bound by the range of the valences.\nNormalization and regularization strategies, such as BatchNorm (Ioffe & Szegedy, 2015) or DropOut (Srivastava et al., 2014), have been repeatedly shown to improve the quality of artificial neural networks. But CWA networks are by definition normalized and range-limited. Therefore, one can conjecture that CWA plays a normalization role in biological neural networks. As seen in Table ??, bare CWA without additional normalization or regularization is competitive with standard affine networks that employ these strategies, especially in the case of fully connected networks (FCN), whose neurons, like biological neurons, are characterized by having thousands of inputs.\nIn sum, neural networks with CWA provide a more biologically accurate model with excitatoryinhibitory neurons and conductance-based normalization that succeeds on computer vision datasets without extra normalization and regularization, with implications for neuroscience and deep learning."}
{"introduction": "Deep neural network (DNN) classifiers have become indispensable tools for addressing practically relevant problems, such as autonomous driving (Tian et al., 2018), natural language processing (Young et al., 2018) and health care predictions (Esteva et al., 2019). While a DNN provides substantial utility, training a DNN is costly because of data preparation (collection, organization, and cleaning) and computational resources required for validation of a model (Press, 2016). For this reason, DNNs are often provided by a single entity and consumed by many, such as in the context of Machine Learning as a Service (MLaaS). A threat to the provider is model stealing, in which an adversary derives a surrogate model from only API access to a source model. We refer to an independently trained model for the same task as a reference model.\nConsider a MLaaS provider that wants to protect their service and hence restrict its redistribution, e.g., through a contractual usage agreement because trained models constitute their intellectual property. A threat to the model provider is an attacker who derives surrogate models and publicly deploys them. Since access to the source model has to be provided, users cannot be prevented from deriving surrogate models. Krishna et al. (2019) have shown that model stealing is (i) effective, because even high-fidelity surrogates of large models like BERT can be stolen, and (ii) efficient, because surrogate models can be derived for a fraction of the costs with limited access to domain data. This paper proposes a DNN fingerprinting method to predict whether a model is a (stolen) surrogate or a (benign) reference model relative to a source model. DNN fingerprinting is a new area of research that extracts a persistent, identifying code (fingerprint) from an already trained model. Model stealing can be categorized into model modification, such as weight pruning (Zhu & Gupta, 2017), or model extraction that uses some form of knowledge distillation (Hinton et al., 2015) to derive a surrogate from scratch. Claimed security properties of existing defenses ( (Adi et al., 2018;Zhang et al., 2018)), have been broken by model extraction attacks (Shafieinejad et al., 2019). Our fingerprinting method is the first passive defense that is specifically designed towards withstanding model extraction attacks, which extends to robustness against model modification attacks.\nOur research provides new insight into the transferability of adversarial examples. In this paper, we hypothesize that there exists a subclass of targeted, transferable, adversarial examples that transfer exclusively to surrogate models, but not to reference models. We call this subclass conferrable. Any conferrable example found in the source model should have the same misclassification in a surrogate model, but a different one in reference models. We propose a metric to measure conferrability and an ensemble adversarial attack that optimizes this new metric. We generate conferrable examples as the source model's fingerprint.\nRetrained CIFAR-10 surrogate models can be verified with a perfect ROC AUC of 1.0 using our fingerprint, compared to an ROC AUC of 0.63 for related work (Cao et al., 2019). While our fingerprinting scheme is robust to almost all derivation and extraction attacks, we show that some adapted attacks may remove our fingerprint. Specifically, our fingerprint is not robust to transfer learning when the attacker has access to a model pre-trained on ImageNet32 and access to CIFAR-10 domain data. Our fingerprint is also not robust against adversarial training (Madry et al., 2017) from scratch. Adversarial training is an adapted model extraction attack specifically designed to limit the transferability of adversarial examples. We hypothesize that incorporating adversarial training into the generation process of conferrable adversarial examples may lead to higher robustness against this attack."}
{"introduction": "Generating sparse neural networks through pruning has recently led to a major reduction in the number of parameters, while having minimal loss in performance. Conventionally, pruning methods operate on pre-trained networks. Generally, such methods use an edge scoring mechanism for eliminating the less important connections. Popular scoring mechanisms include weight magnitudes (Han et al. (2015b); Janowsky (1989); Park et al. (2020)), loss sensitivity with respect to units (Mozer & Smolensky (1989)) and with respect to weights (Karnin (1990)), Hessian (LeCun et al. (1990); Hassibi & Stork (1993)), and first and second order Taylor expansions (Molchanov et al. (2016;2019)). More recent approaches use much more sophisticated variants of these scores (Han et al. (2015a); Guo et al. (2016); Carreira-Perpin\u00e1n & Idelbayev (2018); Yu et al. (2018); Dong et al. (2017); Guo et al. (2016)).\nFurther analysis of pruning has showed the existence of sparse subnetworks at initialization which, when trained, are capable of matching the performance of the fully-connected network (Frankle & Carbin (2018); Frankle et al. (2019); Liu et al. (2018)). However, identifying such \"winning ticket\" networks requires expensive training and pruning cycles. More recently, SNIP (Lee et al. (2018)), You et al. (2019) and GraSP (Wang et al. (2020)) showed that it is possible to find \"winning tickets\" prior to training -but still having access to at least some training data to compute initial gradients. Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al. (2019); Tanaka et al. (2020)). Some network science methods have proposed to construct sparse neural networks but without focusing on their learning performance (Shafiee et al. (2016); Prabhu et al. (2018); Kepner & Robinett (2019)). The closest approach that tackles the same problem with our work is SynFlow (Tanaka et al. (2020)). That work introduced the concept of \"layer collapse\" in pruning -the state when all the edges in a layer are eliminated while there exists edges in other layers that can be pruned. They proved that iterative pruning on the basis of positive gradient-based scores avoids layer collapse and introduced an iterative algorithm and an objective function that conserves information flow and avoids layer collapse.\nOur goal is to identify sparse subnetworks that perform almost as well as the fully connected network (i.e., to identify a \"winning ticket\") without any training data. Given a fully-connected and initialized architecture, and a target number of learnable parameters, we select a set of input-output paths that will be conserved in the network and prune every remaining connection. The selection of the conserved paths is based strictly on their initial weight values -and not on any training data. Then, the pruned network is trained only once. We refer to this method as PHEW (Paths with Higher Edge Weights). Our main contributions are:\n1. We propose to form sparse networks through an input-output path conservation process rather than edge-based pruning (Sec 2).\n2. We introduce the Edge-Weight-Product (EWP) metric for selecting paths based on their initial weights, and show that higher EWP-valued paths also have larger loss gradient magnitudes at initialization (Sec 3).\n3. We empirically show that sparse networks constructed with paths of higher initial gradients converge faster and perform better (Sec 4).\n4. We present a data-agnostic algorithm (PHEW) to construct a sparse network with a given number of parameters using paths of higher EWP values (Sec 5).\n5. Finally, we evaluate the structural similarity relationship between PHEW networks and pruned networks constructed through Iterated Magnitude Pruning (IMP), concluding that the former belong in the family of \"winning tickets\" subnetworks (Sec 6.2)."}
{"introduction": "Retrosynthesis, first identified by Corey & Wipke (1969), is a fundamental chemical problem to infer a set of reactant compounds that can be synthesized into a desired product compound through a series of chemical reactions. The search space of sets of compounds is innately huge. Further, a product compound can be synthesized through different series of reactions from different reactant compound sets. Such difficulties require the huge efforts of human chemical experts and the large knowledge base to build a retrosynthesis engine for years. Thus, expectations of machine-learning (ML) based retrosynthesis engines is growing in recent years. The need for the retrosynthesis becomes intensive in these days along with the development of in silico (computational) chemical compound generations (Jin et al., 2018;Kusner et al., 2017), which are also applied to new drug discovery for COVID-19 (Cant\u00fcrk et al., 2020;Chenthamarakshan et al., 2020). These generation models can generate unseen compounds in computers but do not answer how to synthesize them in practical. Retrosynthesis engines can help chemists and pharmacists fill this gap.\nPractical retrosynthesis planning requires a strong model to learn inherent biases in the target dataset while keeping generalization performance to generate unseen (test) product compounds. The current trend is to focus on developing such a strong ML model architecture such as seq-to-seq models (Liu et al., 2017;Karpov et al., 2019) and graph-to-graphs models (Shi et al., 2020;Yan et al., 2020;Somnath et al., 2020), which achieve the State-of-the-Art (SotA) retrosynthesis accuracy.\nHowever, model architecture is not the only issue to consider. In the current deep neural network (DNN) era, the quantity (many samples) and the quality (less noisy, corrupted samples) of the available dataset often governs the final performance of the ML model. The problem is that there are only a few large and high-quality supervised training datasets that are available publicly. Instead, only a small and/or a noisy dataset is usually available for the target application task. To cope with this problem, data transfer approaches are widely employed as ordinary research pipelines in computer vision (CV), natural language processing (NLP), and machine translation (MT) domains (Kornblith et al., 2019;Xie et al., 2020;He et al., 2020;Khan et al., 2019;Sennrich et al., 2016). A data transfer approach tries to transfer knowledge to help a difficult training on the small and/or noisy target dataset. That knowledge is imported from augmented dataset, which is usually a large or Product(s) X:\nForm. of p(Y|X): Retrosynthesis model Graph representations (Somnath et al., 2020) (Yan et al., 2020) (Shi et al., 2020 SMILES representations (Liu et al., 2017) (Karpov et al., 2018) Figure 1: Overview of the retrosynthesis problem.\nclean dataset in the same domain but does not share the same task or the same assumptions with the target dataset. Such augmented datasets are beneficial if the quantity or the quality of the augmented dataset is superior compared to the target dataset. However, this data transfer approach is not still well investigated in the previous retrosynthesis studies as we explained above.\nIn this paper, we conduct a systematic investigation of the effect of data transfer for the improvement of the retrosynthesis models. We examine three standard methods of data transfer: joint training, self-training, and pre-training plus fine-tuning. The result shows that every data transfer method can improve the test prediction accuracy of an off-the-shelf Transformer retrosynthesis model. Especially, a Transformer with pre-training plus fine-tuning achieves comparable performance with, and in some cases better performance than, the SotA models. In addition, we conducted an intensive manual inspection for the erroneous prediction results. This inspection clarifies the limitations of our approaches. But at the same time, it reveals that the pre-training plus fine-tuning model can generate chemically appropriate or sensible proposals more than 99% cases of top-1 predictions."}
{"introduction": "Brain activation dynamics, as measured by fMRI, exist in an extremely high-dimensional collection space and contain high levels of noise. Noise sources include measurement imprecisions and physiological factors such as a subject's blood pressure variability, head motion, and respiration (Brooks et al., 2013;Mumford & Poldrack, 2007). Moreover, even after accounting for the idiosyncrasies in noise, neural responses evoked by the same stimulus can vary greatly between different subjects. This makes using fMRI to extract meaningful signals or general trends that are task-relevant quite difficult. Experiments involving fMRI are expensive and usually involve pilot studies conducted in a smaller scale with simpler stimuli and on a smaller group of subjects. Thus, it is desirable to have a method that can learn common trends from pilot data and can generalize to a larger group of subjects or more complex tasks.\nLike for other high dimensional biomedical data, it is broadly acknowledged that neural activity actually lies in a lower-dimensional latent space. Moreover we hypothesize that several features of these latent spaces are shared across subjects given that measurements are usually task-based and are taken when subjects experience the same stimuli. Dimensionality reduction methods have been used to discover such low-dimensional spaces and facilitate understanding the underlying brain activities. Methods such as principal component analysis (PCA) and factor analysis have been the common choice (Smith et al., 2014). However, these linear methods are sensitive to noise and restrict interactions in the latent space. Nonlinear dimensionality reduction algorithms are shown to better capture the geometry of high-dimensional biological data (Moon et al., 2019;Van der Maaten & Hinton, 2008) and account for dynamics in neural activities (Gao et al., 2020). PHATE (Moon et al., 2019), a diffusion based manifold learning method, was shown to capture both global and local geometry of complex biological data, which appears to be a good candidate to model fMRI data.\nUsing a nonlinear manifold such as PHATE to reduce the dimensionality of data is appealing as it can extract meaningful signals, denoise the data, and accelerate downstream analysis. However, unlike a method like PCA which learns a projection operator that can be applied to new data, manifold learning methods are fixed to the input data and do not naturally extend to new data from new tasks or from new subjects. To extend an existing manifold to out-of-sample data, landmark interpolation or Nystr\u00f6m extensions are commonly used. However it is usually unsatisfactory as shown by decremented performance(Appendix Fig. 1). To tackle these shortcomings, we turn to neural networks such as autoencoders that provide nonlinear dimensionality reduction via a learned parametric function that is readily applicable to new data. While our key goal is learning an informative low-dimensional data manifold shared across multiple subjects, this is not something automatically done by autoencoders. First, there is nothing enforcing autoencoder latent embeddings to respect data manifold geometry, and indeed they often just spread out points for ease of decoding. Second, individual variation often dominates embeddings which prevents a common space to be automatically learned.\nTo address these issues, we propose a manifold-regularized multi-decoder autoencoder (MRMD-AE) that can process fMRI data from a group of subjects and extract common latent space representations that respect individual data geometry. Key features of the MRMD-AE include: (a) A common encoder that projects fMRI data from any subject on to a shared latent space, and subject-specific decoders that learn to reconstruct data for each subject faithfully. (b) A manifold-geometry regularization of the latent space based on a precomputed PHATE embedding. (c) Penalties for distances between individual patient encodings of common stimuli to ensure that the latent space is not split into individual embeddings.\nWe show results on three types of tasks in two different datasets. First, we show that we are able to extend the manifold embedding to new timepoints of data not used in training. This shows the extensibility of our manifold in contrast to non-neural network based manifold learning methods. Second, we show that our latent space improves the ability to classify or infer stimulus features based on subject fMRI measurements. Surprisingly, the multiple decoder improves classification accuracy over a common decoder, showing that this framework allows the network to separate common from individual variations. Finally, we show that even untrained, translation between subjects has increased accuracy on withheld timepoints. Further, we show highly improved cross subject translation after training for this task."}
{"introduction": "Machine learning (ML) models are gradually replacing humans in high-stakes decision making roles. For example, in Philadelphia, an ML model classifies probationers as high or low-risk (Metz & Satariano, 2020). In North Carolina, \"analytics\" is used to report suspicious activity and fraud by Medicaid patients and providers (Metz & Satariano, 2020). Although ML models appear to eliminate the biases of a human decision maker, they may perpetuate or even exacerbate biases in the training data (Barocas & Selbst, 2016). Such biases are especially objectionable when it adversely affects underprivileged groups of users (Barocas & Selbst, 2016).\nIn response, the scientific community has proposed many mathematical definitions of algorithmic fairness and approaches to ensure ML models satisfy the definitions. Unfortunately, this abundance of definitions, many of which are incompatible (Kleinberg et al., 2016;Chouldechova, 2017), has hindered the adoption of this work by practitioners. There are two types of formal definitions of algorithmic fairness: group fairness and individual fairness. Most recent work on algorithmic fairness considers group fairness because it is more amenable to statistical analysis (Ritov et al., 2017). Despite their prevalence, group notions of algorithmic fairness suffer from certain shortcomings. One of the most troubling is there are many scenarios in which an algorithm satisfies group fairness, but its output is blatantly unfair from the point of view of individual users (Dwork et al., 2011).\nIn this paper, we consider individual fairness instead of group fairness. Intuitively, an individually fair ML model treats similar users similarly. Formally, an ML model is a map h : X \u2192 Y, where X and Y are the input and output spaces. The leading notion of individual fairness is metric fairness (Dwork et al., 2011); it requires d y (h(x 1 ), h(x 2 )) \u2264 Ld x (x 1 , x 2 ) for all x 1 , x 2 \u2208 X ,\n(1.1)\nwhere d x and d y are metrics on the input and output spaces and L \u2265 0 is a Lipschitz constant. The fair metric d x encodes our intuition of which samples should be treated similarly by the ML model. We emphasize that d x (x 1 , x 2 ) being small does not imply x 1 and x 2 are similar in all respects. Even if d x (x 1 , x 2 ) is small, x 1 and x 2 may differ in certain problematic ways, e.g. in their protected/sensitive attributes. This is why we refer to pairs of samples x 1 and x 2 such that d x (x 1 , x 2 ) is small as comparable instead of similar.\nDespite its benefits, individual fairness was dismissed as impractical because there is no widely accepted fair metric for many ML tasks. Fortunately, there is a line of recent work on learning the fair metric from data (Ilvento, 2019;Wang et al., 2019). In this paper, we consider two data-driven choices of the fair metric: one for problems in which the sensitive attribute is reliably observed, and another for problems in which the sensitive attribute is unobserved (see Appendix B).\nThe rest of this paper is organized as follows. In Section 2, we cast individual fairness as a form of robustness: robustness to certain sensitive perturbations to the inputs of an ML model. This allows us to leverage recent advances in adversarial ML to train individually fair ML models. More concretely, we develop an approach to audit ML models for violations of individual fairness that is similar to adversarial attacks (Goodfellow et al., 2014) and an approach to train ML models that passes such audits (akin to adversarial training (Madry et al., 2017)). We justify the approach theoretically (see Section 3) and empirically (see Section 4)."}
{"introduction": "Creating agents that learn to interact in large-scale systems is a key challenge in artificial intelligence. Impressive results have been recently achieved in restricted settings (e.g., zero-sum, two-player games) using game-theoretic principles such as iterative best response computation (Lanctot et al., 2017), self-play (Silver et al., 2018), and evolution-based training (Jaderberg et al., 2019;Liu et al., 2019). A key principle underlying these approaches is to iteratively train a growing population of player policies, with population evolution informed by heuristic skill ratings (e.g., Elo (Elo, 1978)) or game-theoretic solution concepts such as Nash equilibria. A general application of this principle is embodied by the Policy-Space Response Oracles (PSRO) algorithm and its related extensions (Lanctot et al., 2017;Balduzzi et al., 2019). Given a game (e.g., poker), PSRO constructs a higherlevel meta-game by simulating outcomes for all match-ups of a population of players' policies. It then trains new policies for each player (via an oracle) against a distribution over the existing meta-game policies (typically an approximate Nash equilibrium, obtained via a meta-solver1 ), appends these new policies to the meta-game population, and iterates. In two-player zero sum games, fictitious play (Brown, 1951), double oracle (McMahan et al., 2003), and independent reinforcement learning can all be considered instances of PSRO, demonstrating its representative power (Lanctot et al., 2017).\nPrior applications of PSRO have used Nash equilibria as the policy-selection distribution (Lanctot et al., 2017;Balduzzi et al., 2019), which limits the scalability of PSRO to general games: Nash equilibria are intractable to compute in general (Daskalakis et al., 2009); computing approximate Nash equilibria is also intractable, even for some classes of two-player games (Daskalakis, 2013); finally, when they can be computed, Nash equilibria suffer from a selection problem (Harsanyi et al., 1988;Goldberg et al., 2013). It is, thus, evident that the reliance of PSRO on the Nash equilibrium as the driver of population growth is a key limitation, preventing its application to general games. Recent work has proposed a scalable alternative to the Nash equilibrium, called \u03b1-Rank, which applies readily to general games (Omidshafiei et al., 2019), making it a promising candidate for population-based training. Given that the formal study of PSRO has only been conducted under the restricted settings determined by the limitations of Nash equilibria, establishing its theoretical and empirical behaviors under alternative meta-solvers remains an important and open research problem.\nWe study several PSRO variants in the context of general-sum, many-player games, providing convergence guarantees in several classes of such games for PSRO instances that use \u03b1-Rank as a meta-solver. We also establish connections between Nash and \u03b1-Rank in specific classes of games, and identify links between \u03b1-Rank and the Projected Replicator Dynamics employed in prior PSRO instances (Lanctot et al., 2017). We develop a new notion of best response that guarantees convergence to the \u03b1-Rank distribution in several classes of games, verifying this empirically in randomly-generated general-sum games. We conduct empirical evaluations in Kuhn and Leduc Poker, first establishing our approach as a competitive alternative to Nash-based PSRO by focusing on two-player variants of these games that have been investigated in these prior works. We subsequently demonstrate empirical results extending beyond the reach of PSRO with Nash as a meta-solver by evaluating training in 3-to 5-player games. Finally, we conduct preliminary evaluations in MuJoCo soccer (Liu et al., 2019), another complex domain wherein we use reinforcement learning agents as oracles in our proposed PSRO variants, illustrating the feasibility of the approach."}
{"introduction": "The primary objective of artificial intelligence is to imitate human intelligence tabular rasa. Especially, with the advent of deep learning (DL), the models are striving to perform composite tasks by learning complex relationships and patterns available in noisy, unstructured data (Ruder, 2017). With this sudden growth in the consumption of data by models, there has been a lot of study on the privacy and security of the learnt model (Shokri & Shmatikov, 2015). Data governance and model governance frameworks, control and protect sharing of data and model meta information between two entities and also their social implications (Helbing, 2019).\nThe premise of model privacy has majorly revolved around preserving the model content from human (man-in-the-middle) adversarial attacks (Abadi et al., 2016). However, the model itself could learn all the private information from the data and become much more intelligent than the original intent it was trained for. With the strive for model generalization, including techniques for transfer learning and multi-task learning, the model is encouraged to learn more and more generic features from the data that could be used for more than one task (S\u00f8gaard & Goldberg, 2016). Consider the example described in Figure 1, where a classifier is trained to detect the shape of an object from images. However, using the features extracted by the above classifier, the size and location of the object in the image can also be predicted. Thus, a shape classifier is more intelligent than its objective of only predicting the shape of the object. While in certain applications, this is a required property of classification models (such as in, transfer learning and domain adaptation), in most of the privacy preserving applications, the data and its other visual attributes have to be kept private from the model itself. As an additional real-world example, we train a DL model to predict the gender from a face image. However, the DL model learns most generic features from the face image, enabling it to predict the age and the identity of the person. The input face image could be saved securely from a human attacker, however, there is not much focus on securing from the model itself.\nAdditionally as shown in Figure 1 (a), the task of debiasing is to remove the the bias (color) in learning a specific task (shape). This happens due to the high correlation between the color and shapes in the input images. However, as shown in Figure 1 (b), our task in model trust is to forcefully ensure that the model learns to perform only one or few selected tasks (shape) from the input images and unlearn all other tasks (color, size, location). If multi-class classification tasks could be done from the same image, the research question is, \"How can we ensure that the model is learnt only for one or a few tasks (called as, preserved tasks), and is strictly not learnt for the other tasks (called as, suppressed tasks)?\". To pursue research on this problem, there are few evident challenges: (i) there is a lack of a balanced and properly curated image dataset where multiple classification tasks could be performed on the same image, (ii) the complete knowledge of both the preserved tasks and the suppressed tasks should be known apriori, that is, we cannot suppress those tasks that we don't have information about, and (iii) presence of very few model agnostic studies to preserve and suppress different task groups. In this research, we propose a novel framework to measure the trust score of a trained DL model and a solution approach to improve the trust score during training. The major research contributions are summarized as follows:\n1. A simulated, class-balanced, multi-task dataset, PreserveTask with five tasks that could be performed on each image: shape, size, color, location, and background color classification."}
{"introduction": "We consider a setting composed of multiple interacting artificially intelligent agents. These agents will be instantiated by humans, corporations, or machines with specific individual incentives. However, it is well known that the interactions between individual agent goals can lead to inefficiencies at the group level, for example, in environments exhibiting social dilemmas (Braess, 1968;Hardin, 1968;Leibo et al., 2017). In order to resolve these inefficiencies, agents must reach a compromise.\nAny arbitration mechanism that leverages a central coordinator1 faces challenges when attempting to scale to large populations. The coordinator's task becomes intractable as it must both query preferences from a larger population and make a decision accounting for the exponential growth of agent interactions. If agents or their designers are permitted to modify their incentives over time, the principal must collect all this information again, exacerbating the computational burden. A central coordinator represents a single point of failure for the system whereas one motivation for multi-agent systems research inspired by nature (e.g., humans, ants, the body, etc.) is robustness to node failures (Edelman and Gally, 2001). Therefore, we focus on decentralized approaches.\nA trivial form of decentralized compromise is to require every agent to minimize group loss (maximize welfare). Leaving the optimization problem aside, this removes inefficiency, but similar to a mechanism with a central coordinator, requires communicating all goals between all agents, an expensive step and one with real consequences for existing distributed systems like wireless sensor networks (Kulkarni et al., 2010) where transmitting a signal saps a node's energy budget. There is also the obvious issue that this compromise may not appeal to an individual agent, especially one that is expected to trade its low-loss state for a higher average group loss. One additional, more subtle consequence of optimizing group loss is that it cannot distinguish between behaviors in environments with a group loss that is constant sum, for instance, in zero-sum games. But zero-sum games have rich structure to which we would like agents to respond. Electing a team leader (or voting on a decision) implies one candidate (decision) wins while another loses. Imagine two agents differ on their binary preference with each trying to minimize their probability of losing. A group loss is indifferent; we prefer the agents play the game (and in this, case argue their points).\nDesign Criteria: We seek an approach to compromise in multi-agent systems that applies to the setting just described. The celebrated Myerson-Satterthwaite theorem (Arrow, 1970;Satterthwaite, 1975;Green and Laffont, 1977;Myerson and Satterthwaite, 1983) states that no mechanism exists that simultaneously achieves optimal efficiency (welfare-maximizing behavior), budget-balance (no taxing agents and burning side-payments), appeals to rational individuals (individuals want to opt-in to the mechanism), and is incentive compatible (resulting behavior is a Nash equilibrium). Given this impossibility result, we aim to design a mechanism that approximates weaker notions of these criteria. In addition, the mechanism should be decentralized, extensible to large populations, and adapt to learning agents with evolving incentives in possibly non-stationary environments.\nDesign: We formulate compromise as agents mixing their incentives with others. In other words, an agent may become incentivized to minimize a mixture of their loss and other agents' losses. We design a decentralized meta-algorithm to search over the space of these possible mixtures.\nWe model the problem of efficiency using price of anarchy. The price of anarchy, \u21e2 2 [1, 1), is a measure of inefficiency from algorithmic game theory with lower values indicating more efficient games (Nisan et al., 2007). Forcing agents to minimize a group (average) loss with a single local minimum results in a \"game\" with \u21e2 = 1. Note that any optimal group loss solution is also Paretoefficient. Computing the price of anarchy of a game is intractable in general. Instead, we derive a differentiable upper bound on the price of anarchy that agents can optimize incrementally over time. Differentiability of the bound makes it easy to pair the proposed mechanism with, for example, deep learning agents that optimize via gradient descent (Lerer and Peysakhovich, 2017;OpenAI et al., 2019). Budget balance is achieved exactly by placing constraints on the allowable mixtures of losses. We appeal to individual rationality in three ways. One, we initialize all agents to optimize only their own losses. Two, we include penalties for agents that deviate from this state and mix their losses with others. Three, we show empirically on several domains that opting into the proposed mechanism results in better individual outcomes. We also provide specific, albeit narrow, conditions under which agents may achieve a Nash equilibrium, i.e. the mechanism is incentive compatible, and demonstrate the agents achieving a Nash equilibrium under our proposed mechanism in a traffic network problem.\nThe approach we propose divides the loss mixture coefficients among the agents to be learned individually; critically, the agents do not need to observe or directly differentiate with respect to the other agent strategies. In this work, we do not tackle the challenge of scaling communication of incentives to very large populations; we leave this to future work. Under our approach, scale can be achieved through randomly sharing incentives according to the learned mixture weights or sparse optimization over the simplex (Pilanci et al., 2012;Kyrillidis et al., 2013;Li et al., 2016).\nOur Contribution: We propose a differentiable, local estimator of game inefficiency, as measured by price of anarchy. We then present two instantiations of a single decentralized meta-algorithm, one 1st order (gradient-feedback) and one 0th order (bandit-feedback), that reduce this inefficiency. This meta-algorithm is general and can be applied to any group of individual agent learning algorithms.\nThis paper focuses on how to enable a group of agents to respond to an unknown environment and minimize overall inefficiency. Agents with distinct losses may find their incentives well aligned to the given task, however, they may instead encounter a social dilemma (Sec. 3). We also show that our approach leads to interesting behavior in scenarios where agents may need to sacrifice team reward to save an individual (Sec. F.4) or need to form parties and vote on a new team direction (Sec. 3.4). Ideally, one meta-algorithm would allow a multi-agent system to perform sufficiently well in all these scenarios. The approach we propose, D3C (Sec. 2), is not that meta-algorithm, but it represents a holistic effort to combine critical ingredients that we hope takes a step in the right direction.2"}
{"introduction": "In recent years, the amount of video data has significantly increased. In addition to many cinema movies, news videos, and TV-shows, people frequently shoot events with their cellphone and share it with others on social media. To illustrate, it has been reported that every minute, 300 hours of new videos are uploaded to YouTube. Consequently, the user's ability to manage, search, and retrieve a specific item of content is limited. One remedy to this challenge can be an automatic video summarization algorithm where the goal is to create a shortened video that contains the essence of the original video. If fact, several commercial video summarization products are already on the market.\nVarious video summarization algorithms have been suggested in the literature. In most methods, the process consists of two main stages -segmenting the video into short video shots, and then choosing a subset of the shots to aggregate a summary (Otani et al., 2019). In order to be a good summary, this shot subset selection should optimize a certain property. For example, the selected shots should well represent the content of the video in the sense that each object from the original video has a similar object in the summary.\nVideo summarization approaches can generally be divided into supervised and unsupervised. Supervised methods include exploiting a ground truth importance score of each frame to train a model (Zhang et al., 2016;Zhao et al., 2018;Gygli et al., 2015) and utilizing auxiliary data such as web images (Khosla et al., 2013), titles (Song et al., 2015), category (Potapov et al., 2014) and any other side information (Yuan et al., 2017). A pitfall of a supervised approach is the necessity of expensive human-made labels. This drawback is especially restrictive because of the complicated and vague structure of a good summary, which requires a lot of labeled data.\nIn contrast, unsupervised methods do not need human-made labels as they follow rational guidelines for creating a good summary. One group of unsupervised algorithms maximizes the similarity between the original video and the generated summary using generative adversarial networks as evaluators (Mahasseni et al., 2017;Jung et al., 2019;Yuan et al., 2019), or by dictionary learning (Cong et al., 2011;Zhao & Xing, 2014). Another salient group of methods seek to minimize the total distance between shots and their nearest selected shots while satisfying the limit on the summary duration. Attempts in this direction include using submodular optimization (Gygli et al., 2015), reinforcement learning (Zhou et al., 2018) and clustering methods (Chheng, 2007;De Avila et al., 2011;Hadi et al., 2006).\nEven though several methods have been proposed for minimizing this total distance, no experiments have presented to directly measure the success of these methods in obtaining solutions with low total distance. Our experiments indicate that the best current existing method obtains solutions with total distance, which is, in some datasets around 10% worse than the optimal solution on average. Hence, we see that there is room for a new method that leads to better solutions.\nIn this paper, we propose ILS-SUMM, a novel unsupervised video summarization algorithm which uses the Iterated Local Search (ILS) optimization framework (Lourenc \u00b8o et al., 2003) to find a representative subset of shots. We formalize the following optimization problem: given the entire set of shots with varied shot duration, select a subset which minimizes the total distance between shots and their nearest selected shots while satisfying a knapsack constraint, i.e., the limit on the summary duration. This problem is known in the Operations Research field as the Knapsack Median (KM) problem, and is known to be NP-hard. A major challenge in performing a local search in the solution domain is the high chance of getting stuck in a local minimum because of the hard knapsack constraint (Fang et al., 2002). Therefore we use the ILS framework, which is the basis for several state-of-the-art algorithms for NP-hard problems (Lourenc \u00b8o et al., 2019).\nILS-SUMM creates a video summary by selecting shots that well represent the original video, using the ILS framework. First, it initializes a solution that satisfies the summary duration limit. Then, it applies steps of improvements by adding or replacing one shot at a time, while allowing only steps that obey the knapsack constraint. When a local minimum point is reached, the ILS executes a gentle, yet noticeable, perturbation to the current solution, to get out from the local minimum while trying to keep part of the high quality of the solution it obtained. We perform extensive experiments on the SumMe and TvSum benchmarks showing that our method finds solutions that are on average less than 2% worse than the optimal solution, which is significantly superior than the results of previous methods. Moreover, experiments on long real open-source movies indicate ILS-SUMM scalability. A Python implementation of the proposed method is released in [www.github.com/ArthurName/ILS-SUMM]."}
{"introduction": "Representation learning deals with uncovering useful underlying structures of data, and autoencoders (Hinton & Salakhutdinov, 2006) have been a staple in a variety of problems. While much research focuses on its use in unsupervised or semi-supervised settings with such diverse objectives as sparsity (Ranzato et al., 2007), generation (Kingma & Welling, 2013), and disentanglement (Chen et al., 2018), autoencoders are also useful in purely supervised settings-in particular, adding an auxiliary feature-reconstruction task to supervised classification problems has been shown to empirically improve generalization (Le et al., 2018); in the linear case, the theoretically quantifiable benefit matches that of simplistic norm-based regularization (Bousquet & Elisseeff, 2002;Rosasco & Poggio, 2009).\nIn this paper, we consider the inverse problem setting where the target space Y is high-dimensional; for instance, consider the multi-label classification tasks of object tagging, text annotation, and image segmentation. This is in contrast to the vast majority of works designed to tackle a high-dimensional feature space X (where commonly |X | |Y|, such as in standard classification problems). In this setting, the usual (and universal) strategy of learning to reconstruct features (Weston et al., 2012;Kingma et al., 2014;Le et al., 2018) may not be most useful: learning latent representations that encapsulate the variation within X does not directly address the more challenging problem of mapping back up to a higher-dimensional Y. Instead, we argue for leveraging intermediate representations that are compact and more easily predictable from features, yet simultaneously guaranteed to be predictive of targets. In the process, we provide a unified theoretical perspective on recent applications of autoencoders to label-embedding in static, high-dimensional classification problems (Yu et al., 2014;Girdhar et al., 2016;Yeh et al., 2017). Extending into the temporal setting, we further empirically demonstrate the generality of target-embedding for recurrent, multi-variate sequence forecasting.\nOur contributions are three-fold. First, we motivate and formalize the target-embedding autoencoder (TEA) framework: a general approach applicable to any underlying architecture. Second, we provide a theoretical learning guarantee in the linear case by demonstrating uniform stability; specifically, we obtain an O(1/N ) bound on instability by analogizing the benefit of the auxiliary reconstruction task to a form of regularization-without incurring additional bias from explicit shrinkage. Finally, we extend empirical validation of this approach beyond the domain of static classification: using the task of multivariate disease trajectory forecasting as case study, we experimentally validate the for wa rd (sh ar ed ) em be d predict Y < l a t e x i t s h a 1 _ b a s e 6 4 = \" ( n u l l ) \" > ( n u l l ) < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" ( n u l l ) \" > ( n u l l ) < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" ( n u l l ) \" > ( n u l l ) < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" ( n u l l ) \" > ( n u l l ) < / l a t e x i t > (b) Target-Embedding Autoencoder (a) Feature-Embedding Autoencoder advantage that TEAs confer on both linear and nonlinear architectures using real-world datasets with both continuous and discrete targets. To the best of our knowledge, we are the first to formalize and quantify the theoretical benefit of autoencoder-based target-representation learning in a purely supervised setting, and to extend its application to the domain of multivariate sequence forecasting."}
{"introduction": "Learning tasks with sequential data as inputs (and possibly outputs) arise in a wide variety of contexts, including computer vision, text and speech recognition, natural language processing, and time series analysis in the sciences and engineering. While recurrent gradient-based models have been successfully used in processing sequential data sets, it is well-known that training these models to process (very) long sequential inputs is extremely challenging on account of the so-called exploding and vanishing gradients problem (Pascanu et al., 2013). This arises as calculating hidden state gradients entails the computation of an iterative product of gradients over a large number of steps. Consequently, this (long) product can easily grow or decay exponentially in the number of recurrent interactions.\nMitigation of the exploding and vanishing gradients problem has received considerable attention in the literature. A classical approach, used in Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRUs) (Cho et al., 2014), relies on gating mechanisms and leverages the resulting additive structure to ensure that gradients do not vanish. However, gradients might still explode, and learning very long-term dependencies remains a challenge for these architectures (Li et al., 2018). An alternative approach imposes constraints on the structure of the hidden weight matrices of the underlying recurrent neural networks (RNNs), for instance by requiring these matrices to be unitary or orthogonal (Henaff et al., 2016;Arjovsky et al., 2016;Wisdom et al., 2016;Kerg et al., 2019). However, constraining the structure of these matrices might lead to significantly reduced expressivity, i.e., the ability of the model to learn complicated inputoutput maps. Yet another approach relies on enforcing the hidden weights to lie within pre-specified bounds, leading to control on gradient norms. Examples include Li et al. (2018), based on independent neurons in each layer, and Rusch & Mishra (2021a), based on a network of coupled oscillators. Imposing such restrictions on weights might be difficult to enforce, and weight clipping could reduce expressivity significantly. This brief survey highlights the challenge of designing recurrent gradient-based methods for sequence modeling which can mitigate the exploding and vanishing gradients problem, while at the same time being sufficiently expressive and possessing the ability to learn complicated input-output maps efficiently. We seek to address this challenge by proposing a novel gradient-based method.\nThe starting point for our method is the observation that realistic sequential data sets often contain information arranged according to multiple (time, length, etc., depending on the data and task) scales. Indeed, if there were only one or two scales over which information correlated, then a simple model with a parameter chosen to correspond to that scale (or, e.g., scale difference) should be able to model the data well. Thus, it is reasonable to expect that a multiscale model should be considered to process efficiently such multiscale data. To this end, we propose a novel gradientbased architecture, Long Expressive Memory (LEM), that is based on a suitable time-discretization of a set of multiscale ordinary differential equations (ODEs). For this novel gradient-based method (proposed in Section 2):\n\u2022 we derive bounds on the hidden state gradients to prove that LEM mitigates the exploding and vanishing gradients problem (Section 4); \u2022 we rigorously prove that LEM can approximate a very large class of (multiscale) dynamical systems to arbitrary accuracy (Section 4); and \u2022 we provide an extensive empirical evaluation of LEM on a wide variey of data sets, including image and sequence classification, dynamical systems prediction, keyword spotting, and language modeling, thereby demonstrating that LEM outperforms or is comparable to state-of-the-art RNNs, GRUs and LSTMs in each task (Section 5).\nWe also discuss a small portion of the large body of related work (Section 3), and we provide a brief discussion of our results in a broader context (Section 6). Much of the technical portion of our work is deferred to Supplementary Materials."}
{"introduction": "Learning to predict the physical motion of objects from data is an open area of research. Yet, recent (hierarchical) relation network based forward dynamics predictors (Battaglia et al., 2016;Chang et al., 2016;Mrowca et al., 2018;Li et al., 2019) seem to be a promising alternative to conventional physics engines that are key components of robot control, computer vision and reinforcement learning (RL) systems. Physics simulators, both traditional numerical solvers and learned prediction models, still suffer from insufficient accuracy in challenging scenarios. Small errors in the input and model can lead to dramatically different object trajectories. Take the orange ball that is falling on the blue wedge in Figure 1. Depending on where the orange ball starts or what bias the model has, the ball could either end up on the left or right side. Both are valid outcomes. However, deterministic physics engines will either predict one trajectory or the other. While it is important to reduce errors in each prediction, it is also important to acknowledge that uncertain situations might not have one but multiple possible outcomes. In machine learning, uncertainty-aware neural networks avoid deterministic point estimates by predicting distributions or by randomly sampling in the prediction interval. In the context of dynamics predictions, we propose to use Monte Carlo sampling based dropout on the model weights of a learned forward dynamics predictor to model uncertainty and sample multiple plausible trajectories for an initial state. To stabilize each trajectory and reduce error accumulation over long-time horizons, we use a state-invariant recurrent training mechanism. By feeding back predictions as input over multiple time steps, the model becomes more robust to its own prediction errors without the need for a hidden state. Finally, we introduce a new shape loss on the model predictions that constrains the pairwise distances between objects and object parts and greatly improves shape preservation and the stability of trajectories over long-time horizons. Our final fully differentiable forward dynamics model is able to sample multiple, more accurate and more stable trajectories over long-time horizons compared to existing baselines.\nAn accurate forward dynamics predictor that is able to predict a distribution of future states can be of great importance for robotic control. In model-free reinforcement learning, accomplishing tasks through random exploration is sample inefficient and hardly generalizable. Model-based methods promise greater generalization abilities, but suffer from deterministic world models that are hard to learn and fail in stochastic environments. With our stochastic forward dynamics predictor, we can move part of the sampling process into the environment, physically grounding the random exploration of model-free agents. As the agent is able to observe multiple trajectories at a given state without actually executing multiple actions, the sample efficiency is greatly improved while the stochasticity of each state and action is implicitly learned. We show on several control experiments that a model-free agent trained in our stochastic forward dynamics environment is not only able to better explore and learn faster but often also comes to better solutions than agents trained in deterministic environments.\nIn summary, (1) we propose a stochastic differentiable forward dynamics model that is able to generate multiple plausible trajectories via Monte Carlo (MC) based graph-convolutional dropout. (2) We greatly improve the accuracy and stability of long-term predictions by proposing a new fullyconnected shape loss term and training the model recurrently end-to-end in a state-invariant way.\n(3) We demonstrate how our stochastic dynamics model can be used to improve the efficiency and performance of model-free reinforcement learning agents on several physical manipulation tasks."}
{"introduction": "Reinforcement Learning (RL) is concerned with sequential decision making, in which the agent interacts with the environment to maximize its cumulative rewards. This framework has achieved tremendous empirical successes in various fields such as Atari games, Go and StarCraft (Mnih et al., 2013;Silver et al., 2017;Vinyals et al., 2019). However, state-of-the-art algorithms often require a large amount of training samples to achieve such a good performance. While feasible in applications that have a good simulator such as the examples above, these methods are limited in applications where interactions with the real environment are costly and risky, such as healthcare and robotics.\nOne solution to this challenge is sim-to-real transfer (Floreano et al., 2008;Kober et al., 2013). The basic idea is to train an RL agent in a simulator that approximates the real world and then transfer the trained agent to the real environment. This paradigm has been widely applied, especially in robotics (Rusu et al., 2017;Peng et al., 2018;Chebotar et al., 2019) and autonomous driving (Pouyanfar et al., 2019;Niu et al., 2021). Sim-to-real transfer is appealing as it provides an essentially unlimited amount of data to the agent, and reduces the costs and risks in training.\nHowever, sim-to-real transfer faces the fundamental challenge that the policy trained in the simulated environment may have degenerated performance in the real world due to the sim-to-real gap-the mismatch between simulated and real environments. In addition to building higher-fidelity simulators to alleviate this gap, domain randomization is another popular method (Sadeghi & Levine, 2016;Tobin et al., 2017;Peng et al., 2018;OpenAI et al., 2018). Instead of training the agent in a single simulated environment, domain randomization randomizes the dynamics of the environment, thus exposes the agent to a diverse set of environments in the training phase. Policies learned entirely in the simulated environment with domain randomization can be directly transferred to the physical world with good performance (Sadeghi & Levine, 2016;Matas et al., 2018;OpenAI et al., 2018).\nIn this paper, we focus on understanding sim-to-real transfer and domain randomization from a theoretical perspective. The empirical successes raise the question: can we provide guarantees for the sub-optimality gap of the policy that is trained in a simulator with domain randomization and directly transferred to the physical world? To do so, we formulate the simulator as a set of MDPs with tunable latent variables, which corresponds to unknown parameters such as friction coefficient or wind velocity in the real physical world. We model the training process with domain randomization as finding an optimal history-dependent policy for a latent MDP, in which an MDP is randomly drawn from a set of MDPs in the simulator at the beginning of each episode.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel formulation of sim-to-real transfer and establish the connection between domain randomization and the latent MDP model (Kwon et al., 2021). The latent MDP model illustrates the uniform sampling nature of domain randomization, and helps to analyze the sim-to-real gap for the policy obtained from domain randomization.\n\u2022 We study the optimality of domain randomization in three different settings. Our results indicate that the sim-to-real gap of the policy trained in the simulation can be o(H) when the randomized simulator class is finite or satisfies certain smoothness condition, where H is the horizon of the real-world interaction. We also provide a lower bound showing that such benign conditions are necessary for efficient learning. Our theory highlights the importance of using memory (i.e., history-dependent policies) in domain randomization.\n\u2022 To analyze the optimality of domain randomization, we propose a novel proof framework which reduces the problem of bounding the sim-to-real gap of domain randomization to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest.\n\u2022 As a byproduct of our proof, we provide the first provably efficient model-based algorithm for learning infinite-horizon average-reward MDPs with general function approximation (Algorithm 4 in Appendix C.3). Our algorithm achieves a regret bound of \u00d5(D \u221a d e T ), where T is the total timesteps and d e is a complexity measure of a certain function class F that depends on the eluder dimension (Russo & Van Roy, 2013;Osband & Van Roy, 2014)."}
{"introduction": "The regularisation technique known as dropout underpins numerous state-of-the-art results in deep learning (Hinton et al. 2012;Srivastava et al. 2014), and its application has received much attention in the form of optimisation (Wang & Manning 2013) and attempts at explaining or improving its approximation properties (Baldi & Sadowski 2013;Zolna et al. 2017;Ma et al. 2016). The dominant perspective today views dropout as either an implicit ensemble method (Warde-Farley et al. 2013) or averaging over an approximate Bayesian posterior (Gal & Ghahramani 2016a). Regardless of which view we take, dropout training is carried out the same way, by minimising the expectation of the loss over randomly sampled dropout masks. However, at test time these views naturally lead to different algorithms: the Bayesian approach computes an arithmetic average as it marginalises out the weight uncertainty, while the ensemble approach typically uses the geometric average due to its close relationship to the loss. Collectively they are called MC dropout and neither is clearly better than the other (Warde-Farley et al. 2013). A third way to make predictions is to \"turn dropout off\", that is, propagate expected values through the network in a single, deterministic pass. This deterministic (also known as standard) dropout is considered to be an excellent approximation to MC dropout. This situation is unsatisfactory as it does not provide theoretical grounding for dropout, without which the choice of dropout variant remains arbitrary. In this paper, we provide such theoretical foundations. First, we prove the dropout objective to be a common lower bound on the objectives of a family of infinitely many models. This family includes models corresponding to the three aforementioned methods of evaluation: the arithmetic averaging, the geometric averaging, and the deterministic. Thus by maximising the dropout objective we get a single set of parameters and many models that all have the same parameters but differ in how they make predictions. This allows us to train once and perform model selection at validation time by evaluating the different methods of making predictions corresponding to individual models in the family. Second, we turn the conventional perspective on its head by showing that while dropout training performs stochastic regularisation, the trained model is best viewed as deterministic, not as a stochastic model with a deterministic approximation. This paper is structured as follows. In \u00a72, we revisit variational dropout (Gal & Ghahramani 2016a) and demonstrate that, despite common perception, sharing of masks is not necessary, neither in theory nor in practice. Then, by recasting dropout in a simple conditional form, we highlight the counterintuitive role played by the variational posterior. \u00a73 contains our main contributions. Here we construct a family of conditional models whose MAP objectives are all lower bounded by the usual dropout objective, and identify a member of this family as best in terms of model fit. In \u00a74, we select the best of this family in terms of generalisation to improve language modelling. Finally, creating a cheap approximation to the bias of this model allows us to get better results from model tuning."}
{"introduction": "Many animals possess an intuitive understanding of the physical world. They use this understanding to accurately and rapidly predict events from sparse sensory inputs. In addition to general physical principles, many animals also learn specific models of new environments as they experience them over time. For example, they can explore an environment to determine which parts of it can be navigated safely and remember this knowledge for later reuse.\nAuthors have looked at equipping artificial intelligences (AIs) with analogous capabilities, but focusing mostly on performing predictions from instantaneous observations of an environment, such as a few frames in a video. However, such predictions can be successful only if observations are combined with sufficient prior knowledge about the environment. For example, consider predicting the motion of a bouncing ball. Unless key parameters such as the ball's elasticity are known a priori, it is impossible to predict the ball's trajectory accurately. However, after observing at least one bounce, it is possible to infer some of the parameters and eventually perform much better predictions.\nIn this paper, we are interested in learning intuitive physics in an entirely unsupervised manner, by passively watching videos. We consider situations in which objects interact with scenarios that can only be partially inferred from their appearance, but that also contain objects whose parameters cannot be confidently predicted from appearance alone (fig. 1). Then, we consider learning a system that can observe a few physical experiments to infer such parameters, and use this knowledge to perform better predictions in the future.\nOur model has three goals. First, it must learn without the use of any external or ad-hoc supervision. We achieve this by training our model from raw videos, using a video prediction error as a loss.\nSecond, our model must be able to extract information about a new scenario by observing a few experiments, which we formulate as meta-learning. We also propose a simple representation of the experiments based on the concept of \"dynamic image\" that allows to process long experiments more efficiently than using a conventional recurrent network. Third, our model must learn a good representation of physics without access to any explicit or external supervision. Instead, we propose three tests to support this hypothesis. (i) We show that the model can predict far in the future, which is a proxy to temporal invariance. (ii) We further show that the model can extend to scenarios that are geometrically much larger than the ones used for training, which is a proxy to spatial invariance. (iii) Finally, we show that the model can generalize to several In order to support these claims, we conduct extensive experiments in simulated scenarios, including testing the ability of the model to cope with non-trivial visual variations of the inputs. While the data is simpler than a real-world application, we nevertheless make substantial progress compared to previous work, as discussed in section 2. We do so by learning from passive, raw video data a good model of dynamics and collisions that generalizes well spatially, temporally, and to a variable number of objects. The scalability of our approach, via the use of the dynamic image, is also unique. Finally, we investigate the problem of learning the parameters of new scenarios on the fly via experiences and we propose an effective solution to do so."}
{"introduction": "Neural networks are universal function approximators in the asymptotic limit (Hornik et al., 1989), but their practical success is largely due to the use of strong structural priors such as convolution (Le-Cun et al., 1989), recurrence (Sutskever et al., 2014;Williams & Zipser, 1990;Werbos, 1990), and self-attention (Vaswani et al., 2017). These architectural constraints promote generalization and data efficiency to the extent that they align with the data domain. From this perspective, end-to-end learning relies on structural priors to scale, but the practitioner's toolbox is limited to functions that can be expressed differentiably. Here, we increase the size of that toolbox by introducing the Differentiable Digital Signal Processing (DDSP) library, which integrates interpretable signal processing elements into modern automatic differentiation software (TensorFlow). While this approach has broad applicability, we highlight its potential in this paper through exploring the example of audio synthesis.\nObjects have a natural tendency to periodically vibrate. Small shape displacements are usually restored with elastic forces that conserve energy (similar to a canonical mass on a spring), leading to harmonic oscillation between kinetic and potential energy (Smith, 2010). Accordingly, human hearing has evolved to be highly sensitive to phase-coherent oscillation, decomposing audio into spectrotemporal responses through the resonant properties of the basilar membrane and tonotopic mappings into the auditory cortex (Moerel et al., 2012;Chi et al., 2005;Theunissen & Elie, 2014). However, neural synthesis models often do not exploit this periodic structure for generation and perception."}
{"introduction": "Human beings can continuously learn different new tasks without forgetting the learnt knowledge of old tasks in their lifespan. Aiming to achieve this remarkable capability for the deep neural networks (DNNs), continual learning (CL) (Chen & Liu, 2018) has garnered much attention in recent years. Nevertheless, many existing CL methods still leave the DNN vulnerable to forget the knowledge of old tasks when learning new tasks. Such a phenomenon is known as 'Catastrophic Forgetting' (McCloskey & Cohen, 1989), which has become one of the major challenges for CL.\nMany approaches (e.g., (Rusu et al., 2016;Li & Hoiem, 2017;Dhar et al., 2019;Guo et al., 2020;Zeng et al., 2019)) have been proposed to address the forgetting issue, which can be generally divided into two classes depending on the network architecture, i.e., expansion methods and nonexpansion methods. In order to understand the fundamental limit of a fixed capacity neural network, we focus on non-expansion methods in this work. The basic idea for non-expansion methods is to constrain the gradient update either explicitly or implicitly when learning the new task, so as to minimize the introduced interference to old tasks. For example, the regularization-based methods (e.g., (Kirkpatrick et al., 2017;Serra et al., 2018)) penalize the modification on the most important weights of old tasks through model regularizations; experience-replay based methods (e.g., (Shin et al., 2017;Chaudhry et al., 2019)) constrain the gradient directions by replaying the data of old tasks during learning of new tasks, in the format of either real data or synthetic data from generative models; and orthogonal-projection based methods (e.g., (Farajtabar et al., 2020;Saha et al., 2021)) update the model with gradients in the orthogonal directions of old tasks, without the access to old task data. In particular, the recently proposed Gradient Projection Memory (GPM) (Saha et al., 2021) has demonstrated superior performance compared to other approaches.\nTo sufficiently minimize the interference to old tasks, most existing non-expansion methods (particularly the orthogonal-projection based methods), often put restrictive constraints on the optimization space of the new task, which may throttle the learning performance for the new task. A plausible conjecture is that such a scenario is likely to occur when the new task is strongly correlated with old tasks, and in this study we provide evidence to support this conjecture. The underlying rationale is as follows: The weights that are important to the new task are also important to the old tasks strongly correlated with the new task, which are often frozen to address the forgetting in the existing methods; however, they should be updated in the learning of the new task.\nTo tackle this challenge, a key insight is that for a new task that is strongly correlated with old tasks, although the model optimization space could be more restrictive, there should be better forward knowledge transfer from the correlated old tasks to the new task. With this insight, we propose an innovate continual learning approach to facilitate the forward knowledge transfer without forgetting. The main contributions can be summarized as follows:\n(1) Inspired by (Schulman et al., 2015), we introduce a novel notion of 'trust region' based on the norm of gradient projection onto the subspace spanned by task inputs, which selects the old tasks strongly correlated to the new task in a layer-wise and single-shot manner. Intuitively, the new task and the selected old tasks in the trust region have similar input features for the corresponding layer.\n(2) We propose a novel approach for the new task to leverage the knowledge of the strongly correlated old tasks in the trust region through a scaled weight projection. Particularly, a scaling matrix is learnt in each layer for the new task to scale the weight projection onto the subspace of old tasks in the trust region, in order to reuse the frozen weights of old tasks without modifying the model.\n(3) Building on the introduced trust region, scaled weight projection, and a module to construct task input subspace, we develop a continual learning approach, trust region gradient projection (TRGP), that jointly optimizes the scaling matrices and the model for the new task. To mitigate the forgetting issue further, the model is updated along the directions orthogonal to the subspaces of old tasks.\n(4) We evaluate TRGP on standard CL benchmarks using various network architectures. Compared to related state-of-the-art approaches, TRGP achieves substantial performance improvement on all benchmarks, and demonstrates universal improvement on all tasks. The superior performance indicates that TRGP can effectively promote the forward knowledge transfer while alleviating forgetting."}
{"introduction": "The ability to infer 3D properties such as geometry, texture, material, and light from photographs is key in many domains such as AR/VR, robotics, architecture, and computer vision. Interest in this problem has been explosive, particularly in the past few years, as evidenced by a large body of published works and several released 3D libraries (TensorflowGraphics by Valentin et al. (2019), Kaolin by J. et al. (2019), PyTorch3D by Ravi et al. (2020)).\nThe process of going from images to 3D is often called \"inverse graphics\", since the problem is inverse to the process of rendering in graphics in which a 3D scene is projected onto an image by taking into account the geometry and material properties of objects, and light sources present in the scene. Most work on inverse graphics assumes that 3D labels are available during training (Wang et al., 2018;Mescheder et al., 2019;Groueix et al., 2018;Wang et al., 2019;Choy et al., 2016), and trains a neural network to predict these labels. To ensure high quality 3D ground-truth, synthetic datasets such as ShapeNet (Chang et al., 2015) are typically used. However, models trained on synthetic datasets often struggle on real photographs due to the domain gap with synthetic imagery.\nTo circumvent these issues, recent work has explored an alternative way to train inverse graphics networks that sidesteps the need for 3D ground-truth during training. The main idea is to make (DIB-R in our work). We exploit StyleGAN as a synthetic data generator, and we label this data extremely efficiently. This \"dataset\" is used to train an inverse graphics network that predicts 3D properties from images. We use this network to disentangle StyleGAN's latent code through a carefully designed mapping network. graphics renderers differentiable which allows one to infer 3D properties directly from images using gradient based optimization, Kato et al. (2018); Liu et al. (2019b); Li et al. (2018); Chen et al. (2019). These methods employ a neural network to predict geometry, texture and light from images, by minimizing the difference between the input image with the image rendered from these properties. While impressive results have been obtained in Liu et al. (2019b); Sitzmann et al. ( 2019); Liu et al. (2019a); Henderson & Ferrari (2018); Chen et al. (2019); Yao et al. (2018); Kanazawa et al. (2018), most of these works still require some form of implicit 3D supervision such as multi-view images of the same object with known cameras. Thus, most results have been reported on the synthetic ShapeNet dataset, or the large-scale CUB (Welinder et al., 2010) bird dataset annotated with keypoints from which cameras can be accurately computed using structure-from-motion techniques.\nOn the other hand, generative models of images appear to learn 3D information implicitly, where several works have shown that manipulating the latent code can produce images of the same scene from a different viewpoint (Karras et al., 2019a). However, the learned latent space typically lacks physical interpretation and is usually not disentangled, where properties such as the 3D shape and color of the object often cannot be manipulated independently.\nIn this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable graphics renderers. We exploit a GAN, specifically StyleGAN (Karras et al., 2019a), as a generator of multi-view imagery to train an inverse graphics neural network using a differentiable renderer. In turn, we use the inverse graphics network to inform StyleGAN about the image formation process through the knowledge from graphics, effectively disentangling the GAN's latent space. We connect StyleGAN and the inverse graphics network into a single architecture which we iteratively train using cycle-consistency losses. We demonstrate our approach to significantly outperform inverse graphics networks on existing datasets, and showcase controllable 3D generation and manipulation of imagery using the disentangled generative model."}
{"introduction": "In the past decade, we have witnessed the tremendous success of deep Convolutional Neural Networks (CNNs) in many computer vision applications (Krizhevsky et al., 2012;Girshick et al., 2014;Long et al., 2015;He et al., 2017). The most common way of constructing a deep CNN is to stack a number of convolutional layers as well as other basic layers organized with the predefined feature connection topology. Along with great advances in CNN architecture design by manual engineering (Krizhevsky et al., 2012;He et al., 2016;Howard et al., 2017) and automatic searching (Zoph & Le, 2017;Pham et al., 2018;Howard et al., 2019), lots of prevailing classification backbones have been presented. Recent works (Wang et al., 2017;Hu et al., 2018b;Park et al., 2018;Woo et al., 2018;Yang et al., 2019;Chen et al., 2020) show that incorporating attention mechanisms into convolutional blocks can further push the performance boundaries of modern CNNs, and thus it has attracted great research interest in the deep learning community.\nThe well-known SENet (Hu et al., 2018b) uses an attention module consisting of squeeze and excitation operations to adaptively recalibrate the output features of convolutional layers, strengthening the representation power of a CNN via encouraging informative feature channels while suppressing less important ones. Numerous attentive feature recalibration variants (Woo et al., 2018;Park et al., 2018;Hu et al., 2018a) have been proposed since then. In Lin et al. (2020) and Quader et al. (2020), two attention extensions to modulate the convolutional weights instead of the output features are also presented. Unlike the aforementioned methods in which the number of convolutional parameters of a target network is fixed, dynamic convolution, which applies the attention mechanism over n additive convolutional kernels to increase the size and the capacity of a network while maintaining efficient inference, has recently become popular in optimizing efficient CNNs. This line of research is pioneered by Conditionally Parameterized Convolutions (CondConv) (Yang et al., 2019) and Dynamic Convolution (DyConv) (Chen et al., 2020) whose basic ideas are the same. Generally, unlike a regular convolutional layer which applies the same (i.e., static) convolutional kernel to all input samples, a dynamic convolutional layer learns a linear combination of n convolutional kernels weighted with their attentions conditioned on the input features. Despite significant accuracy improvements for light-weight CNNs, dynamic convolution designed in this way has two limitations. Firstly, the main limitation lies in the attention mechanism design. The dynamic property of CondConv and DyConv comes from computing convolutional kernels as a function of the input features. However, we observe that they endow the dynamic property to convolutional kernels through one dimension (regarding the convolutional kernel number) of the kernel space while the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. As a result, the weights of each convolutional kernel share the same attention scalar for a given input, limiting their abilities to capture rich contextual cues. That is, the potentials of dynamic convolutional property have not been fully explored by existing works, and thus they leave considerable room for improving the model performance. Secondly, at a convolutional layer, replacing regular convolution by dynamic convolution increases the number of convolutional parameters by n times. When applying dynamic convolution to a lot of convolutional layers, it will heavily increase the model size. To handle this limitation, Li et al. (2021) proposes a dynamic convolution decomposition method which can get more compact yet competitive models. Instead, in this paper we address both of the above limitations in a new perspective: formulating a more diverse and effective attention mechanism and inserting it into the convolutional kernel space.\nOur core contribution is a more generalized yet elegant dynamic convolution design called Omnidimensional Dynamic Convolution (ODConv). Unlike existing works discussed above, at any convolutional layer, ODConv leverages a novel multi-dimensional attention mechanism to learn four types of attentions for convolutional kernels along all four dimensions of the kernel space in a parallel manner. We show that these four types of attentions learnt by our ODConv are complementary to each other, and progressively applying them to the corresponding convolutional kernels can substantially strengthen the feature extraction ability of basic convolution operations of a CNN. Consequently, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters.\nAs a drop-in design, ODConv can be used to replace regular convolutions in many CNN architectures. It strikes a better tradeoff between model accuracy and efficiency compared to existing dynamic convolution designs, as validated by extensive experiments on the large-scale ImageNet classification dataset (Russakovsky et al., 2015) with various prevailing CNN backbones. ODConv also shows better recognition performance under similar model complexities when compared to other state-of-the-art attention methods for output feature recalibration (Woo et al., 2018;Hu et al., 2018b;Wang et al., 2020;Lin et al., 2020) or for convolutional weight modification (Ma et al., 2020;Lin et al., 2020;Quader et al., 2020). Furthermore, the performance improvements by ODConv for the pre-trained classification models can transfer well to downstream tasks such as object detection on the MS-COCO dataset (Lin et al., 2014), validating its promising generalization ability."}
{"introduction": "SGD is the main optimization algorithm for training deep neural networks, and understanding SGD is widely regarded as a key step on the path to understanding deep learning (Bottou, 2012;Zhang et al., 2018;Xing et al., 2018;Mori et al., 2021;Du et al., 2018;Allen-Zhu et al., 2018;Wojtowytsch, 2021a;b;Gower et al., 2021;Ziyin et al., 2022b;Gurbuzbalaban et al., 2021;Zou et al., 2021;Li et al., 2021;Feng and Tu, 2021). Despite its algorithmic simplicity (could be described by only two lines of equations), SGD is hard to understand. The difficulty is threefold: (1) SGD is discrete-time in nature, and discrete-time dynamics are typically much more complicated to solve than their continuous-time counterparts (May, 1976); (2) SGD noise is state-dependent (Ziyin et al., 2022b;Hodgkinson and Mahoney, 2020); and (3) the loss landscape can be nonlinear and non-convex, involving many local minima, saddle points, and degeneracies (the Hessian is not full-rank) in the landscape (Xing et al., 2018;Wu et al., 2017). Each of these difficulties is so challenging that very few works attempt to deal with all of them simultaneously. Most previous works on SGD are limited to the cases when the loss landscape is strongly convex (Ziyin et al., 2022b;Liu et al., 2021;Hoffer et al., 2017;Mandt et al., 2017), or when the noise is assumed to be Gaussian and time-independent (Zhu et al., 2019;Jastrzebski et al., 2017;Xie et al., 2021); for the works that tackle SGD in a non-convex setting, often strong conditions are assumed. The reliance on strong assumptions regarding each of the challenges means that our present understanding of SGD for deep learning could be speculative. This work aims to examine some commonly held presuppositions about SGD and show that when all the three challenging factors are taken together, many counter-intuitive phenomena may arise. Since most of these phenomena are potentially undesired, this work can also be seen as a worst-case analysis of SGD.\nIn this work, we study the behavior of SGD in toy landscapes with non-convex loss and multi-minima. We approach the problem of SGD convergence from a different angle from many of the related works: instead of studying when SGD will converge, our result helps answer the question of when SGD might fail. In particular, the problem setting considers discrete-time SGD close to saddle points, where the noise is due to minibatch sampling, and the learning rate is held constant throughout training. In the next section, we define the SGD algorithm and the necessary notations. In Sec. 3, we discuss the related works. A warmup example is provided in Sec. 4. Sec. 5 introduces the main results. Sec. 6 presents the numerical simulations, including a minimal example involving a neural network. We also encourage the readers to examine the appendix. Sec. A presents additional experiments. Sec. B presents a continuous-time analysis of the problems in the main text and is relevant for discussing the unique discrete-time features of SGD. Sec. C presents all the proofs."}
{"introduction": "Federated learning has become an important paradigm in large-scale machine learning where the training data remains distributed over a large number of clients, which may be mobile phones or network sensors (Kone\u010dn\u1ef3 et al., 2016b;a;McMahan et al., 2017;Mohri et al., 2019;Kairouz et al., 2019). A centralized model, here referred to as a server model, is then trained, without ever transmitting client data over the network, thereby providing some basic levels of data privacy and security.\nTwo important settings are distinguished in Federated learning (Kairouz et al., 2019, Table 1): the cross-device and the cross-silo settings. The cross-silo setting corresponds to a relatively small number of reliable clients, typically organizations, such as medical or financial institutions. In contrast, in the cross-device federated learning setting, the number of clients may be extremely large and include, for example, all 3.5 billion active android phones (Holst, 2019). Thus, in that setting, we may never make even a single pass over the entire clients' data during training. The cross-device setting is further characterized by resource-poor clients communicating over a highly unreliable network. Together, the essential features of this setting give rise to unique challenges not present in the cross-silo setting. Here, we are interested in the cross-device setting, for which we will formalize and study stochastic optimization algorithms.\nThe de facto standard algorithm for this setting is FEDAVG (McMahan et al., 2017), which performs multiple SGD updates on the available clients, before communicating to the server. While this approach can reduce the total amount of communication required, performing multiple steps on the same client can lead to 'over-fitting' to its atypical local data, a phenomenon known as client drift (Karimireddy et al., 2020). Furthermore, algorithmic innovations such as momentum (Sutskever et al., 2013;Cutkosky and Orabona, 2019), adaptivity (Kingma and Ba, 2014;Zaheer et al., 2018;Zhang et al., 2019), and clipping (You et al., 2017;2019;Zhang et al., 2020) are critical to the success of deep learning applications and need to be incorporated into the client updates, replacing the SGD update of FEDAVG. Perhaps due to such deficiencies, there exists a large gap in performance between the centralized setting, where data is centrally collected on the server, and the federated setting (Zhao et al., 2018;Hsieh et al., 2019;Hsu et al., 2019;Karimireddy et al., 2020).\nTo overcome such deficiencies, we propose a new framework, MIME, that mitigates client drift and adapts arbitrary centralized optimization algorithms, e.g. SGD with momentum or Adam, to the federated setting. In each local client update, MIME uses global statistics, e.g. momentum, and an 2. Each client is likely to participate at most once, due to the extremely large number of clients; furthermore, each individual client may have very little data of its own. 3. There may be a wide heterogeneity or non-i.i.d.-ness due to the difference of data distributions for the clients. Thus, our objective will be to minimize the following quantity within the fewest number of clientserver communication rounds:\nHere, f i denotes the loss function of client i and {\u03b6 i,1 , . . . , \u03b6 i,ni } its local data. Since the number of clients is extremely large, while size of each local data is rather modest, we represent the former as an expectation and the latter as a finite sum. In each round, the algorithm samples a subset of clients (of size S) and performs some updates to the server model. There is some inherent tension between the second and the third challenge outlined above: if there exists a client with arbitrarily different data whom we may never encounter during training, then there is no hope to actually minimize f . Thus for (1) to be tractable, it is necessary to assume bounded dissimilarity between different f i .\n(A1) G 2 -BGD or bounded gradient dissimilarity: there exists G \u2265 0 such that\nNext, we also characterize the variance in the Hessians. Note that if f i (\u2022; \u03b6) is L-smooth, (A2) is always satisfied with \u03b4 \u2264 2L and hence is more of a definition rather than an assumption. Note that however, in realistic examples we expect the clients to be similar and hence that \u03b4 L.\n(A2) \u03b4-BHD or bounded Hessian dissimilarity: Almost surely, f is \u03b4-weakly convex i.e. \u2207 2 f i (x) -\u03b4I and the loss function of any client i satisfies\nIn addition, we assume that f (x) is bounded from below by f and is L-smooth, as is standard."}
{"introduction": "The success of Convolutional Neural Networks (CNNs) (LeCun et al., 1998;2015;Schmidhuber, 2015;Krizhevsky et al., 2012) is a key factor for the rise of deep learning, attributed to their capability of exploiting translation symmetries, hereby introducing a strong inductive bias. Recent work has shown that designing CNNs to exploit additional symmetries via group convolutions has even further increased their performance (Cohen & Welling, 2016;2017;Worrall et al., 2017;Cohen et al., 2018;Kondor & Trivedi, 2018;Weiler et al., 2018;Bekkers et al., 2018;Bekkers, 2019;Weiler & Cesa, 2019). Graph neural networks (GNNs) and CNNs are closely related to each other via their aggregation of local information. More precisely, CNNs can be formulated as message passing layers (Gilmer et al., 2017) based on a sum aggregation of messages that are obtained by relative position-dependent linear transformations of neighbouring node features. The power of message passing layers is, however, that node features are transformed and propagated in a highly non-linear manner. Equivariant GNNs have been proposed before as either PointConv-type (Wu et al., 2019;Kristof et al., 2017) implementations of steerable (Thomas et al., 2018;Anderson et al., 2019;Fuchs et al., 2020) or regular group convolutions (Finzi et al., 2020). The most important component in these methods are the convolution layers. Although powerful, such layers only (pseudo1 ) linearly transform the graphs and non-linearity is only obtained via point-wise activations.\nIn this paper, we propose non-linear E(3) equivariant message passing layers using the same principles that underlie steerable group convolutions, and view them as non-linear group convolutions. Central to our method is the use of steerable vectors and their equivariant transformations to represent and process node features; we present the underlying mathematics of both in Sec. 2 and illustrate it in Fig. 1 on a molecular graph. As a consequence, information at nodes and edges can now be rotationally invariant (scalar) or covariant (vector, tensor). In steerable message passing frameworks, the Clebsch-Gordan (CG) tensor product is used to steer the update and message functions by geometric information such as relative orientation (pose). Through a notion of steerable node attributes we provide a new class of equivariant activation functions for general use with steerable feature fields (Weiler et al., 2018;Thomas et al., 2018). Node attributes can include information such as node velocity, force, or atomic spin. Currently, especially in molecular modelling, most datasets are build up merely of atomic number and position information. In this paper, we demonstrate the potential of enriching node attributes with more geometric and physical quantities. We demonstrate the effectiveness of SEGNNs by setting a new state of the art on n-body toy datasets, in which our method leverages the abundance of geometric and physical quantities available. We further test our model on the molecular datasets QM9 and OC20. Although here only (relative) positional information is available as geometric quantity, SEGNNs achieve state of the art on the IS2RE dataset of OC20, and competitive performance on QM9. For all experiments we provide extensive ablation studies.\nThe main contributions of this paper are: (i) A generalisation of equivariant GNNs such that node and edge attributes are not restricted to scalars. (ii) A new class of equivariant activation functions for steerable vector fields, based on the introduction of steerable node attributes and steerable multi-layer perceptrons, which permit the injection of geometric and physical quantities into node updates. (iii) A unifying view on various equivariant GNNs through the definition of non-linear convolutions. (iv) Extensive experimental ablation studies that shows the benefit of steerable over non-steerable (invariant) message passing, and the benefit of non-linear over linear convolutions.\nFigure 1: Commutation diagram for an equivariant operator \u03c6 applied to a 3D molecular graph with steerable node features (visualised as spherical functions); As the molecule rotates, so do the node features. The use of steerable vectors allows neural networks to exploit, embed, or learn geometric cues such as force and velocity vectors."}
{"introduction": "Transfer learning from pre-trained language models (PLMs) is now the prevalent paradigm in natural language processing, yielding strong performance on many tasks (Peters et al., 2018;Devlin et al., 2019;Qiu et al., 2020). The most common way to adapt general-purpose PLMs to downstream tasks is to fine-tune all the model parameters (full fine-tuning). However, this results in a separate copy of fine-tuned model parameters for each task, which is prohibitively expensive when serving models that perform a large number of tasks. This issue is particularly salient with the ever-increasing size of PLMs, which now range from hundreds of millions (Radford et al., 2019;Lewis et al., 2020) to hundreds of billions (Brown et al., 2020) or even trillions of parameters (Fedus et al., 2021).\nTo mitigate this issue, a few lightweight alternatives have been proposed to update only a small number of extra parameters while keeping most pretrained parameters frozen. For example, adapter tuning (Houlsby et al., 2019) inserts small neural modules called adapters to each layer of the pretrained network and only the adapters are trained at fine-tuning time. Inspired by the success of prompting methods that control PLMs through textual prompts (Brown et al., 2020;Liu et al., 2021a), prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) prepend an additional l tunable P v < l a t e x i t s h a 1 _ b a s e 6 4 = \" A T 6 P 8 x 3 K l 5 n U N b 4 K J f v 6 u P v v u g 0 = \" > A A A 1 p n i c l V t b d 9 v K d V b S t E m V t j l p H / u C F d k r T p e s Z f k c t 1 l 5 i i 7 U x a I k S q I u 9 q G P C 4 K b I C z c h B l C p B j 2 u c 9 9 b f 5 Y / k 3 2 z G C w 9 4 C Q z 4 r W s o T 5 v j 2 D u X w z + y M I D / M 4 E v L N m 7 / 8 5 K d / 9 7 O / / 4 e f / + I f 1 3 / 5 T / / 8 L 7 / 6 5 t f / e i O y a R H A d Z D F W X E 3 9 A X E U Q r X M p I x 3 O U F + M k w h t v h / Z 7 i b 0 s o R J S l f T n P 4 V P i h 2 k 0 j g J f I n Q 3 G C a L 3 v L z / e d v N t 5 s v d E / 3 u r F d n W x s V b 9 9 D 7 / + r v / H Y y y Y J p A K o P Y F + L 7 7 T e 5 / L T w C x k F M S z X B 1 M B u R / c + y F 8 j 5 e p n 4 D 4 t N A d X n o v E R l 5 4 6 z A f 6 n 0 N M p r L P x E i H k y x M j E l x P R 5 B T Y x n 0 / l e P f f 1 p E a T 6 V k A b m R u N p 7 M n M U 6 P 3 R l E B g Y z n e O E H R Y R 9 9 Y K J X / i B x D l a X 3 + p f r y z z q 1 3 u t M / 8 v Y 7 B 8 d n x / 3 j 8 7 M r T 1 P r b R 3 Z x L 9 q G G J z m C y x D e / U L + 4 9 g f f B G R Z e N v Y C P z f X a s Q F j K E o o j R U n R p F Z S R s 2 D g K p w X g g F J 4 D L I k 8 d P R Y o B g D G O 5 X C w G k H i v u n j 9 u + V y J S b A d Y D C R u 3 p U l t c E Y W T u r F L V W i L k l l u Y / p Z 3 h Y x z K T M E h u 0 q 0 s r c d W 4 f R v m P x c x t B H D 5 y I C G x E 8 F z G y E S M V g c t w h K O L 1 Q g 9 3 8 N 4 t e g w x m 0 y 8 n B u E r c N v F b g 8 v v t T 9 j K c O x t b K t G m s O e L R e D x C 9 C F J h f L A 6 O 7 5 p 9 w W s n B K X U D O m f 7 5 / r + w w k z K S W / q I A 7 L 0 i / m B u 7 L b Z 0 U 3 K S Z Y v B p 0 m 2 3 l A t v N 5 M S j K p 4 G I E u 8 B r 8 t 8 E i 1 f K e h P + G u 2 M m W d / K l R K 1 e 1 5 A S k / / V 6 d b V Z V W 3 0 q j X y 4 Q n n q r U r b l y u 4 p 6 5 e S N y 9 r Q S O V O R T 6 u R q 4 E r M S M d N G o n 1 Z 1 e t T X 9 8 N Q c V T N C T S z C D X S m 0 V k D n W t 0 3 k A T j S Z N F W g 0 b c Z O p R L H F L s 0 2 / T s i F e C c i d I 9 b 0 R M j I V 8 T b + M P Z p 7 p p h q i o L a m s J s Z X G E O O 9 w 8 1 5 o M 8 6 c x j i S Q 2 b X p w 9 Q v E 6 w F y 2 t T 7 A n a p P K x h v b C / M u f g / A y w t 9 P Z o q 4 6 n Q C T 9 e M s 7 w D N W S M x D 6 k g V 6 i B E 3 r R 4 Y F s 8 a L a o a f m Y 2 X t u v K 3 u K j w b 5 O H w q s J b W + N h 6 o + o y s a 3 G 9 + t V N u s 6 9 i r b 3 l T 3 + n h X J l k 8 d X p w I R i O l 9 l F m c + W h q w E 2 J q X 9 n a V y 2 1 L 2 0 t n S c f s z p 5 b d U T Y + 4 u 9 M z U q e 2 Z q W k 2 O C k A m k 2 y 9 j a + X W 2 R Z o 2 1 / e 1 q 2 3 7 q A S 6 C q t w y Z f B g x m x D n h + 0 0 8 4 0 z 6 H w V D u m m U 7 V T K e t m R 2 v 8 B 9 p 3 h u N v X 7 9 2 i + z a O R N h c r 4 0 d j L M y E i d G e m 6 T z 2 M S N V 7 T / f O 2 V S c k x Q L W N U j K l e x f z N g 6 w a 2 q s b 2 v v R h n D M a Q j a 2 p h Y Y d r Q c N 0 j l I q l b V O v X z 8 r E + y d H 4 c Z m r J J 0 j J O 5 E z v 6 q C v D p Q 1 t T L S H d v U T k t T V v D 2 f j i I u q 2 v H w Z 9 p 9 L O j 1 Z a m V Q 0 D L I a O V O f Q k 1 3 1 d X X F s X U b 6 q 3 V 9 f v u f X t S O s b Y K / V 9 b M d r g Q H U a z E G q s L t C s Y o K 6 q 9 s Z x l h W a 1 l e G 1 5 d V A F L 4 m W P F 5 M g C N 0 L l c w I / X u w 3 A 0 o / j k Y 8 4 L O 5 L p K F o Z Y r T Y K Q 7 R U 0 s 6 x H B L l Q 1 j E X U Z y l 2 v b h 1 G I T W e K V f h F h E g O r b 8 x f C 2 P c 0 q x I s N U X A 4 R e L O 1 0 F g 3 a J 2 b o M k N i A p c J i B m 5 z I g Y c B k g Z u w y Y 2 J C l w m J m b j M h J j I Z S J i v r j M F 2 L u X e a e m N h l 4 q W W c Z F 4 k c A d i 5 9 b R 3 N 1 2 J k V 3 P S + T I X 0 R l n 6 W + m p z 4 8 o x 7 k 6 e Z y F 8 Z K q 7 d R 9 n M G p s P a H B S L L 1 j A a j y 9 Z T G r l n z 2 k w C m 0 9 q c H I t P W s B q P V 5 m l t u c T l E s 4 9 e x K D k W 7 r W Q x G v 6 2 n M R g R t 5 7 H Y J T c e i K D k X P r m Q x G 0 6 2 n M h h h t 5 7 L Y N T d e j K D k X j r 2 Q x G 5 6 2 n M x i x t 5 7 P Y B T / / A m N e 6 G I g t q h J D u 0 P 3 Z o 2 y S 7 B O 8 y e I / g P Q b v E 7 z P 4 A 7 B H Q Y f E H z A 4 E O C D x l 8 R P A R g 4 8 J P m b w e 4 L f M / i E 4 B M G d w n u M v i U 4 F M G n x F 8 x u B z g s 8 Z 3 C O 4 x + A L g i 8 Y f E n w J Y O v C L 5 i c J / g P o O v C b 5 m 8 A 3 B N w y + J f i W w X c E 3 z H 4 A 8 E f G P y R 4 I / P H 6 + u 6 M C o j m l 0 h + l X S 4 9 x u 5 z b c 7 k 9 z u 2 7 3 D 7 n O i 7 X 4 d y B y x 1 w 7 t D l D j l 3 5 H J H n D t 2 u W P O v X e 5 9 5 w 7 c b k T z n V d r s u 5 U 5 c 7 5 d y Z y 5 1 x 7 t z l z j n X c 7 k e 5 y 5 c 7 o J z l y 5 3 y b k r l 7 v i X N / l + p y 7 d r l r z t 2 4 3 A 3 n b l 3 u l n N 3 L n f H u Q 8 u 9 4 F z H 1 3 O y v 6 G W 4 j y C f T n C P z s + q a u W 2 Y p L O z n W Y s l U w M N E k o a t S d W u O u H 1 b P R i t B P U y 1 c R b P A o U H I n m h z g g i Z E m 1 J E C E r U l Y d J A O i 7 Q c i Z D u 0 6 U C E z I a 2 G o i Q x S i r T r I e f j E I 2 Q l t J h A h E 6 E t B C I x m x 6 D k G H Q d g G R l E 2 r Q T I 2 S Q Y h S 6 A N A S J k B L Q N Q I T S v 0 7 + i A i 2 D g a h V F 9 W q 8 X W q j Q I p X W d 1 B G h Z K 5 T O S K U w n U C R 4 Q S t 0 7 b i L S Z V N e d l n 6 c T 9 R 6 6 7 + 1 M M t h p Z n q Q b w B 6 R M Y P b C o q N h P h i N V w 1 w Q k S U Q K l z / J V g r V a n U A t g g I v i b I B G F i a q q / x J s 9 V x / S 1 A N Z L H g / V 8 o s d o S i j W g E g p 1 x A a 1 U A K 1 J R T o m E o o z p B K K M w J l b C 7 r K 8 o y C 9 U Q j H e s 7 l Z K B H W I 1 8 o A d o S T i a b R R R f x q Z k o U R n S y i 6 B y q h 4 A o 2 U w s l t H q C F k p k t o Q T z a Y Z B V Z S C c X 1 S C U U 1 o x K K K o 5 l V B Q T 8 v q G 2 Z M v z O D 6 9 S L O q O U q x M u I p R o d Z p F h N K r T q 6 I U F L V K R U R S q U 6 k S J C C V S n T 0 Q o b e q k i Q g l S 5 0 q E a E U q R M k I p Q Y d V p E h N K h T o a I U B L U K R A R S n 0 6 8 S F C C U + n O 0 Q o z e k k p 1 9 q q J E L g 1 B K 0 w k N E U p k O o 0 h Q u l L J y 9 E K G n p l I U I p S q d q B C h B K X T E y K U l n R S Q o S S k U 5 F i F A K 0 g k I k Y 9 s B S l d D H m 2 S H p 1 t u i x b J F 0 7 d Z X T L f a / v X g q j 2 s u C u z j 7 W K + p A K 9 d 7 F P g S x X w C K a r K j T i C 8 o / G A Y h y p J 6 i Q B t k o S k N s z J / G C h H j + j p Z L o R 6 + H s F 8 r k G h l k 8 + r F m h r P l o v n l p s T + m W / K d T q t 2 t M P r 6 u h S W M 7 U 8 H U L 3 c t R v q X e x a j H S D 3 L U Z 7 Q H Y s R r t A H l i M 9 o E 8 t B j t B H l k M d o L 8 t h i t B v k e 4 v R f p A n F q M d I b s W o z 0 h T y 1 G u 0 K e W Y z 2 h T y 3 G O 0 M 2 b M Y 7 Q 1 5 Y T H a H f L S Y r Q / 5 J X F a I f I v s V o j 8 h r i 9 E u k T c W o 3 0 i b y 1 G O 0 X e W Y z 2 i v x g M d o t 8 q P F j F F D I R 8 W f j 4 x b G g / / g b O p 5 B w l 8 G k i 3 C P w S S N c J / B p I 6 w w 2 A S S H j A Y N J I e M h g k k l 4 x G B S S n j M Y B J L + J 7 B p J f w h M E k m b D L Y F J N e M p g E k 5 4 x m D S T n j O Y J J P 2 G M w K S i 8 Y D C J K L x k M O k o v G I w S S n s M 5 j U F F 4 z m A Q V 3 j C Y N B X e M p h k F d 4 x m J Q V f m A w i S v 8 y G D 7 Q Q C P t s q q i f r h y p C J S + w S S t o S e 4 S S t M Q + o V p Z L 7 1 9 / Q X H V I D n e w K k h 7 e O Y e R 1 N r 0 h B L 7 C 5 S Q S 3 m M 2 j U c I Y Q k 8 o b 8 O Q S 8 5 L T z 1 o l w W Y 0 P q 7 T K Y 5 e g t 9 X e 8 9 p v 2 D t 2 R R C s O C C X N i k N C S b L i i F B S r D g m l A Q r 3 h N K e h U n h J J c R Z d Q U q s 4 J Z T E K s 4 I J a 2 K c 0 J J q q J H K\nw s U / p x I u e o 9 K u N g X V M J F v q Q S L u 4 V l X B R + 1 T C x b y m E i 7 i D Z V w 8 W 6 p h I t 2 R y V c r A 9 U w k X 6 y O 5 X + a / K e 6 k l A 7 5 k 0 v g w P G j U r t Z v x O L W N u i m 9 x j J S T a V H p o g 7 x E T X Q 6 F a 5 O A f J L j k a r b y\no 2 3 7 O s n E l / j 5 3 b 2 F E 9 n j k b 1 l W 2 e S b A T x 1 w a i A + q R m N L K f a q g 3 t e C c h n F I 6 g i B 7 p Q 9 7 6 u g c e E z I K J L 9 T L 6 / 5 U Z v p z F R R O D x s v k e c m p u 5 j V W W 1 A y N w 4 k y x J a 5 A A g 8 d G 2 e K q I V A P 1 l z g 2 M / j / 0 A l v X r N 9 0 K W H o v v e r a n d 7 G S 8 d L 7 l 9 c r i v Y G z 7 d J n u 5 5 L m 9 c W o m O Z v j B h k X S / s 0 z i U K C J f 1 8 7 U m F U g a o y p F 4 w i K Z t M i G 8 v E n 1 G k B Z p x m C w y / c a T e f S 2 2 k o e T 9 X o n 9 T z A Z c 9 6 S 7 5 6 0 4 n 3 Z U F v P E L 6 o E q N N u X + M c v c O 2 L j E V e r S z A X l Y S r Q p K o L d Z P C 7 8 R D 2 m m j x m B d p W 4 c + F 9 6 L 7 w 9 s X 6 l U f / R 8 / p q l 5 Z V X k u P 5 C v 2 r 2 Y g B x z G L s Y 9 K X 3 i 4 m Q N z y q f o 1 x / 0 O i X r l T X l j 0 y i L V u + b Z t N Q 5 0 x t l S M J m 7 p 5 k X m j D F R z j 9 F 9 l M M o 8 r c a L 1 l n R R K r J / 3 L R f e H N 8 s W M k t B c d t t n H z U 9 d 6 2 c b l i 8 h Z G a 6 H 7 w y B K x 3 L e 3 D q 5 X 6 h H x n h s + G q z X A G e t c I P w Y t S L 8 0 q m y 9 h t u X t T T K h p i d T B j C Y e P v 4 i T i F 3 w p v m G X 3 W + v O Q 5 7 z X J 3 O W f E f q P E i 1 B 3 A v 4 N N d f W 1 Q H V O m k C 8 a m 9 S q x X D 9 O 9 n I v o o q L 5 6 H T A G O f C H u M / i 7 H F Y g H + / b r B M H Y K 5 n O O x P r A X h q n h 0 i 9 w / B M 8 / N f X P 3 + z s d 3 8 n 0 q r F z d v t 7 b / c 2 v 7 4 r u N P + 5 W / 4 v p F 2 v / v v a b t V d r 2 2 v / t f b H t a O 1 3 t r 1 W r A W r / 3 f 2 v + v / f n d q 3 d n 7 6 7 f 3 Z r Q n / 6 k q v N v a 8 7 P u / / + K x h A z T o = < / l a t e x i t > P k < l a t e x i t s h a 1 _ b a s e 6 4 = \" W K J Q j f 2 / z u X c c s 8 n g P m y h 3 Z Z T I k = \" > A A A 1 p n i c l V t b d 9 v K d V b S t E m V t j l p H / u C F d k r T p e s Z f k c t 1 l 5 i i 7 U x a I k S q I u 9 q G P C 4 K b I C z c h B l C p B j 2 u c 9 9 b f 5 Y / k 3 2 z G C w 9 4 C Q z 4 r W s o T 5 v j 2 D u X w z + y M I D / M 4 E v L N m 7 / 8 5 K d / 9 7 O / / 4 e f / + I f 1 3 / 5 T / / 8 L 7 / 6 5 t f / e i O y a R H A d Z D F W X E 3 9 A X E U Q r X M p I x 3 O U F + M k w h t v h / Z 7 i b 0 s o R J S l f T n P 4 V P i h 2 k 0 j g J f I n Q 3 G C a L 3 v J z + f m b j T d b b / S P t 3 q x X V 1 s r F U / v c + / / u 5 / B 6 M s m C a Q y i D 2 h f h + + 0 0 u P y 3 8 Q k Z B D M v 1 w V R A 7 g f 3 f g j f 4 2 X q J y A + L X S H l 9 5 L R E b e O C v w X y o 9 j f I a C z 8 R Y p 4 M M T L x 5 U Q 0 O Q W 2 c d 9 P 5 f j 3 n x Z R m k 8 l p I G 5 0 X g a e z L z 1 O i 9 U V R A I O M 5 X v h B E W F f v W D i F 3 4 g c Y 7 W 1 1 + q H + + s c + u d 7 v S P v P 3 O w f H Z c f / 4 / O z K 0 9 R 6 W 0 c 2 8 a 8 a h t g c J k t s w z v 1 i 3\ny m z x A b t 6 t J K X D V u 3 4 b 5 z 0 U M b c T w u Y j A R g T P R Y x s x E h F 4 D I c 4 e h i N U L P 9 z B e L T q M c Z u M P J y b x G 0 D r x W 4 / H 7 7 E 7 Y y H H s b 2 6 q R 5 r B n y 8 U g 8 Y s Q B e Y X i 4 P j u 2 Z f 8 N o J Q S k 1 Q / r n + + f 6 P g M J M 6 m l v y g A e 6 + I P 5 g b u 2 1 2 d J N y k u W L Q a f J d h 6 Q 7 X x e D I r y a S C i x H v A 6 z K f R M t X C v o T / p q t T F k n f 2 r U y l U t O Q H p f 7 1 e X W 1 W V R u 9 a o 1 8 e M K 5 a u 2 K G 5 e r u G d u 3 o i c P a 1 E z l T k 0 2 r k a u B K z E g H j d p J d a d X b U 0 / P D V H 1 Y x Q E 4 t w A 5 1 p d N Z A 5 x q d N 9 B E o 0 l T B R p N m 7 F T q c Q x x S 7 N N j 0 7 4 p W g 3 A l S f W + E j E x F v I 0 / j H 2 a u 2 a Y q s q C 2 l p C b K U x x H j v c H M e 6 L P O H I Z 4 U s O m F 2 e P U L w O M J d t r Q 9 w p + r T C s Y b 2 w t z L v 7 P A E s L v T 3 a q u M p E E k / 3 v I O 8 I w V E v O Q O l K F O g i R N y 0 e 2 B Y P m i 1 q W j 5 m 9 p 4 b b 6 u 7 C s 8 G e T i 8 q v D W 1 n i Y + i O q s v H t x n c r 1 T b r O v b q W 9 7 U d 3 o 4 V y Z Z f H U 6 M K G Y z l e Z x Z m P l g b s h J j a V 7 b 2 V U v t S 1 t L 5 8 n H r E 5 e W / X E m L s L P T N 1 a n t m a p o N T g q A Z p O s v Y 1 v V 1 u k W W N t f 7 v a t p 9 6 g I u g K r d M G T y Y M d u Q 5 w f t t D P N c y g 8 1 Y 5 p p l M 1 0 2 l r Z s c r / E e a 9 0 Z j r 1 + / 9 s s s G n l T o T J + N P b y T I g I 3 Z l p O o 9 9 z E h V + 8 / 3 T p m U H B N U y x g V Y 6 p X M X / z I K u G 9 u q G 9 n 6 0 I R x z G o K 2 N i Z W m D Y 0 X P c I p W J p 2 9 T r 1 8 / K B H v n x 2 G G p m y S t I w T O d O 7 O u i r A 2 V N r Y x 0 x z a 1 0 9 K U F b y 9 H w 6 i b u v r h 0 H f q b T z o 5 R 5 e z s r / h F q J 8 A v 0 5 A j + 7 v q n r l l k K C / t 5 1 m L J 1 E C D h J J G 7 Y k V 7 v p h 9 W y 0 I v T T V A t X 0 S x w a B C y J 9 q c I E\n0 6 2 2 f z 2 4 a g 8 r 7 s r s Y 6 2 i P q R C v X e x D 0 H s F 4 C i m u y o E w j v a D y g G E f q C S q k Q T a K 0 h A b 8 6 e x Q s S 4 v k 6 W C 6 E e / l 6 B f K 6 B Y R a P f q y Z 4 W y 5 a H 6 5 K b F / 5 p t y n U 6 r 9 v T D 6 2 p o 0 t j O V D D 1 y 1 2 L k f 7 l n s V o B 8 h 9 i 9 E e k B 2 L 0 S 6 Q B x a j f S A P L U Y 7 Q R 5 Z j P a C P L Y Y 7 Q b 5 3 m K 0 H + S J x W h H y K 7 F a E / I U 4 v R r p B n F q N 9 I c 8 t R j t D 9 i x G e 0 N e W I x 2 h 7 y 0 G O 0 P e W U x 2 i G y b z H a I / L a Y r R L 5 I 3 F a J / I W 4 v R T p F 3 F q O 9 I j 9 Y j H a L / G g x Y 9 R Q y I e F n 0 8 M G 9 q P v 4 H z K S T c Z T D p I t x j M E k j 3 G c w q S P s M J g\nW S 2 9 f f 8 E x F e D 5 n g D p 4 a 1 j G H m d T W 8 I g a 9 w O Y m E 9 5 h N 4 x F C W A J P 6 K 9 D 0 E t O C 0 + 9 K J f F 2 J B 6 u w x m O X p L / R 2 v / a a 9 Q 3 c k 0 Y o D Q k m z 4 p B Q k q w 4 I p Q U K 4 4 J J c G K 9 4 S S X s U J o S R X 0 S W U 1 C p O C S W x i j N C S a v i n F C S q u g R S k o V F 4 S S U M U l o a R T c U U o y V T 0 C S W V i m t C S a T i h l D S q L g l l C Q q 7 g g l h Y o P h J J A x U d C 6 + c z K b p B 0 B 8 s f P N k p r K G Q L 6 g 6 3 4 k U K Z x h 0 o o 4 F 0 q o X D 3 q I S C 3 a c S i q l D J R T R A Z V Q P I d U Q t E c U Q n F c k w l F M l 7 K q E 4 T q i E o u h S C c V w S i U U w R m V c P H P q Y S L 3 q M S L v Y F l X C R L 6 m E i 3 t F J V z U P p V w M a + p h I t 4 Q y V c v F s q 4 a L d U Q k X 6 w O V c J E + s v t V / q v y X m r J g C + Z N D 4 M D x q 1 q / U b s b i 1 D b r p P U Z y k k 2 l h y b I e 8 R E l 0 P h 2 i Q g n + R 4 p O r 2 s t a A D l y\nx 3 4 e c H T E S y m I I 3 T U d Q x H P 1 h t P I l 7 4 X Q g o F 5 i B V j g Q q f T h V C a n 5 B q a v X h J U L 2 o W y U I X d D p U r U K S R 0 W E i d C p X 7 + / O 5 z r J K j f G V E 3 w a z Z a N u + T j L x J X 5 + d 2 / h R P Z 4 Z G / Z 1 p k k G 0 H 8 t Y H o g H o k p r R y n y q o 9 7 W g X E b x C K r I g S 7 U v a 9 r 4 D E h s 2 D i C / X y u j + V m f 5 c B Y X T w 8 Z L 5 L m J q f t Y V V n t w A i c O F N s i S u Q w E P H x p k i a i H Q T 9 b c 4 N j P Y z + A Z f 3 6 T b c C l t 5 L r 7 p 2 p 7 f x 0 v G S + x e X 6 w r 2 h k + 3 y V 4 u e W 5 v n J p J z u a 4 Q c b F 0 j 6 N c 4 k C w m X 9 f K 1 J B Z L G q E r R O I K i 2 b T I x j L x Z x R p g W Y c J o t M v / F k H r 2 t t p L H U z X 6 J / V 8 w G V P u k v + u t N J d 2 U B b / y C e q A K z f Y l / v E L X P s i Y 5 F X K w u w l 5 V E q 4 I S 6 G 0 W j w s / U Y + p J o 9 Z g b Z V + H P h v e j + 8 P a F e t V H / 8 e P a W p e W R U 5 r r / Q r 5 q 9 G E A c s x j 7 m P S l t 4 s J E L d 8 q n 7 N c b 9 D o l 5 5 U 9 7 Y N M q i 1 f u m 2 T T U O V N b 5 U j C p m 5 e Z N 4 o A 9 X c Y 3 Q f 5 T C K / K 3 G S 9 Z Z k c T q S f 9 y 0 f 3 h z b K F z F J Q 3 H Y b J x 9 1 v b d t X K 6 Y v I X R W u j + M I j S s Z w 3 t 0 7 u F + q R M R 4 b v t o s V 4 B n r f B D 8 K L U S 7 P K 5 k u Y b X l 7 k 0 y o 6 c m U A Q w m 3 j 5 + I k 7 h t 8 I b Z t n 9 1 r r z k O c 8 V 6 d z V v w H a r w I d Q f w 7 2 B T X X 0 t U J 2 T J h C v 2 p v U a s U w / f u Z i D 4 K q q 9 e B 4 x B D v w h 7 r M 4 e x w W 4 N + v G y x T h 2 A u 5 3 i s D + y F Y W q 4 9 A s c / w Q P / / X 1 z 9 9 s b D f / p 9 L q x c 3 b r e 3 / 3 N q + + G 7 j j 7 v V / 2 L 6 x d q / r / 1 m 7 d X a 9 t p / r f 1 x 7 W i t t 3 a 9 F q z F a / + 3 9 v 9 r f 3 7 3 6 t 3 Z u + t 3 t y b 0 p z + p 6 v z b m v P z 7 r / / C m H n z U U = < / l a t e x i t > P v < l a t e x i t s h a 1 _ b a s e 6 4 = \" O K / j 7 6 f t H g D 4 R n E d I S g X d 3 S S V m s = \" > A A A 1 t H i c l V t b c 9 v K k V a y 2 d 2 s s p e T 3 X 3 L C y q y K 0 5 K V l k + x 9 m t P E U X 6 m J R d + p i H / q 4 Q L A J w s J N m C F I i m G e 8 z P 2 d f O P 8 m + 2 Z w a D 7 g E h n 1 p V W c J 8 X 0 9 j L l 9 P N 0 F 4 k M e R k G / e / O 0 n P / 2 7 n / 3 9 P / z j z / 9 p / R f / / C / / + m / f / P L f b 0 U 2 K Q K 4 C b I 4 K + 4 H v o A 4 S u F G R j K G + 7 w A P x n E c D d 4 2 F P 8 X Q m F i L K 0 J + c 5 f E r 8 M I 1 G U e B L h D 5 / 8 5 / 9 Q b K 4 W 3 5 e 9 C X M 5 G K Y T d P l 8 v M 3 G 2 + 2 3 u g f b / V i u 7 r Y W K t + L j 7 / 8 r u / 9 I d Z M E k g l U H s C / H 9 9 p t c f l r 4 h Y y C G J b r / Y m A 3 A 8 e / B C + x 8 v U T 0 B 8 W u j h L 7 2 X i A y 9 U V b g v 1 R 6 G u U 9 F n 4 i x D w Z o G X i y 7 F o c g p s 4 7 6 f y N F / f 1 p E a T 6 R k A b m R q N J 7 M n M U 2 v h D a M C A h n P 8 c I P i g j H 6 g V j v / A D i S u 2 v v 5 S / X h n n T v v d K d 3 5 O 1 3 D o 7 P j n v H 5 2 f X n q b W 2 w a y i X / V N M T m I F m i D + / U L x 4 8 g f f B 9 R Z e N v I C P z f X a s Y F j K A o o j R U g x p G Z S S s 2 S g K J w X g h F K Y B l m S + O l w 0 U c w h p F c L h Z 9 S L x X X b z + 7 X K 5 Y h P g P k B h r f Z 0 q 8 2 u i M J x 7 e x K N d q s Z J Z b m 1 6 W t 1 k M M i m z x B r t 6 t a K X T V v 3 5 r 5 z 1 k M r M X g O Y v A W g T P W Q y t x V B Z 4 D Y c 4 e x i N U P P 9 9 B e b T q M M G i G H q 5 N 4 v r A a w U u v 9 / + h F 4 G I 2 9 j W z l p T n u 2 X P Q T v w h R Y H 6 x O D i + b 4 4 F r x 0 T l F L T p H e + f 6 7 v o + N P S 3 9 R A I 5 e E X 8 w N 3 Z 9 d r R L O c 7 y R b / T Z D u P y H Y w m o v y q S + i x H v E 6 z I f R 8 t X C v o T / p q t L F k n f 2 r 0 y l U v O Q b p f 7 1 f 3 W 1 W d R u + a r V 8 f M K 1 a h 2 K a 5 c r u 2 d u 3 r C c P a 1 Y z p T l 0 6 r l q u G K z V A b D d t J d a d X b a 4 f n 5 q z a l q o h U W 4 g c 4 0 O m u g c 4 3 O G 2 i i 0 a S p A o 2 m T d u J\nr G G 1 s L 8 y 5 + O c + t h Y 6 P N q 6 4 y k Q S T / e 8 g 7 w j B U S 8 5 A 6 U o U 6 C J E 3 H g + s x 4 O m R 0 3 L a W b v u f G 2 u q v w r J G H 0 6 s a b 2 2 P x 4 k / p C 4 b 3 2 5 8 t 9 J t s + 5 j r 7 7 l r r 7 T 0 7 k 2 y e K r y 4 E J x Q y + y i z O e r Q 4 s A t i e l / b 3 t c t v a 9 s L 5 0 n p 1 m d v L b q h T F 3 F 3 p l 6 t T 2 z N I 0 H Y 4 L g K Z L 5 m / j 2 1 W P t G r M 9 7 e r v v 3 U A 9 w E 1 b l l y e D R z N m a P D 9 p x 8 8 k z 6 H w l B / j p l O 5 6 b S 5  \nr J G H 0 6 s a b 2 2 P x 4 k / p C 4 b 3 2 5 8 t 9 J t s + 5 j r 7 7 l r r 7 T 0 7 k 2 y e K r y 4 E J x Q y + y i z O e r Q 4 s A t i e l / b 3 t c t v a 9 s L 5 0 n p 1 m d v L b q h T F 3 F 3 p l 6 t T 2 z N I 0 H Y 4 L g K Z L 5 m / j 2 1 W P t G r M 9 7 e r v v 3 U A 9 w E 1 b l l y e D R z N m a P D 9 p x 8 8 k z 6 H w l B / j p l O 5 6 b S 5 2 f E K f 0 r r 3 n D 2 + v V r v 8 y i o T c R K u N H I y / P h I i w V j O u 8 9 j H j F T 5 f 3 5 0 q k j J M U G 1 z F E x p n t l 8 / + e Z O V o r 3 a 0 9 6 O O c M 5 p C L q 0 M b b C + N B w P S K U i q W t q 9 e v n 5 U J j s 6 P w w y\nx n 8 g e A P D P 5 I 8 M f n j 1 d X d G B U x z S 6 w / S r p c e 4 X c 7 t u d w e 5 / Z d b p 9 z H Z f r c O 7 A 5 Q 4 4 d + h y h 5 w 7 c r k j z h 2 7 3 D H n 3 r v c e 8 6 d u N w J 5 7 o u 1 + X c q c u d c u 7 M 5 c 4 4 d + 5 y 5 5 y 7 c L k L z l 2 6 3 C X n r l z u i n P X L n f N u Z 7 L 9 T h 3 4 3 I 3 n L t 1 u V v O 3 b n c H e f u X e 6 e c x 9 c 7 g P n P r q c l f 0 t L y H\nu H a 0 d r F 2 s x a s P a 3 9 z 9 r / r v 3 1 3 e / f 9 d 8 F 7 8 C Y / v Q n V Z / / W H N + 3 q X / B 3 k w 0 0 g = < / l a t e x i t > Wdown < l a t e x i t s h a 1 _ b a s e 6 4 = \" 8 T U x x 9 D 9 G T w j k T M x K K 4 4 w R q i y p 0\nr V q H 4 t r l y u 6 F m z c s Z 8 8 r l j N l + b x q u W q 4 Y j P S R q N 2 U t 3 p T Z v r x + f m r J o W a m E R b q A z j c 4 a 6 F y j 8 w a a a D R p q k C j a d N 2 K p U 4 p j i k 2 a Z n Z 7 x i l D t G a u w N k 5 H p i L f x h 7 F P a 9 c 0\n4 8 d V j 7 R q z P e P q 7 7 9 1 A P c B N W 5 Z c n g 0 c z Z m r w 8 a c f P N M + h 8 J Q f 4 6 Z T u e m 0 u d n x C v + J 1 r 3 h 7 O 3 b t 3 6 Z R S N v K l T G j 8 Z e n g k R Y a V m X O e x j x m p 8 v / y 6 F S R k m O C a p m j Y k z 3 y u a P n m T l a K 9 2 t P e L j n D O a Q i 6 t D G 2 w v j Q c D 0 i l I q l r a u 3 b 1 + U C Y 7 O j 8 M M i 7 J J 0 j J P 5 M z o a q P v T p S 5 W p n p j n W 1 0\nH q i g 6 M 6 p h G d 5 h + t f Q Y t 8 u 5 P Z f b 4 9 y + y + 1 z r u N y H c 4 d u N w B 5 w 5 d 7 p B z R y 5 3 x L l j l z v m 3 C e X + 8 S 5 E 5 c 7 4 V z X 5 b q c O 3 W 5 U 8 6 d u d w Z 5 8 5 d 7 p x z P Z f r c e 7 C 5 S 4 4 d + l y l 5 y 7 c r k r z v V d r s + 5 a 5 e 7 5 t y N y 9 1 w 7 t b l b j l 3 5 3 J 3 n P v s c p 8 5\n2 6 q X P J 6 q 2 T + r 5 w M u e 9 J d 8 t e d T r o r G 3 j j F z Q C 1 W j 6 l / j H L 3 D v i 4 x Z X q 1 s w F 5 W E q 0 a S q C 3 W T w u / E Q 9 p p o 8 Z Q W W r c K f C + 9 V 9 + f 3 r 9 S r P v o / f k x T 8 8 q q y H H / h X 7 V 7 N U A 4 p j Z 2 M e k r 7 1 d T I A Y 8 q n 6 N c d 4 h 0 S 9 8 q Z q Y\ny z k e 6 w N 7 Y Z g a L v 0 C 5 z / B w 3 9 9 / e s P G 9 v N / 6 m 0 e n H z f m v 7 X 7 a 2 L z 5 s / G G 3 + l 9 M v 1 n 7 h 7 V / X H u z t r 3 2 r 2 t / W D t a 6 6 1 d r w V r s 7 X / X v u f t f / 9 + O H j / U f / Y 2 B M f / 2 r q s / f r z k / H + P / B 9 W g 0 m E = < / l a t e x i t >"}
{"introduction": "Monoclonal antibodies are increasingly adopted as therapeutics targeting a wide range of pathogens such as SARS-CoV-2 (Pinto et al., 2020). Since the binding specificity of these Y-shaped proteins is largely determined by their complementarity-determining regions (CDRs), the main goal of computational antibody design is to automate the creation of CDR subsequences with desired properties. This problem is particularly challenging due to the combinatorial search space of over 20 60 possible CDR sequences and the small solution space which satisfies the desired constraints of binding affinity, stability, and synthesizability (Raybould et al., 2019).\nThere are three key modeling questions in CDR generation. The first is how to model the relation between a sequence and its underlying 3D structure. Generating sequences without the corresponding structure (Alley et al., 2019;Shin et al., 2021) can lead to sub-optimal performance (Ingraham et al., 2019), while generating from a predefined 3D structure (Ingraham et al., 2019) is not suitable for antibodies since the desired structure is rarely known a priori (Fischman & Ofran, 2018). Therefore, it is crucial to develop models that co-design the sequence and structure. The second question is how to model the conditional distribution of CDRs given the remainder of a sequence (context). Attention-based methods only model the conditional dependence at the sequence level, but the structural interaction between the CDR and its context is crucial for generation. The last question relates to the model's ability to optimize for various properties. Traditional physics-based methods (Lapidoth et al., 2015;Adolf-Bryfogle et al., 2018) focus on binding energy minimization, but in practice, our objective can be much more involved than binding energies (Liu et al., 2020).\nIn this paper, we represent a sequence-structure pair as a graph and formulate the co-design task as a graph generation problem. The graph representation allows us to model the conditional dependence between a CDR and its context at both the sequence and structure levels. Antibody graph generation poses unique challenges because the global structure is expected to change when new nodes are inserted. Previous autoregressive models (You et al., 2018;Gebauer et al., 2019) cannot modify a generated structure because they are trained under teacher forcing. Thus errors made in the previous steps can lead to a cascade of errors in subsequent generation steps. To address these problems, we propose a novel architecture which interleaves the generation of amino acid nodes with the prediction of 3D structures. The structure generation is based on an iterative refinement of a global graph rather than a sequential expansion of a partial graph with teacher forcing. Since the context sequence is long, we further introduce a coarsened graph representation by grouping nodes into blocks. We apply graph convolution at a coarser level to efficiently propagate the contextual information to the CDR residues. After pretraining our model on antibodies with known structures, we finetune it using a predefined property predictor to generate antibodies with specific properties.\nWe evaluate our method on three generation tasks, ranging from language modeling to SARS-CoV-2 neutralization optimization and antigen-binding antibody design. Our method is compared with a standard sequence model (Saka et al., 2021;Akbar et al., 2021) and a state-of-the-art graph generation method (You et al., 2018) tailored to antibodies. Our method not only achieves lower perplexity on test sequences but also outperforms previous baselines in property-guided antibody design tasks."}
{"introduction": "After AlexNet was proposed, several other more sophisticated Convolutional Networks (ConvNets), e.g. VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), ResNet (He et al., 2015), DenseNet (Huang et al., 2017), consecutively achieved lower error rates on classification benchmarks such as CIFAR (Krizhevsky, 2009), SVHN (Netzer et al., 2011) and Ima-geNet (Krizhevsky et al., 2012). The witnessed reduction of the classification error, however, was not accompanied by a similar increase of generalization and robustness to common corruptions or perturbations in the test images. When test images contain artefacts that are not present in the training data (e.g. shot noise, JPEG compression, blur), the performance of SOTA networks, like ResNet or DenseNet, degrade relatively more than that of AlexNet (Hendrycks & Dietterich, 2019).\nRobustness of DeepNets and ConvNets is gaining attention from researchers in machine learning and computer vision. One point of analysis concerns adversarial attacks that consist of very small, visually imperceptible perturbations of the input signals such that confuse the classification model (Akhtar & Mian, 2018). Several adversarial attacks were proposed based on the knowledge of the classification models (Goodfellow et al., 2015) or on iterative methods (Kurakin et al., 2016;Madry et al., 2018). Class-specific and universal black box attacks (Moosavi-Dezfooli et al., 2017) were also proposed. As new types of adversarial perturbations are designed, defense algorithms need to be developed (Lu et al., 2017;Metzen et al., 2017).\nIn contrast to adversarial perturbations, common corruptions and perturbations are modifications of the input signals due to typical artefacts generated by sensor noise, illumination changes, transformations determined by changes of camera perspective (e.g. rotations or elastic transformations), quality loss due to compression, among others. These can be considered as average cases of ad-versarial attacks and are very common in computer vision tasks. In this study, we focus on the robustness of ConvNets to common corruptions and perturbations and demonstrate how the use of inhibition-augmented filters increases the robustness of existing ConvNet models. We use a layer that implements a local-response suppression mechanism (Strisciuglio et al., 2020), named pushpull, to replace some convolutional layers in existing ConvNets. The design of this layer was inspired by neurophysiological evidence of the push-pull inhibition exhibited by some neurons in the early part of the visual system of the brain (Hirsch et al., 1998). Lauritzen & Miller (2003) stated that 'push-pull inhibition [...] acts to sharpen spatial frequency tuning, [...] and increase the stability of cortical activity'. These neurons have inhibitory interneurons with receptive fields of opposite polarity. The interneurons suppress the responses of the associated neurons in noisy local patterns; that is they pull the push responses. From an engineering point of view, the push-pull inhibition can be considered as a band-pass filter. A computational model of the push-pull inhibition was introduced in image processing operators for contour and line (Azzopardi et al., 2014;Strisciuglio et al., 2019) detection that are robust to various types of noise. Early results on augmenting ConvNets with push-pull inhibition were reported in (Strisciuglio et al., 2020), where only the first convolutional layer of existing networks was replaced and preliminary experiments on the MNIST and CIFAR images were reported.\nWe deploy a push-pull layer in the state-of-the-art (wide-)ResNet and DenseNet architectures by replacing the convolutional layer in the entry layer of the networks and all convolutional layers inside the first residual or dense block. The number of layers and parameters of the modified networks remains the same of their original counterpart. We train several models with and without the push-pull layers, using the images from ImageNet and CIFAR training sets, and test them on the ImageNet-C/P and CIFAR-10-C/P benchmark test sets (Hendrycks & Dietterich, 2019), which contain images with several types of corruption and perturbation. Furthermore, we combine the push-pull layer with other techniques to improve the robustness performance of ConvNets, namely the data augmentation techniques AutoAugment (Cubuk et al., 2018) and cutout (Devries & Taylor, 2017), and low-pass anti-aliasing filters (Zhang, 2019), of which we provide more details in Section 2. We show how combined data-and architecture-related actions have jointly a positive effect to further improve the robustness and generalization performance of existing models.\nOur contributions are twofold: a) new residual and dense layers in which push-pull filters replace the original convolutions, b) a thorough analysis of the impact of the push-pull inhibition on the robustness of existing models, and of its combination with other strategies for model robustness.\nThe rest of the paper is organized as follows. We discuss related works in Section 2 and provide details of the push-pull layer and modified residual and dense layers in Section 3. We report the experiments and results in Section 4 and Section 5. Finally, we draw conclusions in Section 6."}
{"introduction": "State-of-the-art generative adversarial networks (GANs) are able to synthesize such high quality images that humans may have a difficult time distinguishing them from natural images (Brock et al., 2018;Karras et al., 2019). Not only can GANs produce pretty pictures, but they are also useful for applied tasks from projecting noisy images onto the natural image manifold to generating training data (Samangouei et al., 2018;Sixt et al., 2018;Bowles et al., 2018). Similarly, massive transformer models are capable of performing question-answering and translation (Brown et al., 2020). In order for GANs and text generators to be valuable, they must generate diverse data rather than memorizing a small number of samples. Diverse data should contain a wide variety of semantic content, and its distribution should not concentrate around a small subset of modes from the true image distribution.\nA number of metrics have emerged for evaluating GAN-generated images and synthetic text. However, these metrics do not effectively quantify data diversity, and they work on a small number of specific benchmark tasks (Salimans et al., 2016;Heusel et al., 2017). Diversity metrics for synthetic text use only rudimentary tools and only measure similarity of phrases and vocabulary rather than semantic meaning (Zhu et al., 2018). Our novel contributions can be summarized as follows:\n\u2022 We design a framework (RND) for comparing diversity of datasets using random network distillation. Our framework can be applied to any type of data, from images to text and beyond. RND does not suffer from common problems that have plagued evaluation of generative models, such as vulnerability to memorization, and it can even be used to evaluate the diversity of natural data (not synthetic) since it does not require a reference dataset.\n\u2022 We validate the effectiveness of our method in a controlled setting by synthetically manipulating the diversity of GAN-generated images. We use the same truncation strategy employed by BigGAN to increase FID scores, and we confirm that this strategy indeed decreases diversity. This observation calls into question the usefulness of such popular metrics as FID scores for measuring diversity.\n\u2022 We benchmark data, both synthetic and natural, using our random distillation method. In addition to evaluating the most popular ImageNet-trained generative models and popular language models, we evaluate GANs in the data scarce regime, i.e. single-image GANs, which were previously difficult to evaluate. We also evaluate the diversity of natural data."}
{"introduction": "Many domains in science and engineering use numerical simulations to model empirically observed phenomena. These models are designed by domain experts and are built to produce mechanistic insights. However, in many cases, some parameters of the simulator cannot be experimentally measured and need to be inferred from data. A principled way to identify parameters that match empirical observations is Bayesian inference. However, for many models of interest, one can only sample from the model by simulating a (stochastic) computer program, but explicitly evaluating the likelihood p(x|\u03b8) is intractable. Traditional methods to perform Bayesian inference in such simulation-based inference (SBI), also known as likelihood-free inference scenarios, include Approximate Bayesian computation (ABC) (Beaumont et al., 2002) and synthetic likelihood (SL) (Wood, 2010) methods. However, these methods generally struggle with high-dimensional data and typically require one to design or learn (Chen et al., 2021) summary statistics and distance functions.\nRecently, several methods using neural density(-ratio) estimation have emerged. These methods train neural networks to learn the posterior (SNPE, Papamakarios & Murray, 2016;Lueckmann et al., 2017;Greenberg et al., 2019), the likelihood (SNLE, Papamakarios et al., 2019;Lueckmann et al., 2019a), or the likelihood-to-evidence ratio (SNRE, Thomas et al., 2021;Hermans et al., 2020;Durkan et al., 2020;Miller et al., 2022).\nTo improve the simulation efficiency of these methods, sequential training schemes have been proposed: Initially, parameters are sampled from the prior distribution to train an estimation-network. Subsequently, new samples are drawn adaptively to focus training on specific regions in parameter space, thus allowing the methods to scale to larger models with more parameters.\nIn practice, however, it has remained a challenge to realize the full potential of these sequential schemes: For sequential neural posterior estimation (SNPE) techniques, the loss function needs to be adjusted across rounds (Greenberg et al., 2019), and it has been reported that this can be problematic if the proposal distribution is very different from prior, and lead to 'leakage' of probability mass into regions without prior support (Durkan et al., 2020). Both sequential neural likelihood (SNLE) and likelihood-ratio (SNRE) methods require MCMC sampling, which can become prohibitively slow-MCMC sampling is required for each round of simulations, which, for high-dimensional models, can take more time than running the simulations and training the neural density estimator.\n-Figure 1: Illustration of SNVI. We first learn the likelihood p(x|\u03b8) for any \u03b8. We then use variational inference to learn the posterior distribution by minimizing a general divergence measure D. The obtained posterior distribution is sampled with sampling importance resampling (SIR) to run new simulations and refine the likelihood estimator.\nOur goal is to provide a method which combines the advantages of posterior-targeting methods and those targeting likelihood(-ratios): Posterior targeting methods allow rapid inference by providing a functional approximation to the posterior which can be evaluated without the need to use MCMC sampling. Conversely, a key advantage of likelihood(-ratio) targeting methods is their flexibilitylearned likelihoods can e.g. be used to integrate information from multiple observations, or can be used without retraining if the prior is changed. In addition, they can be applied with any activelearning scheme without requiring modifications of the loss-function.\nWe achieve this method by combining likelihood(-ratio) estimation with variationally learned inference networks using normalizing flows (Rezende & Mohamed, 2015;Papamakarios et al., 2017;Durkan et al., 2019a) and sampling importance resampling (SIR) (Rubin, 1988). We name our approach Sequential Neural Variational Inference (SNVI). We will show that our simulation-based inference methods are as accurate as SNLE and SNRE, while being substantially faster at inference as they do not require MCMC sampling. In addition, real-world simulators sometimes produce invalid outputs, e.g. when a simulation fails. We introduce a strategy that allows likelihood(-ratio) targeting methods (such as SNVI) to deal with such invalid simulation outputs.\nA recent method termed \"Sequential Neural Posterior and Likelihood Approximation\" (SNPLA) also proposed to use variational inference (VI) instead of MCMC to speed up inference in likelihoodtargeting methods (Wiqvist et al., 2021). While this proposal is related to our approach, their VI objective is based on the reverse Kullback Leibler (rKL) divergence for learning the posterior. As we also show on benchmark tasks, this leads to mode-seeking behaviour which can limit its performance. In contrast, we show how this limitation can be overcome through modifying the variational objective in combination with using SIR for adjusting posteriors.\nAfter an introduction on neural network-based simulation-based inference (SBI) and variational inference (Sec. 2), we present our method, Sequential Neural Variational Inference (SNVI) (Sec. 3). In Sec. 4.2, we empirically show that SNVI is significantly faster than state-of-the-art SBI methods while achieving similar accuracy on benchmark tasks. In Sec. 4.3, we demonstrate that SNVI is scalable, and that it is robust to invalid simulation outputs: We obtain the posterior distribution of a complex neuroscience model with one order of magnitude fewer simulations than previous methods."}
{"introduction": "Code summarization is the task of generating a readable summary that describes the functionality of a snippet. Such task requires a high-level comprehension of a source code snippet thus it is an effective task to evaluate whether a Deep Learning Model is able to capture complex relations and structures inside code. Programming languages are context-free formal language, an unambiguous representation, Abstract Syntax Tree (AST), could be derived from a source code snippet. A parse tree based representation of code is precise and without noise. An AST accurately describes the structure of a snippet and relationships between tokens which provides valuable supplementary information for code understanding.\nUsing graph representations of source code has been the focus of multiple methods that perform code summarization. For example, Alon et al. (2019a) encoded AST paths between tokens and aggregated them by an attention mechanism. Huo et al. (2020)  extract AST features, however the cross-modal interaction (Veli\u010dkovi\u0107, 2019) is very limited since the AST and code features are independently extracted by separate models then simply concatenated or summed.\nIn this paper we propose a novel architecture GN-Transformer shown in Figure 2 to fuse Graph information with an equivalent sequence representation. In summary:\n\u2022 We extend Graph Networks (GN) (Battaglia et al., 2018) to a novel GN-Transformer architecture that is a sequence of GN encoder blocks followed by a vanilla Transformer decoder. \u2022 We propose a novel method for early fusion of the AST representation and that of a code snippet sequence called Syntax-Code Graph (SCG) \u2022 We evaluate our approach on the task of code summarization and outperform the previous state of the art in two datasets and across three metrics. We denote '+' as a residual connection followed by a normalization layer. In 'Node embeddings of graph batch', each black bar represents the nodes embedding of a graph in the input batch. Blue dots represent token nodes, grey dots denote padding. Nodes embedding in the grey box are fetched as input to the decoder and AST nodes embedding (red dots) are discarded.\nWe evaluated our model on Java and Python datasets used by Ahmad et al. (2020). We compared our results to those of Ahmad et al. (2020). Two qualitative results are presented in Figure 1. We make available our code, trained models and pre-processed datasets in our supplementary package, and we will open-source it after the review process concludes."}
{"introduction": "Deep Reinforcement Learning (DRL) (Sutton & Barto, 2018) achieves remarkable success in a variety of tasks. However, in most successful applications, DRL requires millions of interactions with the environment. In real-world applications such as navigation (Mirowski et al., 2018) and healthcare (Yu et al., 2019), acquiring a large number of samples by following a possibly suboptimal policy can be costly and dangerous. Alternatively, practitioners seek to develop RL algorithms that learn a policy based solely on an offline dataset, where the dataset is typically available. However, directly adopting online DRL algorithms to the offline setting is problematic. On the one hand, policy evaluation becomes challenging since no interaction is allowed, which limits the usage of on-policy algorithms. On the other hand, although it is possible to slightly modify the off-policy value-based algorithms and sample solely from the offline dataset in training, such modification typically suffers from a significant performance drop compared with their online learning counterpart (Levine et al., 2020). An important reason for such performance drop is the so-called distributional shift. Specifically, the offline dataset follows the visitation distribution of the behavior policies. Thus, estimating the Q-functions of the corresponding greedy policy with the offline dataset is biased due to the difference in visitation distribution. Such bias typically leads to a significant extrapolation error for DRL algorithms since the estimated Q-function tends to overestimate the out-of-distribution (OOD) actions (Fujimoto et al., 2019).\nTo tackle the distributional shift issue in offline RL, previous successful approaches typically fall into two categories, namely, policy constraints (Kumar et al., 2019;Fujimoto & Gu, 2021) and conservative methods (Kumar et al., 2020;Yu et al., 2020;2021). Policy constraints aim to restrict the learned policy to be close to the behavior policy, thus reducing the extrapolation error in policy evaluation. Conservative methods seek to penalize the Q-functions for OOD actions in policy evaluation and hinge on a gap-expanding property to regularize the OOD behavior. Nevertheless, since policy constraints explicitly confine the policy to be close to the behavior policy, such method tends to be easily affected by the non-optimal behavior policy. Meanwhile, although the conservative algorithms such as CQL (Kumar et al., 2020) do not require policy constraints, CQL equally penalizes the OOD actions and lacks a precise characterization of the OOD data, which can lead to overly conservative value functions. To obtain a more refined characterization of the OOD data, uncertainty quantification is shown to be effective when associated with the model-based approach (Yu et al., 2020;Kidambi et al., 2020), where the dynamics model can be learned in static data thus providing more stable uncertainty in policy evaluation. Nevertheless, model-based methods need additional modules and may fail when the environment becomes high-dimensional and noisy. In addition, the uncertainty quantification for model-free RL is more challenging since the Q-function and uncertainty quantifier need to be learned simultaneously (Yu et al., 2021).\nTo this end, we propose Pessimistic Bootstrapping for offline RL (PBRL), an uncertainty-driven model-free algorithm for offline RL. To acquire reliable Q-function estimates and their corresponding uncertainty quantification, two components of PBRL play a central role, namely, bootstrapping and OOD sampling. Specifically, we adopt bootstrapped Q-functions (Osband et al., 2016) for uncertainty quantification. We then perform pessimistic Q-updates by using such quantification as a penalization. Nevertheless, solely adopting such penalization based on uncertainty quantification is neither surprising nor effective. We observe that training the Q-functions based solely on the offline dataset does not regularize the OOD behavior of the Q-functions and suffers from the extrapolation error. To this end, we propose a novel OOD sampling technique as a regularizer of the learned Qfunctions. Specifically, we introduce additional OOD datapoints into the training buffer. The OOD datapoint consists of states sampled from the training buffer, the corresponding OOD actions sampled from the current policy, and the corresponding OOD target based on the estimated Q-function and uncertainty quantification. We highlight that having such OOD samples in the training buffer plays an important role in both the Q-function estimation and the uncertainty quantification. We remark that, OOD sampling controls the OOD behavior in training, which guarantees the stability of the trained bootstrapped Q-functions. We further show that under some regularity conditions, such OOD sampling is provably efficient under the linear MDP assumptions.\nWe highlight that PBRL exploits the OOD state-action pairs by casting a more refined penalization over OOD data points, allowing PBRL to acquire better empirical performance than the policyconstraint and conservatism baselines. As an example, if an action lies close to the support of offline data but is not contained in the offline dataset, the conservatism (Kumar et al., 2020) and policy constraint (Fujimoto et al., 2019) methods tend to avoid selecting it. In contrast, PBRL tends to assign a small Q-penalty for such an action as the underlying epistemic uncertainty is small. Hence, the agent trained with PBRL has a higher chance consider such actions if the corresponding value estimate is high, yielding better performance than the policy constraint and conservatism baselines. Our experiments on the D4RL benchmark (Fu et al., 2020) show that PBRL provides reasonable uncertainty quantification and yields better performance compared to the state-of-the-art algorithms."}
{"introduction": "Language models (LMs) play an important role across a range of natural language processing tasks, such as text summarization (Rush et al., 2015;Gehrmann et al., 2018), neural machine translation (NMT) (Sutskever et al., 2014;Cho et al., 2014a), and image captioning (Herdade et al., 2019;Anderson et al., 2018;Xu et al., 2015). Existing neural LMs are often built on either recurrent units, as used in recurrent neural networks (RNNs) (Cho et al., 2014b;Hochreiter and Schmidhuber, 1997), or purely the attention mechanism based modules, as used in the Transformer and its various generalizations (Vaswani et al., 2017;Dai et al., 2019;Radford et al., 2019). Moving beyond traditional recurrent units, Transformers mainly rely on attention mechanisms, in which the direct connections between long-distance word pairs might ease optimization and enable the learning of longrange dependency (Dai et al., 2019), and have recently demonstrated state-of-the-art performances on a wide range of sequence modeling tasks.\nRather than representing a token using a predefined word embedding vector, each Transformer layer creates a contextualized representation of each token by attending to different parts of the input segment (Ethayarajh, 2019), allowing the same word to take different representations depending on its context. However, Transformers are usually trained on disjoint fixed-length segments, without any information flow across segments (Dai et al., 2019), limiting the contextualization within the current segment. Therefore, they often fail to take full advantage of many other rich contextual information, such as longer-range word dependencies beyond the segment length and semantic relationships between neighboring segments. While a naive solution to explore richer contextual information is to increase the segment length, in practice, it is usually infeasible due to limited resources, which requires O N 2 for the window N of inputs at each layer. Some long-range transformer variants (Dai et al., 2019;Rae et al., 2020;Rae and Razavi, 2020) aim to extend context via compression, which use compressed memory cells for preserving the previous segments' information. The Transformer-XL (Dai et al., 2019) builds up recurrent connections between segments, concatenating the past activations with a memory cell of size M N, which results in an attention cost of O (N (M + N )). However the memory cell still requires a considerable space L \u21e5 M \u21e5 d model in a L-layer transformer with embedding size of d model , which consumes a non-negligible cost (Rae and Razavi, 2020). Rae et al. (2020) shorten the range of attention for Transformers by compressing the past memories into fine-grained and coarser compressed memory slots, while still suffering from memory consuming as the memory size is quiet large (> 1000). In addition, some efficient versions focusing on Transformer model's self-attention mechanism have also recently been explored. These models reduce memory requirements by leveraging sparsity in the attention layers (Sukhbaatar et al., 2019), exploiting a factorized sparse representation (Child et al., 2019), replacing dot-product attention with locality-sensitive hashing to decrease complexity (Kitaev et al., 2020), or using product-key attention to increase the key space (Lample et al., 2019). Besides, Chen et al. (2019) represent sentence-level context as latent topic representations by using a convolution neural network, and utilize the context representations to improve translation. However, leveraging the contextualized topic information by capturing semantic coherence via a deep probabilistic topic model, to our knowledge, has not been directly applied to Transformer before. Furthermore, compared with pre-training, fine-tuning is relatively inexpensive (Devlin et al., 2019). Nevertheless, most of the current contextualized models are trained independently on different datasets, without making good use of the publicly released pre-trained models (Radford et al., 2019;Devlin et al., 2019;Radford et al., 2018;Brown et al., 2020;Peters et al., 2018;Yang et al., 2019), paired with unsupervised pre-training on a large amount of training data. This motivates us to explore a general intervention based on those predecessors for performance gain with little computation cost, providing longer-range dependencies through a deep topic model.\nDifferent from RNN or Transformer-based LMs, topic models (Blei et al., 2003;Teh et al., 2006;Zhou and Carin, 2015;Gan et al., 2015;Zhou et al., 2016;Zhao et al., 2018) are well suited for capturing global semantic coherency by extracting word concurrence patterns into semantically meaningful topics, which can be viewed as the contextualized word representations of the entire target corpus including all segments. Since topic models are appropriate to capture long-range dependencies, some approaches attract significant recent interest by leveraging topic models to improve RNN-based language models (Dieng et al., 2017;Ahn et al., 2016;Lau et al., 2017;Wang et al., 2018a;Guo et al., 2019). Dieng et al. (2017) and Ahn et al. (2016) integrate the syntactic dependencies of RNNs and semantic topics of latent topic models. Lau et al. (2017) introduce an attention based convolutional neural network to extract semantic topics for extending the RNN cell. Wang et al. (2018a) learn the global semantic coherence of a document via a neural topic model and use the learned latent topics to build a mixture-of-experts language model. Guo et al. (2019) extract recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Motivated by recent successes on integrating topic information into RNN-based LMs, here we focus on using topic model to provide richer contextual information for improving the Transformer. In particular, we consider using Poisson gamma belief network (PGBN) (Zhou et al., 2016;Zhang et al., 2018), a state-of-the-art probabilistic topic model which can be equivalently represented as a multi-stochastic-layer deep generalization of vanilla topic models (Blei et al., 2003;Zhou et al., 2012), to extract globally shared semantical topic representations of user-defined contexts.\nTo this end, three different types of contextual topic information are provided to introduce long-range semantic dependencies into Transformers. (i) We first introduce the contextual token embedding (TE) guided by topic model to enrich the representation of each token, which not only extracts global semantics from the corpus, but also provides localized representation of a token given either its preceding or surrounding context (which one to use is task-dependent). (ii) To utilize contextual information of a segment, we develop the contextual segment embedding (SE) to construct a set of virtual words, which is placed before the word sequence of the current segment and fed into Transformer. As such, the generation of any token in one segment depends on semantic context from the previous segments. (iii) After that, we further develop a multi-head topic attention (TA) module into the Transformer, selecting semantically related topics for generating each token, a design inspired by how a token is generated by a topic model given the topics and corresponding topic proportion. To encourage topic select-attention to focus on the topics where the predicting token is more likely to be assigned to by the topic model, during training, we add a restriction between the attention weights and the latent representation of the predicting word. Besides, a sparse penalty is employed on the topic select-attention, encouraging the network to focus on only a small subset of extracted topics. Moving beyond conventional transformers, our model can not only utilize longer-range word dependencies beyond the segment length and semantic relationships across all segments, but also generalize easily to any pre-trained Transformer-based model by jointly fine-tuning on the target corpus. It only adds minor memory and computation overhead comparing with fine-tuning the Transformer-based model alone. We demonstrate the effectiveness of our method both quantitatively and qualitatively."}
{"introduction": "Recently, direct deep learning on unstructured 3-D point clouds has gained significant interest. Many point cloud networks have been proposed. PointNet and PointNet++ utilizes max-pooling followed by multi-layer perceptron PointConv (Wu et al., 2019) realizes a real convolution operation on point clouds. DGCNN (Wang et al., 2018) builds on PointNet++ (Qi et al., 2017) by learning features from edges instead of vertices. SPLATNet (Su et al., 2018) embeds features into a high-dimensional lattice and applies convolution on the lattice. Other works such as (Xu et al., 2018;Atzmon et al., 2018;Li et al., 2018;Fey et al., 2018;Tatarchenko et al., 2018) all have their own merits. As with 2-D image classifiers, we are curious about what indeed these models have learned. Following (Fong & Vedaldi, 2017)'s definition of explanations as meta-predictors, we want to explain those models by identifying which parts of a shape contribute most to the final score, and which parts the least.\nA natural representation of this explanation is a saliency map, which associates each point in the point cloud with an importance score. To show that a saliency map is valid, following the deletion and insertion metric proposed by (Petsiuk et al., 2018), we should expect the predicted score to drop quickly when we \"cover up\" those parts with highest importance score from the network, and to rise quickly when we gradually \"reveal\" only those parts with highest importance score to the network.\nHere we put \"cover up\" and \"reveal\" in quotes because they have not been defined yet on 3-D data. It is easy to \"cover up\" some parts of a 2-D image: simply turn those pixels into grey or black, or apply the a significant Gaussian blur to those pixels. It is not easy to extend this notion to 3-D point clouds, since however we move the points, they will always be part of the point cloud, and thus contributing to the underlying shape. Current point-based deep networks may not be robust enough to generalize to new point clouds after these operations. For example, not all point cloud networks accept inputs with varying number of points, hence unable to adapt deleting points. Prior work (Zheng et al., 2018) proposed an approximation of point deletion to simply moving those \"deleted\" points to the median position of the point cloud. However, their argument for the validity of this operation is true only when the network has a max-pooling layer that directly operates on point positions. Such an assumption cannot be made for a model-agnostic algorithm. Additionally, during the process of moving the points toward the median position, extra unnatural geometric structures appear, e.g., a car might suddenly have a pointy bump on its surface pointing inward. This non-smooth data is not within the training distribution of the point cloud network, hence their performance on it are undefined and will likely suffer. In practice, we often see point cloud classifiers give significantly lower predictions on point clouds with such unnatural geometric structures, which may work for the task of generating adversarial examples in (Zheng et al., 2018), but does not bring real understanding of the features those networks use to classify the point cloud.\nOur goal is to perform this \"cover up\" process in a manner so that the resulting point cloud is still part of the training distribution. For this, we attempt to smoothly morph the 3-D shape to remove distinctive shape features. As an example, for an airplane one thought would be to smoothly eliminate the wings to some other shape. Such kind of smoothing and fairing have been wellestablished on 3-D meshes. However, we have not found a satisfactory approach that directly applies on point clouds, which usually have very sparse and irregular sampling distribution.\nIn this paper, we propose a new algorithm for smoothing point clouds. For each point in the point cloud, we fit a local plane from its neighborhood. Under some assumptions we prove that the distance from the point to its local plane can be used to approximate the local curvature. This allows us to utilize a mean-curvature-flow-based algorithm similar to (Desbrun et al., 1999) to smooth the shape. Our new algorithm does not rely on explicit edges which are not available in point clouds, and is practically capable of smoothing many different shapes to a sphere with constant mean curvature.\nWith the new smoothing tool, we adapt a recent 2-D heatmap algorithm called I-GOS (Qi et al., 2019) onto point clouds. We experiment our method on PointConv (Wu et al., 2019) and DGCNN (Wang et al., 2018), two state-of-the-art point cloud networks. Results on the ModelNet40 dataset reveals that, different from image-based networks that often classify based on a small distinctive feature, point-based networks usually rely heavily on the entire shape to classify (usually more than 50% points need to be inserted for the score to be close to the original classification score, a proportion higher compared to 2-D images). However, certain important parts can be found so that once distorted, the score will drop quickly. Also, symmetry is very important for the networks to recognize certain classes. We believe that these results improve our understanding of those networks and may help improving their training in the future."}
{"introduction": "One potential path to increased data-efficiency, generalization, and robustness of machine learning methods is to train generative models. These models can learn useful representations without human supervision by learning to create examples of the data itself. Many types of generative models have flourished in recent years, including likelihood-based generative models, which include autoregressive models (Uria et al., 2013), variational autoencoders (VAEs) (Kingma & Welling, 2014;Rezende et al., 2014), and invertible flows (Dinh et al., 2014;2016). Their objective, the negative log-likelihood, is equivalent to the KL divergence between the data distribution and the model distribution. A wide variety of models can be compared and assessed along this criteria, which corresponds to how well they fit the data in an information-theoretic sense.\nStarting with the PixelCNN (Van den Oord et al., 2016), autoregressive models have long achieved the highest log-likelihoods across many modalities, despite counterintuitive modeling assumptions. For example, although natural images are observations of latent scenes, autoregressive models learn dependencies solely between observed variables. That process can require complex function approximators that integrate long-range dependencies (Oord et al., 2016;Child et al., 2019). In contrast, VAEs and invertible flows incorporate latent variables and can thus, in principle, learn a simpler model that mirrors how images are actually generated. Despite this theoretical advantage, on the landmark ImageNet density estimation benchmark, the Gated PixelCNN still achieves higher likelihoods than all flows and VAEs, corresponding to a better fit with the data.\nIs the autoregressive modeling assumption actually a better inductive bias for images, or can VAEs, sufficiently improved, outperform autoregressive models? The answer has significant practical stakes, because large, compute-intensive autoregressive models (Strubell et al., 2019) are increasingly used for a variety of applications (Oord et al., 2016;Brown et al., 2020;Dhariwal et al., 2020;Chen et al., 2020). Unlike autoregressive models, latent variable models only need to learn dependencies between latent and observed variables; such models can not only support faster synthesis and higher-dimensional data, but may also do so using smaller, less powerful architectures.\nWe start this work with a simple but (to the best of our knowledge) unstated observation: hierarchical VAEs should be able to at least match autoregressive models, because autoregressive models are equivalent to VAEs with a powerful prior and restricted approximate posterior (which merely outputs observed variables). In the worst case, VAEs should be able to replicate the functionality of autoregressive models; in the best case, they should be able to learn better latent representations, possibly with much fewer layers, if such representations exist.\nWe formalize this observation in Section 3, showing it is only true for VAEs with more stochastic layers than previous work has explored. Then we experimentally test it on competitive natural image benchmarks. Our contributions are the following:\n\u2022 We provide theoretical justification for why greater depth (up to the data dimension D, but also as low as some value K D) could improve VAE performance (Section 3)\n\u2022 We introduce an architecture capable of scaling past 70 layers, when previous work explored at most 30 (Section 4)\n\u2022 We verify that depth, independent of model capacity, improves log-likelihood, and allows VAEs to outperform the PixelCNN on all benchmarks (Section 5.1)\n\u2022 Compared to the PixelCNN, we show the model also uses fewer parameters, generates samples thousands of times more quickly, and can be scaled to larger images. We show evidence these qualities may emerge from the model learning an efficient hierarchical representation of images (Section 5.2)\n\u2022 We release code and models at https://github.com/openai/vdvae."}
{"introduction": "In recent decades, the progress of deep learning, together with advances in GPU devices, has led to a growing popularity of deep neural network (DNN) models in both academia and industry. DNN models have been widely used in various fields, such as image classification (Simonyan & Zisserman, 2014;He et al., 2016a), speech recognition (Hinton et al., 2012;Maas et al., 2017), and machine translation (Wu et al., 2016;Vaswani et al., 2017). However, due to their deep structure, most DNN models are extremely difficult to train. The practical training of a DNN model often highly depends on empirical experience and is extremely time consuming. Therefore, a series of effective optimization methods have been developed for fast DNN training.\nAccording to a recent survey paper by Sun et al. (2019), most of the optimization methods with explicit derivatives can be roughly categorized into two groups: the first-order optimization methods and the high-order optimization methods. The widely used stochastic gradient descent (SGD) algorithm and its variants (Robbins & Monro, 1951;Jain et al., 2018) are typical examples of the first-order optimization methods. The SGD algorithm only computes the first-order derivatives (i.e., the gradient) using a randomly sampled batch. By doing so, the SGD algorithm can handle large-sized datasets with limited computational resources. Unfortunately, the practical feasibility of SGD comes at the cost of sublinear convergence speed (Johnson & Zhang, 2013a). For better convergence speed, various accelerated SGD algorithms have been developed. For instance, the popularly used momentum method (Polyak, 1964;Qian, 1999) and the Nesterov Accelerated Gradient Descent (NAG) (Nesterov, 1983;Sutskever et al., 2013) method. Both of them took the information from the previous update gradient direction into consideration. Further improvements include AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and others. For a more stable gradient estimation, the stochastic average gradient (SAG) (Roux et al., 2012) and stochastic variance reduction gradient (SVRG) (Johnson & Zhang, 2013b) methods are also developed.\nExcept for the first-order optimization methods, high-order optimization methods also exist. Popular representatives are the Newton's method and its variants (Shanno, 1970;Hu et al., 2019;Pajarinen et al., 2019). Compared to the first-order methods, high-order methods might lead to faster convergence speed since they take the information of Hessian matrix into consideration.\nFor example, the Newton's method can have a quadratic convergence speed under appropriate conditions (Avriel, 2003). However, calculating and storing the Hessian matrix and its inverse is extremely expensive in terms of both time and storage. This leads to the development of some approximation methods, such as Quasi-Newton method (Avriel, 2003) and stochastic Quasi-Newton method (Luo et al., 2014). The idea of Quasi or stochastic Quasi Newton method is to approximate the inverse Hessian matrix by a positive definite matrix. For example, DFP (Fletcher & Powell, 1963;Davidon, 1991), BFGS (Broyden, 1970;Fletcher & R, 1970;Donald & Goldfarb, 1970) and L-BFGS (Nocedal & Jorge, 1980;Liu & Nocedal, 1989) methods are popular representatives. Moreover, as a useful technique for fast convergence, various pre-conditioning techniques are also popularly used. (Huckle, 1999;Benzi, 2002;Tang et al., 2009). The basis idea of pre-conditioning is to transform a difficult or ill-conditioned linear system (e.g., A\u03b8 = b) into an easier system with better condition (Higham & Mary, 2019). As a consequence, the information contained in the feature covariance can be effectively used (Wang et al., 2019). Other interesting methods trying to extract useful information from the feature covariance also exist; see for example Denton et al. (2015), Ghiasi &Fowlkes (2016), andLai et al. (2017).\nHowever, to our best knowledge, there seems no existing models or methods that are particularly designed for high-dimensional features with a factor structure. In the meanwhile, ample amounts of empirical experience suggest that most high-dimensional features demonstrate a strong factor type of covariance structure. In other words, a significant amount of the feature variability can be explained by a latent factor with very low dimensions. As a consequence, we can decompose the original features into two parts. The first is a low dimensional factor part, which accounts for a significant portion of the total volatility. The second is the residual part with factor effects removed. This residual part has the same dimension as the original feature. Consequently, it has a much reduced variability. Moreover, the inter-feature correlation is also reduced substantially. To this end, the original learning problem concerning for the high dimensional features can be decomposed into two sub learning problems. The first one is a learning problem concerning for the latent factor. This is relatively simple since the dimension of the factor is very low. The second problem is related to the residual feature. Unfortunately, this is still a challenging problem due to the high dimensions. However, compared with the original one, it is much easier because the inter feature dependence has been substantially reduced. For a practical implementation, we propose here a novel method called factor normalization. It starts with a benchmark model (e.g., VGG or ResNet) and then slightly modifies the benchmark model into a new model structure. Compared with the benchmark model, the new model takes the latent factor and residuals as different inputs. The benchmark model is remained to process the residuals. The latent factor is then put back to the model in the last layer. This is to compensate for the information loss due to factor extraction. By doing so, the new model allows the factor-related features and residual-related features to be processed separately. Furthermore, different (i.e., adaptive) learning rates can be allowed for factor and residuals, respectively. This leads to adaptive learning and thus fast convergence speed.\nThe rest of this article is organized as follows. Section 2 develops our theoretical motivation with statistical insights. Section 3 provides the details of the proposed new model. Section 4 demonstrates the outstanding performance of the propose model via extensive empirical experiments. Section 5 concludes the article with a brief discussion for future research."}
{"introduction": "In recent years, Deep Neural Networks (DNNs) have been widely applied in many fields (Goodfellow et al., 2016). Despite the great progress, vulnerability of DNNs has also been found. For example, in classification task in computer vision, by adding well-designed, visually-imperceptible perturbations on clean images, the resulting perturbed images, a.k.a. adversarial examples, can successfully fool many well-trained DNNs (Szegedy et al., 2014;Goodfellow et al., 2015). Such a process of generating adversarial examples is called adversarial attack.\nSince its proposal, there have been many interesting adversarial attacks, which can be categorized into two types, black-box attack (Papernot et al., 2017;Liu et al., 2017;Chen et al., 2017;Su et al., 2019) and white-box attack (Goodfellow et al., 2015;Kurakin et al., 2017;Carlini & Wagner, 2017;Madry et al., 2018;Tang et al., 2019). As the name suggests, white-box attacks need complete information of the target model. While black-box attacks rely on the output of target model or transferability across models.\nFor a neural network f (x; \u03b8) with input x and parameters \u03b8, we denote the trained parameters as \u03b8 and the example to be attacked as x 0 . Adversarial attack tries to find a small \u2206x such that f (x 0 + \u2206x; \u03b8) = f (x 0 ; \u03b8). To defend the attack, i.e., to keep the both sides equal, adversarial training (Szegedy et al., 2014;Goodfellow et al., 2015;Madry et al., 2018) includes a group of adversarial perturbations in training process to keep f (x 0 + \u2206x; \u03b8) = f (x 0 ; \u03b8). Generally speaking, adversarial training is the most efficient defence strategy until now (Athalye et al., 2018;Tramer et al., 2020), but the attack needs to be known in advance. To adapt to all perturbations, researchers consider the response to perturbations \u2206x in B \u03b5 = {\u2206x| \u2206x \u2264 \u03b5}. If the maximum change is small, then a certified robustness could be guaranteed (Raghunathan et al., 2018;Cohen et al., 2019).\nMotivated by the coupling of samples and parameters, one could also impose randomness on parameters to enhance the robustness. Consider a linear layer, which includes convolution layer and fully-connected layer. Imposing perturbation into samples is equal to giving randomness to parameters: \u2200\u2206x \u2208 B \u03b5 , there exists \u2206\u03b8 that satisfies x 0 + \u2206x, \u03b8 = x 0 , \u03b8 + \u2206\u03b8 . Pioneering and representative works on this direction could be found in (He et al., 2019;Liu et al., 2018b), which aim to learn a distribution for the parameters. The advantages include network diversity and low dependence on the attacks. However, its learning is not very effective: the learned distribution tends to shrink to one optimal solution.\nIn this paper, we propose to embed multiple paths into a neural network. In a regular neural network, a block with input z k and output z k+1 is denoted as z k+1 = g(z k ; \u03b8 g ). For the mapping g, we will train multiple paths g i (z k ; \u03b8 g i ), then it could give multiple outputs and the rest layers are trained to adapt all the paths. A key issue here is that we require the parameters of all paths orthogonal to each other, which guarantees the diversity and coverage. Fig. 1(a) gives a comparison illustration of a regular network and a network embedded with L paths.\nThe proposed Orthogonal Multi-Path (OMP) block can be posed in any layer of a neural network. It is not surprising that the follow-up layers are more robust since they are capable to handle features from multiple paths. Let us consider a simple example. A VGG16 is trained on CIFAR10 and we put an OMP block in the first layer. The average feature change \u2206z k+2 \u221e at the layer after OMP block caused by perturbations \u2206x \u221e \u2264 8/255 on test images is bounded by 1.39 (which is actually a certificated robustness measure) in the vanilla VGG16 and is improved to 0.82 by the OMP block. Interestingly, OMP is also helpful for the front layers. In Fig. 1(b), we visualize the learned features after the first layer in a vanilla network and the same network with an OMP block posed on the last layer. Although the OMP block is posed on the final layer, it could correct the learned features at the first layer, resulting in much more alike features of clean and perturbed images. This phenomenon is explained by the backward correction theory proposed by Allen-Zhu & Li (2020). Actually, this result could be a strong evidence to verify that training higher-level layers improves the features of lower-level ones.\nWith forward learning and backward correction, OMP block could make the feature extractor adaptive to multiple paths and then enhance the robustness of the whole networks. For example, under white-box PGD attack with l \u221e bound 8/255, which could destroy the accuracy of many vanilla networks to nearly 10% on CIFAR10, VGG16 with the proposed OMP block could keep over 50% accuracy. The contributions of this work are summarized as follows:\n\u2022 A novel defence method is proposed, which introduces orthogonal multiple paths into a neural network to enhance the robustness.\n\u2022 Extensive empirical results against different white-box and black-box attacks indicate the superior robustness of networks with OMP block in vanilla and adversarial training.\n\u2022 A thorough empirical analysis on different positions of the OMP block is provided, illustrating the distinct properties. Ablation study also demonstrates the necessity and effectiveness of the OMP block."}
{"introduction": "The main goal of generative modeling is to learn a good approximation of the underlying data distribution from finite data samples, while facilitating an efficient way to draw samples. Popular algorithms such as variational autoencoders (VAE, Kingma & Welling (2013); Rezende et al. (2014)) and generative adversarial networks (GAN, Goodfellow et al. (2014)) are theoretically-grounded models designed to meet this goal. However, they come with some challenges. For instance, VAEs suffer from the posterior collapse problem (Chen et al., 2016;Zhao et al., 2017;Van Den Oord et al., 2017), and a mismatch between the posterior and prior distribution (Kingma et al., 2016;Tomczak & Welling, 2018;Dai & Wipf, 2019;Bauer & Mnih, 2019). GANs are known to have the mode collapse problem (Che et al., 2016;Dumoulin et al., 2016;Donahue et al., 2016) and optimization instability (Arjovsky & Bottou, 2017) due to their saddle point problem formulation.\nWasserstein autoencoder (WAE) Tolstikhin et al. (2017) proposes a general theoretical framework that can potentially avoid some of these challenges. They show that the divergence between two distributions is equivalent to the minimum reconstruction error, under the constraint that the marginal distribution of the latent space is identical to a prior distribution. The core challenge of this framework is to match the latent space distribution to a prior distribution that is easy to sample from. Tolstikhin et al. (2017) investigate GANs and maximum mean discrepancy (MMD, Gretton et al. (2012)) for this task and empirically find that the GAN-based approach yields better performance despite its instability. Existing research has tried to address this challenge (Kolouri et al., 2018;Knop et al., 2018) (see section 2 for a discussion). This paper aims to design a generative model to address the latent space distribution matching problem of WAEs. To do so, we make a simple observation that allows us to use the contrastive learning framework. Contrastive learning achieves state-of-the-art results in self-supervised representation learning tasks (He et al., 2020;Chen et al., 2020) by forcing the latent representations to be 1) augmentation invariant; 2) distinct for different data samples. It has been shown that the contrastive learning objective corresponding to the latter goal pushes the learned representations to achieve maximum entropy over the unit hyper-sphere (Wang & Isola, 2020). We observe that applying this contrastive loss term to the latent representation of an AE therefore matches it to the uniform distribution over the unit hyper-sphere. Due to the use of the contrastive learning approach, we call our algorithm Momentum Contrastive Autoencoder (MoCA). Our contributions are as follows:\n1. we address the fundamental algorithmic challenge of Wasserstein auto-encoders (WAE), viz the latent space distribution matching problem, which involves matching the marginal distribution of the latent space to a prior distribution. We achieve this by making the observation that the contrastive term in the recent contrastive learning framework implicitly achieves this precise goal. This is also our novelty.\n2. we show that our proposal of using the contrastive learning framework to optimize the WAE loss achieves faster convergence and more stable optimization compared with existing popular algorithms for WAE.\n3. we perform a thorough ablation analysis of the impact of the hyper-parameters introduced by the contrastive learning framework, on the performance and behavior of WAE."}
{"introduction": "Generative Adversarial Networks (GANs) are currently one of the most popular lines of research in machine learning. Research on GANs mainly revolves around: (a) how to achieve faster and/or more accurate convergence (e.g., by studying different loss functions (Nowozin et al., 2016;Arjovsky & Bottou, 2017;Mao et al., 2017) or regularization schemes (Odena et al., 2018;Miyato et al., 2018;Gulrajani et al., 2017)), and (b) how to design different hierarchical neural networks architectures composed of linear and non-linear operators that can effectively model high-dimensional distributions (e.g., by progressively training large networks (Karras et al., 2018) or by utilizing deep ResNet type of networks as generators (Brock et al., 2019)).\nEven though hierarchical deep networks are efficient universal approximators for the class of continuous compositional functions (Mhaskar et al., 2016), the non-linear activation functions pose difficulties in their theoretical analysis, understanding, and interpretation. For instance, as illustrated in Arora et al. (2019), element-wise non-linearities pose a challenge on proving convergence, especially in an adversarial learning setting (Ji & Liang, 2018). Consequently, several methods, e.g., Saxe et al. (2014); Hardt & Ma (2017); Laurent & Brecht (2018); Lampinen & Ganguli (2019), focus only on linear models (with respect to the weights) in order to be able to rigorously analyze the neural network dynamics, the residual design principle, local extrema and generalization error, respectively. Moreover, as stated in the recent in-depth comparison of many different GAN training schemes (Lucic et al., 2018), the improvements may mainly arise from a higher computational budget and tuning and not from fundamental architectural choices.\nIn this paper, we depart from the choice of hierarchical neural networks that involve activation functions and investigate for the first time in the literature of GANs the use of high-order polynomials as an alternative class of universal function approximators for data generator functions. This choice is motivated by the strong evidence provided by the Stone-Weierstrass theorem (Stone, 1948), which states that every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. Hence, we propose to model the vector-valued generator function Gpzq : R d \u00d1 R o by a high-order multivariate polynomial of the latent vector z, whose unknown parameters are naturally represented by high-order tensors.\nHowever, the number of parameters required to accommodate all higher-order correlations of the latent vector explodes with the desired order of the polynomial and the dimension of the latent vector. To alleviate this issue and at the same time capture interactions of parameters across different orders of approximation in a hierarchical manner, we cast polynomial parameters estimation as a coupled tensor factorization (Papalexakis et al., 2016;Sidiropoulos et al., 2017) that jointly factorizes all the polynomial parameters tensors. To this end, we introduce two specifically tailored coupled canonical polyadic (CP)-type of decompositions with shared factors. The proposed coupled decompositions of the parameters tensors result into two different hierarchical structures (i.e., architectures of neural network decoders) that do not involve any activation function, providing an intuitive way of generating samples with an increasing level of detail. This is pictorially shown in Figure 1. The result of the proposed PolyGAN using a fourth-order polynomial approximator is shown in Figure 1 (a), while Figure 1 (b) shows the corresponding generation when removing the fourth-order power from the generator.\nOur contributions are summarized as follows:\n\u2022 We model the data generator with a high-order polynomial. Core to our approach is to cast polynomial parameters estimation as a coupled tensor factorization with shared factors.\nTo this end, we develop two coupled tensor decompositions and demonstrate how those two derivations result in different neural network architectures involving only linear (e.g., convolution) units. This approach reveals links between high-order polynomials, coupled tensor decompositions and network architectures.\n\u2022 We experimentally verify that the resulting networks can learn to approximate functions with analytic expressions.\n\u2022 We show how the proposed networks can be used with linear blocks, i.e., without utilizing activation functions, to synthesize high-order intricate signals, such as images.\n\u2022 We demonstrate that by incorporating activation functions to the derived polynomial-based architectures, PolyGAN improves upon three different GAN architectures, namely DC-GAN (Radford et al., 2015), SNGAN (Miyato et al., 2018) and SAGAN (Zhang et al., 2019).\n(a) (b)\nFigure 1: Generated samples by an instance of the proposed PolyGAN. (a) Generated samples using a fourth-order polynomial and (b) the corresponding generated samples when removing the terms that correspond to the fourth-order. As evidenced, by extending the polynomial terms, PolyGAN generates samples with an increasing level of detail."}
{"introduction": "The high sample complexity of reinforcement learning (RL) algorithms has prompted a large body of prior work to study pretraining of RL agents. During the pretraining stage, the agent collects unsupervised experience from the environment that is not labeled with any rewards. Prior methods have used this pretraining stage to learn representations of the environment that might assist the learning of downstream tasks. For example, some methods learn representations of the observations (Laskin et al., 2020;Schwarzer et al., 2021) or representations of the dynamics model (Ebert et al., 2018;Ha & Schmidhuber, 2018;Sekar et al., 2020). In this work, we focus on methods that learn a set of potentially-useful policies, often known as skills (Salge et al., 2014;Mohamed & Rezende, 2015;Gregor et al., 2016;Achiam et al., 2018;Eysenbach et al., 2018). That is, the learned representation corresponds to a reparametrization of policies. The aim of this unsupervised pretraining is to learn skills that, when a reward function is given, can quickly be combined or composed to maximize this reward. Prior work has demonstrated that this general approach does accelerate learning downstream RL (Gregor et al., 2016;Eysenbach et al., 2018;Achiam et al., 2018). However, prior work offers little analysis about when and where such methods are provably effective. Even simple questions, such as what it means for a set of skills to be optimal, remain unanswered.\nAlgorithms for unsupervised skill learning are conceptually related to the representation learning methods used to improve supervised learning (Gutmann & Hyv\u00e4rinen, 2010;Belghazi et al., 2018;Wu et al., 2018;Oord et al., 2018;Hjelm et al., 2018;He et al., 2020). Both typically maximize a lower bound on mutual information, and the learned representations are often combined linearly to solve downstream tasks (Hjelm et al., 2018;Oord et al., 2018). However, whereas prior work in supervised learning has provided thorough analysis of when and where these representation learning methods produce useful features (Kraskov et al., 2004;Song & Ermon, 2019;McAllester & Stratos, 2020), there has been comparatively little analysis into when unsupervised skill learning methods produce skills that are useful for solving downstream RL tasks.\nIn this paper, we analyze when and where existing skill learning methods based on mutual information maximization are (or are not) optimal for preparing to solve unknown, downstream tasks. On the one hand, we show that the skills learned by these methods are not complete, in that they cannot be used to represent the solution to every RL problem. This result implies that using the learned skills for hierarchical RL may result in suboptimal performance, and suggests new opportunities for better skill learning algorithms. One the other hand, we show that existing methods acquire a policy initialization that is optimal for learning downstream tasks, if that adaptation is performed using an idealized adaptation procedure. To the best of our knowledge, this is the first result showing that unsupervised skill learning methods are optimal in any sense.\nOur analysis also illuminates a number of properties of these methods. For example, we show that every skill is optimal for some reward function, and we provide a nontrivial upper bound on the number of unique skills learned. This result implies that these methods cannot learn an infinite number of unique skills, and instead will learn duplicate copies of some skills. The key to our analysis is to view RL algorithms and skill learning algorithms as geometric operations (see Fig. 1). Points correspond to distributions over states, and the set of all possible distributions is a convex polytope that lies on a probability simplex. We show that all reward-maximizing policies lie at vertices of this polytope and that maximizing mutual information corresponds to solving a facility assignment problem on the simplex.\nThe main contribution of this paper is a proof that skill learning algorithms based on mutual information are optimal for minimizing regret against unknown reward functions, assuming that adaptation is performed using a certain procedure. Our proof of optimality relies on certain problem assumptions, leaving the door open for future skill learning algorithms to perform better under different problem assumptions. This contribution provides a rigorous notion of what it means for an unsupervised RL algorithm to be optimal, and also answers additional questions about unsupervised skill learning algorithms, such as whether the skills correspond to reward-maximizing policies and how many unique skills will be learned."}
{"introduction": "Image generation has been an extensively studied topic in machine learning and computer vision. Vast numbers of papers have explored generating images through low-dimensional latent representations (Goodfellow et al., 2014;Arjovsky et al., 2017;Li et al., 2017;Kingma & Welling, 2013;van den Oord et al., 2017;Oord et al., 2016). However, it is challenging to learn disentangled representations which allows us to better control the generative models (Higgins et al., 2017;Kim & Mnih, 2018;Locatello et al., 2018;Chen et al., 2016). In this paper, we will explore generating CFG programs from constructive solid geometry (CSG) images (Hubbard, 1990). We can consider these programs as an alternative to the low-dimensional representation of the image. We can view the model for extracting the programs as an encoder and the renderer that reconstructs the image as a decoder. Parsing an image of geometric shapes into programs enables us to manipulate only the desired components of the image while reconstruct the rest. In this paper, we assume access to a non-differentiable renderer. They are more common than differentiable ones, but more challenging to work with neural network because the gradient w.r.t. the input is inaccessible.\nThe standard pipeline to parse an image (e.g. CSG image) into programs (e.g. CFG programs) with non-differentiable renderer is supervised pretraining followed by REINFORCE fine-tuning (Sharma et al., 2018;Ellis et al., 2019). Sampling programs directly from a specified grammar provides data sufficient for supervision. However, Bunel et al. (2018) points out a limitation of supervised training with MLE -it maximizes the likelihood of a single reference program while penalizing many other valid programs. This observation is summarized as program aliasing. Ellis et al. (2019) incorporated REINFORCE to learn a value function in order to prune unpromising partial programs evaluated in the image space. Sharma et al. (2018) utilizes REINFORCE fine-tuning to accustom the program synthesizer to approximate CAD images not generated by grammar. While supervised methods optimize the objective in the string space, RL methods directly optimizes in the image space. Despite these benefits, a pure RL approach was not preferred because correct programs are too sparse in the program space to be sampled frequently enough to learn. We focus on improving the sample efficiency of the REINFORCE algorithm and show that a pure RL approach can achieve comparable result as a two-step model. We further demonstrate that our method generalizes better on a synthetic 2D CSG dataset than a supervised method and yields superior results under beam search on a CAD dataset than a supervised pretrained RL fine-tuned model.\nHere are the key components to successfully learn to parse an image without program supervision:\n\u2022 We use REINFORCE (Williams, 1992) as our main building component because direct gradient information from the non-differentiable renderer is inaccessible. \u2022 We incorporate a grammar-encoded tree LSTM to impose a structure on the search space such that the algorithm is essentially sampling a path in the CFG syntax tree top-down. This guarantees the validity of the output program. \u2022 We propose an entropy estimator suitable for sampling top-down from a syntax tree for entropy regularization to encourage exploration of the search space. \u2022 Instead of using simple Monte Carlo sampling, we perform sampling without replacement from the syntax tree to optimize over top-k programs simultaneously. This leads to faster convergence during training and more effective beam search during testing."}
{"introduction": "Deep neural networks have driven a shift from feature engineering to feature learning. The great progress largely comes from well-designed networks with increasing capacity of models (He et al., 2016a;Xie et al., 2017;Huang et al., 2017;Tan & Le, 2019). To achieve the superior performance, a useful practice is to add more layers (Szegedy et al., 2015) or expand the size of existing convolutions (kernel width, number of channels) (Huang et al., 2019;Tan & Le, 2019;Mahajan et al., 2018). Meantime, the computational cost significantly increases, hindering the deployment of these models in realistic scenarios. Instead of adding much more computational burden, we prefer adding sampledependent modules to networks, increasing the model capacity by accommodating the data variance.\nSeveral existing work attempt to augment the sample-dependent modules into network. For example, Squeeze-and-Excitation network (SENet) (Hu et al., 2018) learns to scale the activations in the channel dimension conditionally on the input. Conditionally Parameterized Convolution (CondConv) (Yang et al., 2019) uses over-parameterization weights and generates individual convolutional kernels for each sample. GaterNet (Chen et al., 2018) adopts a gate network to extract features and generate sparse binary masks for selecting filters in the backbone network based upon inputs. All these methods focus on the adjustment of the micro structure of neural networks, using a data-dependent module to influence the feature representation at the same level. Recall the deep neural network to mammalian brain mechanism in biology (Rauschecker, 1984), the neurons are linked by synapses and responsible for sensing different information, the synapses are activated to varying degrees when the neurons perceive external information. Such a phenomenon inspires us to design a data-dependent network structure so that different samples will activate different network paths.\nIn this paper, we learn to optimize the connectivity of neural networks based upon inputs. Instead of using stacked-style or hand-designed manners, we allow more flexible selection for forwarding paths. Specifically, we reformulate the network into a directed acyclic graph, where nodes represent the convolution block while edges indicate connections. Different from randomly wired neural networks (Xie et al., 2019) that generate random graphs as connectivity using predefined generators, we rewire the graph as a complete graph so that all nodes establish connections with each other. Such a setting allows more possible connections and makes the task of finding the most suitable connectivity for each sample equivalent to finding the optimal sub-graph in the complete graph. In the graph, each node aggregates features from the preceding nodes, performs feature transformation (e.g. convolution, normalization, and non-linear operations), and distributes the transformed features to the succeeding nodes. The output of the last node in the topological order is employed as the representation through the graph. To adjust the contribution of different nodes to the feature representation, we further assign weights to the edges in the graph. The weights are generated dynamically for each input via an extra module (denoted as router) along with each node. During the inference, only crucial connections are maintained, which creates different paths for different instances. As the connectivity for each sample is generated through non-linear functions determined by routers, our method can enable the networks to have more representation power than the static network.\nWe call our method as the Dynamic Graph Network (DG-Net). It doesn't increase the depth or width of the network, while only introduces an extra negligible cost to compute the edge weights and aggregate the features. To facilitate the training, we represent the network connection of each sample as a adjacent matrix and design a buffer mechanism to cache the matrices of a sample batch during training. With the buffer mechanism, we can conveniently aggregate the feature maps in the forward pass and compute the gradient in the backward pass by looking up the adjacent matrices. The main contributions of our work are as follows:\n\u2022 We first introduce the dynamic connectivity based upon inputs to exploit the model capacity of neural networks. Without bells and whistles, simply replacing static connectivity with dynamic one in many networks achieves solid improvement with only a slight increase of (\u223c 1%) parameters and (\u223c 2%) computational cost (see table 1). \u2022 DG-Net is easy and memory-conserving to train. The parameters of networks and routers can be optimized in a differentiable manner. We also design a buffer mechanism to conveniently access the network connectivity, in order to aggregate the feature maps in the forward pass and compute the gradient in the backward pass."}
{"introduction": "Debugging machine learning (ML) models is a critical part of the ML development life cycle. Uncovering bugs helps ML developers make important decisions about both development and deployment. In practice, much of debugging uses aggregate test statistics (like those in leader board style challenges [Rajpurkar et al. (2016)]) and continuous evaluation and monitoring post deployment [Liberty et al. (2020), Simon (2019)]. However, additional issues arise with over-reliance on test statistics. For instance, aggregate statistics like held out test accuracy are known to overestimate generalization performance [Recht et al. (2019)]. Further, statistics offer little insight nor remedy for specific model failures [Ribeiro et al. (2020); Wu et al. (2019)]. Last, reactive debugging of failures as they occur in production does little to mitigate harmful user experiences [La Fors et al. (2019)]. Several techniques exist for identifying undesirable behavior in machine learning models. These methods include explanations [Ribeiro et al. (2016); Slack et al. (2020b); Lakkaraju et al. (2019); Lundberg & Lee (2017)], fairness metrics [Feldman et al. (2015), Slack et al. (2020a)], data set replication [Recht et al. (2019); Engstrom et al. (2020)], and behavioral testing tools [Ribeiro et al. (2020)]. However, these techniques do not provide methods to remedy model bugs or require a high level of human supervision. To enable model designers to discover and correct model bugs beyond aggregate test statistics, we analyze unrestricted adversarial examples: instances on the data manifold that are misclassified [Song et al. (2018)]. We identify model bugs through diagnosing common patterns in unrestricted adversarial examples.\nIn this work, we propose Defuse: a technique for debugging classifiers through distilling1 unrestricted adversarial examples. Defuse works in three steps. First, Defuse identifies unrestricted adversarial examples by making small, semantically meaningful changes to input data using a variational autoencoder (VAE). If the classifier prediction deviates from the ground truth label on the altered instance, it returns the data instance as a potential model failure. This method employs similar techniques from [Zhao et al. (2018)]. Namely, small perturbations in the latent space of generative models can produce images that are misclassified. Second, Defuse distills the changes through clustering on the unrestricted adversarial example's latent codes. In this way, Defuse diagnoses regions in the latent space that are problematic for the classifier. This method produces a set of To illustrate the usefulness of failure scenarios, we run Defuse on a classifier trained on MNIST and provide an overview in figure 1. In the identification step (first pane in figure 1), Defuse generates unrestricted adversarial examples for the model. The red number in the upper right hand corner of the image is the classifier's prediction. Although the classifier achieves high test set performance, we find naturally occurring examples that are classified incorrectly. Next, the method performs the distillation step (second pane in figure 1). The clustering model groups together similar failures for annotator labeling. We see that similar mistakes are grouped together. For instance, Defuse groups together a similar style of incorrectly classified eights in the first row of the second pane in figure 1. Next, Defuse receives annotator labels for each of the clusters.2 Last, we run the correction step using both the annotator labeled data and the original training data. We see that the model correctly classifies the images (third pane in figure 1). Importantly, the model maintains its predictive performance, scoring 99.1% accuracy after tuning. We see that Defuse enables model designers to both discover and correct naturally occurring model failures.\nWe provide the necessary background in Defuse ( \u00a72). Next, we detail the three steps in Defuse: identification, distillation, and correction ( \u00a73). We then demonstrate the usefulness of Defuse on three image data sets: MNIST [LeCun et al. (2010)], the German traffic signs data set [Stallkamp et al. (2011)], and the Street view house numbers data set [Netzer et al. (2011)], and find that Defuse discovers and resolves critical bugs in high performance classifiers trained on these datasets ( \u00a74)."}
{"introduction": "Spectral embedding is a standard technique for the representation of graph data (Ng et al., 2002;Belkin & Niyogi, 2002). Given the adjacency matrix A \u2208 R n\u00d7n + of the graph, it is obtained by solving either the eigenvalue problem:\nor the generalized eigenvalue problem:\nwhere D = diag(A1 n ) is the degree matrix, with 1 n the all-ones vector of dimension n, L = D -A is the Laplacian matrix of the graph, \u039b \u2208 R k\u00d7k is the diagonal matrix of the k smallest (generalized) eigenvalues of L and X \u2208 R n\u00d7k is the corresponding matrix of (generalized) eigenvectors. In this paper, we only consider the generalized eigenvalue problem, whose solution is given by the spectral decomposition of the normalized Laplacian matrix L norm = I -D -1/2 AD -1/2 (Luxburg, 2007).\nThe spectral embedding can be interpreted as equilibrium states of some physical systems (Snell & Doyle, 2000;Spielman, 2007;Bonald et al., 2018), a desirable property in modern machine learning. However, it tends to produce poor results on real datasets if applied directly on the graph (Amini et al., 2013). One reason is that real graphs are most often disconnected due to noise or outliers in the dataset.\nIn order to improve the quality of the embedding, two main types of regularization have been proposed. The first artificially increases the degree of each node by a constant factor (Chaudhuri et al., 2012;Qin & Rohe, 2013), while the second adds a constant to all entries of the original adjacency matrix (Amini et al., 2013;Joseph et al., 2016;Zhang & Rohe, 2018). In the practically interesting case where the original adjacency matrix A is sparse, the regularized adjacency matrix is dense but has a so-called sparse + low rank structure, enabling the computation of the spectral embedding on very large graphs (Lara, 2019).\nWhile (Zhang & Rohe, 2018) explains the effects of regularization through graph conductance and (Joseph et al., 2016) through eigenvector perturbation on the Stochastic Block Model, there is no simple interpretation of the benefits of graph regularization. In this paper, we show on a simple block model that the complete graph regularization forces the spectral embedding to separate the blocks in decreasing order of size, making the embedding less sensitive to noise or outliers in the data.\nIndeed, (Zhang & Rohe, 2018) identified that, without regularization, the cuts corresponding to the first dimensions of the spectral embedding tend to separate small sets of nodes, so-called dangling sets, loosely connected to the rest of the graph. Our work shows more explicitly that regularization forces the spectral embedding to focus on the largest clusters. Moreover, our analysis involves some explicit characterization of the eigenvalues, allowing us to quantify the impact of the regularization parameter.\nThe rest of this paper is organized as follows. Section 2 presents block models and an important preliminary result about their aggregation. Section 3 presents the main result of the paper, about the regularization of block models, while Section 4 extends this result to bipartite graphs. Section 5 presents the experiments and Section 6 concludes the paper."}
{"introduction": "Virtual environments, such as Arcade Learning Environment (ALE) (Bellemare et al., 2013), Mu-JoCo (Todorov et al., 2012), and OpenAI Gym (Brockman et al., 2016), have significantly benefited the development and evaluation of learning algorithms on intelligent agent control and planning. However, existing virtual environments for skill learning typically involves rigid-body dynamics only. Research on establishing standard soft-body environments and benchmarks is sparse, despite the wide range of applications of soft bodies in multiple research fields, e.g., simulating virtual surgery in healthcare, modeling humanoid characters in computer graphics, developing biomimetic actuators in robotics, and analyzing fracture and tearing in material science.\nCompared to its rigid-body counterpart, soft-body dynamics is much more intricate to simulate, control, and analyze. One of the biggest challenges comes from its infinite degrees of freedom (DoFs) and the corresponding high-dimensional governing equations. The intrinsic complexity of soft-body dynamics invalidates the direct application of many successful robotics algorithms designed for rigid bodies only and inhibits the development of a simulation benchmark for evaluating novel algorithms tackling soft-body tasks.\nIn this work, we aim to address this problem by proposing PlasticineLab, a novel benchmark for running and evaluating 10 soft-body manipulation tasks with 50 configurations in total. These tasks have to be performed by complex operations, including pinching, rolling, chopping, molding, and carving. Our benchmark is highlighted by the adoption of differentiable physics in the simulation environment, providing for the first time analytical gradient information in a soft-body benchmark, making it possible to conduct supervised learning with gradient-based optimization. In terms of the soft-body model, we choose to study plasticine (Fig. 1, left), a versatile elastoplastic material for sculpturing. Plasticine deforms elastically under small deformation, and plastically under large deformation. Compared to regular elastic soft bodies, plasticine establishes more diverse and realistic behaviors and brings challenges unexplored in previous research, making it a representative medium to test soft-body manipulation algorithms (Fig. 1, right).\nWe implement PlasticineLab, its gradient support, and its elastoplastic material model using Taichi (Hu et al., 2019a), whose CUDA backend leverages massive parallelism on GPUs to simulate a diverse collection of 3D soft-bodies in real time. We model the elastoplastic material using the Moving Least Squares Material Point Method (Hu et al., 2018) and the von Mises yield criterion. We use Taichi's two-scale reverse-mode differentiation system (Hu et al., 2020) to automatically compute gradients, including the numerically challenging SVD gradients brought by the plastic material model. With full gradients at hand, we evaluated gradient-based planning algorithms on all soft-robot manipulation tasks in PlasticineLab and compared its efficiency to RL-based methods. Our experiments revealed that gradient-based planning algorithms could find a more precious solution within tens of iterations with the extra knowledge of the physical model. At the same time, RL methods may fail even after 10K episodes. However, gradient-based methods lack enough momentum to resolve long-term planning, especially on multi-stage tasks. These findings have deepened our understanding of RL and gradient-based planning algorithms. Additionally, it suggests a promising direction of combining both families of methods' benefits to advance complex planning tasks involving soft-body dynamics. In summary, we contribute in this work the following:\n\u2022 We introduce, to the best of our knowledge, the first skill learning benchmark involving elastic and plastic soft bodies.\n\u2022 We develop a fully-featured differentiable physical engine, which supports elastic and plastic deformation, soft-rigid material interaction, and a tailored contact model for differentiability.\n\u2022 The broad task coverage in the benchmark enables a systematic evaluation and analysis of representative RL and gradient-based planning algorithms. We hope such a benchmark can inspire future research to combine differentiable physics with imitation learning and RL. TDW (Gan et al., 2020). We observe a tendency to use full-physics simulators with realistic dynamics. However, most of these virtual environments are based on rigid-body physical engines, such as MuJoCo (Todorov et al., 2012) and PyBullet (Coumans & Bai, 2016). While some support soft-body dynamics in theory (e.g., TDW and SAPIEN is based on NVIDIA PhysX (PhysX) that supports particle simulation), none has provided the assets and tasks for soft-body manipulation. Differentiable information is also missing in these engines. We fill in this gap with our PlasticineLab benchmark.\nDifferentiable physics engines Differentiable physics engines for machine learning have gained increasing popularity. One family of approaches approximates physical simulators using neural networks, which are naturally differentiable (Battaglia et al., 2016;Chang et al., 2016;Mrowca et al., 2018;Li et al., 2018). A more direct and accurate approach is to implement physics-based simulators using differentiable programming systems, e.g., standard deep learning frameworks equipped with automatic differentiation tools (Degrave et al., 2016;de Avila Belbute-Peres et al., 2018;Schenck & Fox, 2018;Heiden et al., 2019). These systems are typically redistricted to explicit time integration. Other approaches of evaluating simulation gradient computation include using the adjoint methods to differentiate implicit time integrators (Bern et al., 2019;Geilinger et al., 2020), LCP (de Avila Belbute-Peres et al., 2018) and leveraging QR decompositions (Liang et al., 2019;Qiao et al., 2020). Closely related to our work is ChainQueen (Hu et al., 2019b), a differentiable simulator for elastic bodies, and DiffTaichi (Hu et al., 2020), a system to automatically generate high-performance simulation gradient kernels. Our simulator is originated from ChainQueen but with significant modifications in order to add our novel support for plasticity and contact gradients.\nTrajectory optimization Our usage of differentiable simulation in planning soft-body manipulation is closely related to trajectory optimization, a topic that has been extensively studied in robotics for years and has been applied to terrestrial robots (Posa et al., 2014;Erez & Todorov, 2012;de Avila Belbute-Peres et al., 2018), aerial robots (Foehn et al., 2017;Tang & Kumar, 2015;Sreenath et al., 2013), and, closest to examples in our work, robotic manipulators (Marchese et al., 2016;Li et al., 2015). Both trajectory optimization and differentiable physics formulate planning as an optimization problem and derive gradients from governing equations of the dynamics (Tedrake, 2020). Still, the problem of motion planning for soft-body manipulation is under exploration in both communities because of two challenges: first, the high DoFs in soft-body dynamics make traditional trajectory optimization methods computationally prohibitive. Second, and more importantly, contacts between soft bodies are intricate to formulate in a concise manner. Our differentiable physics simulator addresses both issues with the recent development of DiffTaichi (Hu et al., 2020), unlocking gradientbased optimization techniques on planning for soft-body manipulation with high DoFs (> 10, 000) and complex contact.\nLearning-based soft body manipulation Finally, our work is also relevant to prior methods that propose learning-based techniques for manipulating physics systems with high degrees of freedom, e.g. cloth (Liang et al., 2019;Wu et al., 2020), fluids (Ma et al., 2018;Holl et al., 2020), and rope (Yan et al., 2020;Wu et al., 2020). Compared to our work, all of these prior papers focused on providing solutions to specific robot instances, while the goal of our work is to propose a comprehensive benchmark for evaluating and developing novel algorithms in soft-body research. There are also considerable works on soft manipulators (George Thuruthel et al., 2018;Della Santina et al., 2018). Different from them, we study soft body manipulation with rigid manipulators."}
{"introduction": "Optimal transport (OT) is a classical problem in mathematics and operation research. Due to its appealing theoretical properties and flexibility in practical applications, it has recently become an important tool in the machine learning and statistics community; see for example, (Courty et al., 2017;Arjovsky et al., 2017;Tolstikhin et al., 2018;Gulrajani et al., 2017) and references therein. The main usage of OT is to provide a distance named Wasserstein distance, to measure the discrepancy between two probability distributions. However, that distance suffers from expensive computational complexity, which is the main obstacle to using OT in practical applications.\nThere have been two main approaches to overcome the high computational complexity problem: either approximate the value of OT or apply the OT adaptively to specific situations. The first approach was initiated by (Cuturi, 2013) using an entropic regularizer to speed up the computation of the OT (Sinkhorn, 1967;Knight, 2008). The entropic regularization approach has demonstrated its usefulness in several application domains (Courty et al., 2014;Genevay et al., 2018;Bunne et al., 2019). Along this direction, several works proposed efficient algorithms for solving the entropic OT (Altschuler et al., 2017;Lin et al., 2019b;a) as well as methods to stabilize these algorithms (Chizat et al., 2018;Peyr\u00e9 & Cuturi, 2019;Chizat et al., 2018;Schmitzer, 2019). However, these algorithms have complexities of the order O(k 2 ), where k is the number of supports. It is expensive when we need to compute the OT repeatedly, especially in learning the data distribution.\nThe second approach, known as \"slicing\", takes a rather different perspective. It leverages two key ideas: the OT closed-form expression for two distributions in one-dimensional space, and the transformation of a distribution into a set of projected one-dimensional distributions by the Radon transform (RT) (Helgason, 2010). The popular proposal along this direction is Sliced-Wasserstein (SW) distance (Bonneel et al., 2015), which samples the projecting directions uniformly over a unit sphere in the data ambient space and takes the expectation of the resulting one-dimensional OT distance. The SW distance hence requires a significantly lower computation cost than the original Wasserstein distance and is more scalable than the first approach. Due to its solid statistical guarantees and efficient computation, the SW distance has been successfully applied to a variety of practical tasks (Deshpande et al., 2018;Liutkus et al., 2019;Kolouri et al., 2018;Wu et al., 2019;Deshpande et al., 2019) where it has been shown to have comparative performances to other distances and divergences between probability distributions. However, there is an inevitable bottleneck of computing the SW distance. Specifically, the expectation with respect to the uniform distribution of projections in SW is intractable to compute; therefore, the Monte Carlo method is employed to approximate it. Nevertheless, drawing from a uniform distribution of directions in high-dimension can result in an overwhelming number of irrelevant directions, especially when the actual data lies in a low-dimensional manifold. Hence, SW typically needs to have a large number of samples to yield an accurate estimation of the discrepancy. Alternatively, in the other extreme, Max Sliced-Wasserstein (Max-SW) distance (Deshpande et al., 2019) uses only one important direction to distinguish the probability distributions. However, other potentially relevant directions are ignored in Max-SW. Therefore, Max-SW can miss some important differences between the two distributions in high dimension. We note that the linear projections in the Radon transform can be replaced by non-linear projections resulting in the generalized sliced-Wasserstein distance and its variants (Beylkin, 1984;Kolouri et al., 2019).\nApart from these main directions, there are also few proposals that try either to modify them or to combine the advantages of the above-mentioned approaches. In particular, Paty & Cuturi (2019) extended the idea of the max-sliced distance to the max-subspace distance by considering finding an optimal orthogonal subspace. However, this approach is computationally expensive, since it could not exploit the closed-form of the one-dimensional Wasserstein distance. Another approach named the Projected Wasserstein distance (PWD), which was proposed in (Rowland et al., 2019), uses sliced decomposition to find multiple one-dimension optimal transport maps. Then, it computes the average cost of those maps equally in the original dimension.\nOur contributions. Our paper also follows the slicing approach. However, we address key friction in this general line of work: how to obtain a relatively small number of slices simultaneously to maintain the computational efficiency, but at the same time, cover the major differences between two high-dimensional distributions. We take a probabilistic view of slicing by using a probability measure on the unit sphere to represent how important each direction is. From this viewpoint, SW uses the uniform distribution while Max-SW searches for the best delta-Dirac distribution over the projections, both can be considered as special cases. In this paper, we propose to search for an optimal distribution of important directions. We regularize this distribution such that it prefers directions that are far away from one another, hence encouraging an efficient exploration of the space of directions. In the case of no regularization, our proposed method recovers max-(generalized) SW as a special case. In summary, our main contributions are two-fold:\n1. First, we introduce a novel distance, named Distributional Sliced-Wasserstein distance (DSW), to account for the issues of previous sliced distances. Our main idea is to search for not just a single most important projection, but an optimal distribution over projections that could balance between an expansion of the area around important projections and the informativeness of projections themselves, i.e., how well they can distinguish the two target probability measures. We show that DSW is a proper metric in the probability space and possesses appealing statistical and computational properties as the previous sliced distances.\n2. Second, we apply the DSW distance to generative modeling tasks based on the generative adversarial framework. The extensive experiments on real and large-scale datasets show that DSW distance significantly outperforms the SW and Max-SW distances under similar computational time on these tasks. Furthermore, the DSW distance helps model distribution converge to the data distribution faster and provides more realistic generated images than the SW and Max-SW distances.\nOrganization. The remainder of the paper is organized as follows. In Section 2, we provide backgrounds for Wasserstein distance and its slice-based versions. In Section 3, we propose distributional (generalized) sliced-Wasserstein distance and analyze some of its theoretical properties. Section 4 includes extensive experiment results followed by discussions in Section 5. Finally, we defer the proofs of key results and extra materials in the Appendices. Notation. For any \u03b8, \u03b8 \u2208 R d , cos(\u03b8, \u03b8 ) = \u03b8 \u03b8 \u03b8 \u03b8 , where . is 2 norm. For any d \u2265 2, S d-1 denotes the unit sphere in d dimension in 2 norm . Furthermore, \u03b4 denotes the Dirac delta function, and \u2022, \u2022 is the Euclidean inner-product. For any p \u2265 1, L p (R d ) is the set of real-valued functions on R d with finite p-th moment."}
{"introduction": "Training deep neural networks (DNNs) requires large amounts of computational resources, often using many devices operating over days and weeks. Furthermore, with ever-increasing model size and available data, the amount of compute used was noted to increase exponentially over the past few years (Amodei & Hernandez, 2018). Fortunately, the operations made in DNN training are highly suitable for parallelization over many devices. Most commonly, the parallelization is done by \"data-parallelism\" in which the data is divided into separate batches which are distributed between different devices during training. By using many devices, with each device highly utilized, one could train on a large number of samples without paying in run-time. However, this training is done synchronously, with synchronization between the different devices done on every iteration (S-SGD). With the growth in the number of devices participating in the training, the synchronization becomes a prominent bottleneck.\nTo overcome the synchronization bottleneck, asynchronous training (A-SGD) has been proposed (Dean et al., 2012;Recht et al., 2011). The basic idea behind such training is that when a device finishes calculating its batch gradients, an update to the DNN parameters is immediately performed, without waiting for other devices. This approach is known as asynchronous centralized training with a parameter server (PS) that updates the parameters (Li, 2014). The main problem in such training is that, since the PS updates the parameters whenever a device communicates with him, the parameters that are being used in other devices calculation are no longer up-to-date. This phenomena is called gradient staleness or delay as we will refer to it, and it causes a deterioration in generalization performance when using A-SGD.\nThe generalization deterioration can be seen in Fig. 1. Unless the learning rate is significantly decreased, we can see a generalization gap: a deterioration in the generalization of A-SGD (with delay \u03c4 = 32) from the baseline (S-SGD, \u03c4 = 0) value near the steady state -i.e., after we are near full convergence, following many training epochs (2000). Due to the generalization gap demonstrated in the figure, A-SGD with a parameter server and large delays is not commonly used -despite it is relatively simple to implement and despite its vast potential to accelerate training. Therefore, our main goal in this work is to shed some theoretical light on the generalization gap problem and use it to improve A-SGD training. Previous theoretical analysis of A-SGD (Liu et al., 2018;Lian et al., 2016;2015;Dai et al., 2018;Dutta et al., 2018;Arjevani et al., 2018) focused on analyzing the convergence rate, i.e., the time it takes to reach steady state, and not on the properties of the obtained solution. In contrast, in our paper we focus on understanding how the delay affects the selection of the solution we converge to, and changes in this selection can impact generalization.\nWe tackle these questions from the perspective of dynamical stability. The dynamical stability approach was used in (Nar & Sastry, 2018;Wu et al., 2018), to study which minima are accessible under a specific choice of optimization algorithm and hyperparameters. In (Nar & Sastry, 2018), the authors analyzed gradient descent (GD) algorithm as a discrete-time nonlinear dynamical system. By analyzing the Lyapunov stability of this system for different minima, they showed a relation between the learning rate choice and the accessible minima set, i.e., the subset of the local optima that GD can converge to. In (Wu et al., 2018), the authors focused on stochastic gradient descent (SGD), defined a criterion to evaluate the stability of a specific minimum and used this criterion to show how the learning rate and batch-size play a role in SGD minima selection process.\nHere, we use dynamical stability to analyze the dynamics of A-SGD. Using this approach, the main question we try to tackle is:\nHow do learning rate, delay and momentum interact and affect the minima selection process?"}
{"introduction": "Neural Architecture Search (NAS) has become a central topic in recent years with great progress (Liu et al., 2018b;Luo et al., 2018;Wu et al., 2019;Howard et al., 2019;Ning et al., 2020;Wei et al., 2020;Luo et al., 2018;Wen et al., 2019;Chau et al., 2020;Luo et al., 2020). Methodologically, all existing NAS methods try to find the best network architecture by exploring the architecture-toperformance manifold, such as reinforced-learning-based (Zoph & Le, 2016), evolution-based (Real et al., 2019) or gradient-based Liu et al. (2018b) approaches. In order to cover the whole space, they often train and evaluate a large amount of architectures, thus causing tremendous computation cost.\nRecently, predictor-based NAS methods alleviate this problem with two key steps: one sampling step to sample some architecture-performance pairs, and another performance modeling step to fit the performance distribution by training a proxy accuracy predictor. An in-depth analysis of existing methods (Luo et al., 2018) founds that most of those methods (Ning et al., 2020;Wei et al., 2020;Luo et al., 2018;Wen et al., 2019;Chau et al., 2020;Luo et al., 2020) attempt to model the performance distribution over the whole architecture space. However, since the architecture space is often exponentially large and highly non-convex, modeling the whole space is very challenging especially given limited samples. Meanwhile, different types of predictors in these methods have to demand handcraft design of the architecture representations to improve the performance.\nIn this paper, we envision that the ambitious goal of modeling the whole space may not be necessary if the final goal is to find the best architecture. Intuitively, we assume the whole space could be divided into different sub-spaces, some of which are relatively good while some are relatively bad. We tend to choose the good ones while neglecting the bad ones, which makes sure more samples will be used to model the good subspace precisely and then find the best architecture. From another perspective, instead of optimizing the predictor by sampling the whole space as well as existing methods, we propose to jointly optimize the sampling strategy and the predictor learning, which helps achieve better sample efficiency and prediction accuracy simultaneously. Based on the above motivation, we present a novel framework that estimates a series of weak predictors progressively. Rather than expecting a strong predictor to model the whole space, we instead seek a progressive evolving of weak predictors that can connect a path to the best architecture. In this way, it greatly simplifies the learning task of each predictor. To ensure moving the best architecture along the path, we increase the sampling probability of better architectures guided by the weak predictor at each iteration. Then, the consecutive weak predictor with better samples will be trained in the next iteration. We iterate until we arrive at an embedding subspace where the best architectures reside. The weak predictor achieved at the final iteration becomes the dedicated predictor focusing on such a fine subspace and the best performed architecture can be easily predicted.\nCompared to existing predictor-based NAS, our method has several merits. First, since only weak predictors are required to locate the good subspace, it yields better sample efficiency. On NAS-Benchmark-101 and NAS-Benchmark-201, it costs significantly fewer samples to find the top-performance architecture than existing predictorbased NAS methods. Second, it is much less sensitive to the architecture representation (e.g., different architecture embeddings) and the predictor formulation design (e.g., MLP, Gradient Boosting Regression Tree, Random Forest). Experiments show our superior robustness in all their combinations. Third, it is generalized to other search spaces. Given a limited sample budget, it achieves the state-of-the-art ImageNet performance on the NASNet search space."}
{"introduction": "Compiler implementation is a complex and expensive activity (Cooper & Torczon, 2012). For this reason, there has been significant interest in using machine learning to automate various compiler tasks (Allamanis et al., 2018). Most works have restricted their attention to selecting compiler heuristics or making optimization decisions (Ashouri et al., 2018;Wang & O'Boyle, 2018). Whether learned or engineered by human experts, these decisions naturally require reasoning about the program and its behavior. Human experts most often rely upon data flow analyses (Kildall, 1973;Kam & Ullman, 1976). These are algorithms on abstract interpretations of the program, propagating information of interest through the program's control-flow graph until a fixed point is reached (Kam & Ullman, 1977). Two examples out of many data flow analyses are: liveness -determining when resources become dead (unused) and may be reclaimed; and available expressions -discovering which expressions have been computed on all paths to points in the program. Prior machine learning works, on the other hand, have typically represented the entirety of the program's behavior as a fixed-length, statically computed feature vector (Ashouri et al., 2018). Typical feature values might be the number of instructions in a loop or the dependency depth. The weakness of these techniques is shown by the fact that they are trivially confused by the addition of dead code, which changes their feature vectors without changing the program's behavior or its response to optimizations. Such learning algorithms are unable to learn their own abstract interpretations of the program and so cannot avoid these pitfalls or more subtle versions thereof (Barchi et al., 2019).\nRecently, there have been attempts to develop representations that allow finer-grain program reasoning. Many, however, are limited both by how inputs are represented as well as how inputs are processed. Representations based on source code and its direct artifacts (e.g., AST) (Alon et al., 2018a;Yin et al., 2018;Haj-Ali et al., 2020) put unnecessary emphasis on naming and stylistic choices that may not correlate with the functionality of the code (e.g., Fig. 2a). Approaches based on intermediate representations (IR) (Ben-Nun et al., 2018;Mirhoseini et al., 2017;Brauckmann et al., 2020) remove such noise but fail to capture information about the program that is important for analysis (e.g., Fig. 2b variables,Fig. 2c commutativity). In both cases, models are expected to reason about the flow of information in programs using representations that do not directly encode this information. Clearly, a program representation is needed that enables machine learning algorithms to reason about the execution of a program by developing its own data flow analyses.  Since current approaches are ill-suited to program-wide data flow analysis, we propose overcoming their limitations by making the program's control, data, and call dependencies a central part of the program's representation and a primary consideration when processing it. We achieve this by seeing the program as a graph in which individual statements are connected to other statements through relational dependencies. Each statement in the program is understood only in the context of the statements interacting with it. Through relational reasoning (Battaglia et al., 2018), a latent representation of each statement is learned that is a function of not just the statement itself, but also of the (latent) representations of its graph neighborhood. Notably, this formulation has a striking similarity to the IRs used by compilers, and the iterative propagation of information resembles the transfer functions and meet operators in traditional data flow analyses (Kildall, 1973).\nRecently proposed techniques for learning over graphs have shown promise in a number of domains (Schlichtkrull et al., 2018;Ziwei et al., 2020). With a suitable representation and graph-based model, we extend these approaches to the domain of compiler analysis, enabling downstream tasks built on top of such graph models to natively incorporate reasoning about data flow into their decision making. This improves performance on downstream tasks without requiring additional features.\nWe make the following contributions:\n\u2022 We propose a portable, language-independent graph representation of programs derived from compiler IRs. PROGRAML is the first representation to capture whole-program control-, data-, and call relations between instructions and operands as well as their order and data types. PROGRAML is a compiler-agnostic design for use at all points in the optimization pipeline; we provide implementations for LLVM and XLA IRs.\n\u2022 We introduce a benchmark dataset that poses a suite of established compiler analysis tasks as supervised machine learning problems. DEEPDATAFLOW comprises five tasks that require, in combination, the ability to model: control-and data-flow, function boundaries, instruction types, and the type and order of operands over complex programs. DEEP-DATAFLOW is constructed from 461k real-world program IRs covering a diverse range of domains and source languages, totaling 8.5 billion data flow analysis classification labels.\n\u2022 We adapt Gated-Graph Neural Networks (GGNN) to the PROGRAML representation. We show that, within a bounded problem size, our approach achieves \u2265 0.939 F 1 score on all analysis tasks, a significant improvement over state-of-the-art representations. In evaluating the limits of this approach we propose directions to better learn over programs."}
{"introduction": "Since their introduction by Goodfellow et al. (2014), generative adversarial networks (GANs) have seen remarkable progress, with current models capable of generating samples of very high quality (Brock et al., 2018;Karras et al., 2019a;2018;2019b). In recent years, particular effort has been invested in constructing controllable models, which allow manipulating attributes of the generated images. These range from disentangled models for controlling e.g., the hair color or gender of facial images (Karras et al., 2019a;b;Choi et al., 2018), to models that even allow specifying object relations (Ashual & Wolf, 2019). Most recently, it has been demonstrated that GANs trained without explicitly enforcing disentanglement, can also be easily \"steered\" (Jahanian et al., 2020;Plumerault et al., 2020). These methods can determine semantically meaningful linear directions in the latent space of a pre-trained GAN, which correspond to various different image transformations, such as zoom, horizontal/vertical shift, in-plane rotation, brightness, redness, blueness, etc. Interestingly, a walk in the revealed directions typically has a similar effect across all object categories that the GAN can generate, from animals to man-made objects.\nTo detect such latent-space directions, the methods of Jahanian et al. (2020) and Plumerault et al. (2020) require a training procedure that limits them to transformations for which synthetic images can be produced for supervision (e.g., shift or zoom). Other works have recently presented unsupervised techniques for exposing meaningful directions (Voynov & Babenko, 2020;H\u00e4rk\u00f6nen et al., 2020;Peebles et al., 2020). These methods can go beyond simple user-specified transformations, but also require optimization or training of some sort (e.g., drawing random samples in latent space).\nIn this paper, we show that for most popular generator architectures, it is possible to determine meaningful latent space trajectories directly from the generator's weights without performing any kind of training or optimization. As illustrated in Fig. 1, our approach supports both simple userdefined geometric transformations, such as shift and zoom, and unsupervised exploration of directions that typically reveals more complex controls, like the 3D pose of the camera or the blur of the Figure 1: Steerability without optimization. We determine meaningful trajectories in the latent space of a pre-trained GAN without using optimization. We accommodate both user-prescribed geometric transformations, and automatic detection of semantic directions. We also achieve attribute transfer without any training. All images were generated with BigGAN (Brock et al., 2018).\nbackground. We also discuss how to achieve attribute transfer between images, even across object categories (see Fig. 1), again without any training. We illustrate results mainly on BigGAN, which is class-conditional, but our trajectories are class-agnostic. Our approach is advantageous over existing methods in several respects. First, it is 10 4 \u00d7-10 5 \u00d7 faster. Second, it seems to detect more semantic directions than other methods. And third, it allows explicitly accounting for dataset biases."}
