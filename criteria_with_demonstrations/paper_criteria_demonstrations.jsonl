{"aspect_id": 1, "aspect_guideline": "## Introduce your topic\nThe first job of the introduction is to tell the reader what your topic is and why it\u2019s interesting or important. This is generally accomplished with a strong opening hook.\n\nThe hook is a striking opening sentence that clearly conveys the relevance of your topic. Think of an interesting fact or statistic, a strong statement, a question, or a brief anecdote that will get the reader wondering about your topic.\n\nDon\u2019t feel that your hook necessarily has to be deeply impressive or creative. Clarity and relevance are still more important than catchiness. The key thing is to guide the reader into your topic and situate your ideas.", "criteria": "1. **Introduce Your Topic**: Does the introduction effectively identify the subject matter and provide sufficient background to inform the reader about the topic being addressed?\n\n2. **Opening Hook**: Is there a compelling opening sentence or hook that grabs the reader's attention and encourages them to read further?\n\n3. **Relevance and Importance**: Does the introduction adequately convey why the topic is important or interesting, providing enough context for the reader to understand its significance?", "demonstrations": [{"input": "With many papers in the realm of image-to-image style transfer and their techniques (Zhou et al., 2021; Deng et al., 2022; Efros & Freeman, 2001; Bruckner & Groller, 2007; Gatys et al., 2016; Kolkin et al., 2019; Kalischek et al., 2021), this work too finds its place among them. We opt to emphasize diffusion-based models, the focus of which appears quite limited in preceding works. A fuller picture of this endeavor is covered as we navigate through the article.\nVarious methods have come and gone, some involving texture synthesis (Efros & Freeman, 2001; Bruckner & Groller, 2007), while others involve generative models powered by convolutional neural networks (Gatys et al., 2016; Wang et al., 2020). However, one cannot glean from previous publications any example of a step-by-step approach to image rendering. The fundamental motivation behind this research is, therefore, to build upon diffusion-based models (Sohl-Dickstein et al., 2015; Ho et al., 2020) to make this visualization possible.\nThis work situates itself within a plethora of models and algorithms, the most recent being diffusion models, among which Denoising Diffusion Implicit Models (DDIMs) (Song et al., 2020) and Classifier-Free Guidance Diffusion (CFGD) (Ho & Salimans, 2022) are the most prominent. But the salient point is, none of them affords us the precision and efficiency in image-to-image style transfer that we aim to achieve here. The operative element, as far as this research is concerned, is the D3PSR method, which stands for Dual-Domain Diffusion based Progressive Style Rendering. This method enriches the style transfer process by introducing a way to control stylistic changes via a convolutional neural network, thus maintaining the semantic integrity of the original image.\nTo sum up, this article\u2019s contributions are primarily:\n- We introduce the D3PSR technique to enrich the realm of I2I style transfer tasks.\n- We realize a two-domain I2I transfer using a diffusion model for the first time.\n- We demonstrate the D3PSR's efficacy vis-\u00e0-vis existing methods, albeit without a significant comparison of its real-world applications or implications.", "output": "1. **Introduce Your Topic**:\n    - Your introduction does mention the area of research\u2014image-to-image style transfer\u2014in the opening phrase \"With many papers in the realm of image-to-image style transfer.\" However, it quickly gets mired in a list of technical citations (\"Zhou et al., 2021; Deng et al., 2022; Efros & Freeman, 2001; Bruckner & Groller, 2007; Gatys et al., 2016; Kolkin et al., 2019; Kalischek et al., 2021\") without adequately simplifying or summarizing what these works collectively signify. The general reader may find this intimidating. Moreover, the specific contribution of your paper\u2014emphasizing diffusion-based models\u2014is only clarified in the second sentence, \"We opt to emphasize diffusion-based models.\" Even then, it's vague about why this focus is novel or necessary, causing your paper's unique angle to become lost.\n\n2. **Opening Hook**:\n    - The initial sentence, \"With many papers in the realm of image-to-image style transfer and their techniques,\" is more of an overview than an attention-grabbing hook. It doesn't offer an engaging entry point for the reader and is instead populated with multiple citations, making it more academic than intriguing. To better engage your audience from the outset, consider introducing an eye-catching statistic, posing a provocative question, or offering a surprising fact related to the broader importance or challenges in image-to-image style transfer.\n\n3. **Relevance and Importance**:\n    - The importance of your work is mentioned but not emphasized. Phrases like \"none of them affords us the precision and efficiency in image-to-image style transfer that we aim to achieve here\" suggest significance but are not explicit about why such precision and efficiency are crucial. Your attempt to highlight the paper's contributions is presented in the line \"The operative element, as far as this research is concerned, is the D3PSR method,\" yet it doesn't explain why D3PSR is an advancement over existing methods like DDIMs or CFGD. A couple of sentences directly addressing why the step-by-step approach is novel (\"However, one cannot glean from previous publications any example of a step-by-step approach to image rendering\") and why your approach better addresses the current gaps would clarify the importance of your work."}, {"input": "Sequential data modeling is an important topic in machine learning, and the recurrent networks such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014) have served as the benchmarks in this area over a long period of time. The success mainly contributes to the variety of recurrent dynamics introduced by these models, referred to as the recurrent inductive bias. More specifically, the dependence between any two inputs can be described by a parametric form, which heavily depends on their relative temporal locations. However, the recurrent models are well known to suffer from two drawbacks. The first one is the gradient vanishing problem (Hochreiter et al., 2001), i.e. the recurrent models have difficulty in depicting the possibly high correlation between distant inputs. This problem cannot be solved fundamentally by the recurrent models themselves, although it can be alleviated to some extent, say by introducing long memory patterns (Zhao et al., 2020). Secondly, the sequential nature renders these models difficult to be trained in parallel (Vaswani et al., 2017). In practice, many techniques were proposed to improve the computational efficienncy of recurrent models, while they all come with compromises (Luo et al., 2020; Lei et al., 2017).\nIn recent years, Transformers (Vaswani et al., 2017) have been revolutionizing the field of natural language processing by achieving the state-of-the-art performance on a wide range of tasks, such as language modeling (Kenton & Toutanova, 2019), machine translation (Dai et al., 2019) and text summarization (Liu & Lapata, 2019), etc. They have also demonstrated great potential in other types of sequence learning problems, for instance, time series forecasting (Zhou et al., 2021; Li et al., 2019). The success of Transformers is due to the fact that the similarity between any two tokens is well taken into account (Vaswani et al., 2017) , and hence they can model long range dependence effortlessly. Moreover, contrary to the recurrent models, the self-attention mechanism in Transformers is feed-forward in nature, and thus can be computed in parallel on the GPU infrastructure (Vaswani et al., 2017). However, the flexibility also leads to sample inefficiency in training a Transformer, i.e. much more samples will be needed to guarantee good generalization ability (d\u2019Ascoli et al., 2021). Moreover, the chronological orders are usually ignored by Transformers since they are time-invariant, and some additional efforts, in the form of positional encoding, will be required to further aggregate the temporal information (Shaw et al., 2018; Vaswani et al., 2017; Dai et al., 2019).\nIn short, both recurrent and Transformer models have the pros and cons in modeling sequential data. On one hand, due to inductive bias, the recurrent models excel at capturing the recurrent patterns even with relatively small sample sizes; see Figure 1(a). Meanwhile, sample size is the performance bottleneck for the Transformer models and, when there are sufficient samples, they are supposed to be able to depict any recurrent or non-recurrent patterns in the data; see Figure 1(b). On the other hand, sequential data have recurrent patterns more or less, and Transformers may have an improved performance if the recurrent model can be involved to handle these patterns, especially when the sample size is relatively small. Specifically, if the recurrent and non-recurrent components are separable, then one can apply a parsimonious recurrent model on the recurrent component and a Transformer on the non-recurrent one. As a result, the sample efficiency can be improved comparing to the Transformer-only baseline; see illustration in Figure 1(c).\nThere have been various attempts in the literature to combine the two models. Some earlier works were to simply stack them together in a straightforward manner. Chen et al. (2018) mixed and matched a Transformer\u2019s encoder with an recurrent-based decoder. Hao et al. (2019) introduced an additional recurrent encoder to a Transformer, while Wang et al. (2019) stacked a recurrent layer prior to the multihead self-attention. These proposals inherit both the aforementioned shortcomings of Transformer and recurrent models. In particular, for a very long input sequence, the sequential operation in the recurrent layers become extremely expensive.\nRecent efforts have been spent on integrating recurrence and self-attention systematically. Feedback Transformer (Fan et al., 2021) introduces the memory vectors to aggregate information across layers, and uses them to update the next token in a recursive manner. However, the computationally expensive sequential operation limits its attractiveness. Another line of research applies the recurrent operation only to aggregate the temporal information at a coarser scale, while the token-by-token dependence is learned by self-attention instead. Transformer-XL (Dai et al., 2019) partitions the long inputs into segments and introduces a segment-level recurrence. Meanwhile, Temporal Latent Bottleneck (TLB) (Didolkar et al., 2022) and Block-Recurrent Transformer (BRT) (Hutchins et al., 2022) further divide the segments into smaller chunks, and each chunk is summarized into a few state vectors. A recurrent relation is then formed on the sequence of state vectors. These hierarchical designs are useful to reduce the computational burden, while they overlook recurrent dynamics at a finer scale.\nIn an attempt to simplify the numerical calculation of RNNs, we found surprisingly that an RNN layer with linear activation can be broken down into a series of simple RNNs with scalar hidden coefficients. Each simple RNN induces a distinct recurrent pattern, and their combination forms the recurrent dynamics of the RNN layer. Hence the calculation time can be greatly reduced by training these simple RNNs in parallel. On top of that, it can be equivalently rewritten into the positional encodings of a multihead self-attention (MHSA). This spontaneously inspires a solution, the multihead Self-Attention with Recurrence (RSA), to combine self-attention with RNN into one single operation while maintaining parallel computation. This solution enables our design to preserve the merits from both Transformer and recurrent models, while their respective shortcomings are avoided. More importantly, it can be used to replace the self-attention of existing networks, such as Transformer XL, TLB and BRT, to further explore recurrent dynamics at the finer scale. Our paper makes three main contributions below.\n- With ignorable approximation loss, we demonstrate that an RNN layer with linear activation is equivalent to a multihead self-attention (MHSA); see Figure 2. Specifically, each attention head can be used to recover a type of recurrent dependence structure, and multiple heads working in sync replicate the complex temporal dynamics of the RNN layer.\n- In the above MHSA, recurrent dynamics of the RNN layer are encapsulated entirely into the positional encodings, which we name the Recurrence Encoding Matrices (REMs). This makes it possible to add the REMs to any existing Transformer architecture that has the self-attention mechanism, leading to the Self-Attention with Recurrence (RSA) module; see Figure 1(d). A gated mechanism is used to control the proportion of REMs, and the gate value may also be interpreted as the proportion of the recurrent signals in the data.\n- Our experiments on four sequential tasks demonstrate that the proposed RSA module can effectively enhance the baseline Transformers\u2019 forecasting power. Moreover, from the fitted gate values, we have an interesting finding that time series data have stronger recurrent signals than those of the regular languages, while the code or natural languages have much weaker recurrent signals.", "output": "1. **Introduce Your Topic**:\n    - Your introduction does an admirable job of setting the stage for the topic at hand: the limitations of recurrent and Transformer models in sequential data modeling. The introduction is thorough and gives a nuanced view of why the research area is significant.\n\n2. **Opening Hook**:\n    - The opening sentence does an excellent job introducing the importance of the topic: the benchmark role recurrent networks like LSTM and GRU have played in sequential data modeling. It presents a clear problem space that will resonate with many readers familiar with machine learning, thereby serving as an effective hook.\n\n3. **Relevance and Importance**:\n    - The introduction successfully outlines the existing research landscape and the paper's contributions to it. It lists the limitations of both recurrent and Transformer models and then presents the paper's unique approach and its advantages."}, {"input": "In a world full of unstructured data, one can hardly underestimate the role of neural approaches to make sense of it all. Such models have been crucial in capturing semantic relatedness across various text units, from the Word2Vec methodology (Mikolov et al., 2013) to the more refined SBERT Reimers & Gurevych (2019). The emphasis of these systems has been on facilitating an array of semantic similarity tasks, greatly aided by dense retrieval frameworks like DPR (Karpukhin et al., 2020a). Furthermore, the deep representations of knowledge bases (KBs) have been instrumental in enhancing such functionalities as few-shot link prediction, analogical reasoning Pezeshkpour et al. (2018); Pahuja et al. (2021), entity linking Yu et al. (2020) or cross-lingual entity alignment Chen et al. (2018); Xu et al. (2019).\nThis paper is devoted to the exploration of learning cross-modal representations for English text and KB graphs, which not only enables us to exploit the capabilities of pre-existing models but also to effectively interact with text data. For this purpose, we consider KB graphs formatted in RDF (Resource Description Framework, Miller (1998)), which is a semantic web standard delineating graphs as sets of (subject, predicate, object) triples. Given a selection of aligned RDF-text data, our model is trained to generate fixed-length latent representations for both texts and RDF graphs, underlining their semantic similarities through proximity in vector space. The implications of this methodology extend to cross-modal retrieval and the formulation of a cross-modality similarity score suitable for evaluating RDF-to-text generation models.\nOne of the primary challenges in this line of work is the scarcity of parallel data. Our models are trained on multiple RDF-text datasets developed through distant supervision techniques. These datasets are either amalgamated or used independently for training. Subsequently, we evaluate the performance of the models on two benchmarks: WEBNLG, a dataset where texts are crafted to align with the graph semantically; and WIKICHUNKS, a less perfectly aligned dataset which mimics real-world Wikipedia retrieval scenarios. Our observations indicate marked disparities among these models, reflecting differences in dataset alignment quality. Additionally, we establish that our models significantly surpass a robust language-only baseline.\nTo quantitatively measure the output of RDF-to-text generation models, we utilize distance within embedding space. We compute correlations between the model-generated similarity scores for graph-text pairs and human assessments of semantic adequacy, based on ratings from the 2020 WEBNLG Challenge. After fine-tuning the model with data from the 2017 WEBNLG challenge and employing new categories of data augmentation during pre-training, our most proficient system shows superior or equivalent performance in correlation with human evaluations, compared to existing metrics that usually require human references for evaluation, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), BLEURT (Sellam et al., 2020b), METEOR (Banerjee & Lavie, 2005), or BERT-Score (Zhang* et al., 2020).\nIn summary, our work presents the following contributions:\n- We formulate a cross-modal RDF-text model for learning aligned (RDF graph, text) representations, which excels in cross-modal retrieval tasks and outshines state-of-the-art text-text retrieval models.\n- We propose a pioneering evaluation metric for RDF-to-text generation, which outperforms existing evaluation techniques in terms of correlation with human judgments of semantic adequacy, without necessitating an expensive human reference for comparison.", "output": "1. **Introduce Your Topic**:\n    - Your introduction starts by discussing the general importance of neural approaches in parsing unstructured data. You specifically highlight models like Word2Vec and SBERT to frame the broader context. The subsequent focus on the role of deep representations of knowledge bases (KBs) in tasks like few-shot link prediction and entity linking further refines this context. You finally narrow down to your specific subject matter, which is learning cross-modal representations between English text and KB graphs. This is a well-structured progression.\n\n2. **Opening Hook**:\n    - While your opening sentence, \"In a world full of unstructured data,\" establishes the domain of your work, it might fall short in immediately capturing the reader's attention. The concept of a world \"full of unstructured data\" is broad and doesn't necessarily differentiate your paper from others in a crowded field. Incorporating a specific and compelling statistic or question related to unstructured data could strengthen this section. For instance, a sentence mentioning the sheer volume of unstructured data generated per day and the challenges in making sense of it could provide an impactful hook.\n\n3. **Relevance and Importance**:\n    - The introduction effectively uses the opening line about a \"world full of unstructured data\" to hint at the topic's importance. It also provides a solid backdrop by discussing existing neural approaches. However, it falls short in clearly articulating the significance of focusing on \"learning cross-modal representations for English text and KB graphs.\" While the paper's contributions are listed, there's a missing sense of urgency or explanation as to why these specific contributions matter right now. For a reader not deeply embedded in this field, an explanation of the practical applications and consequences of being able to effectively \"interact with text data\" and \"RDF graphs\" might provide compelling context. For instance, how does this research impact real-world tasks like search engines, healthcare databases, or e-commerce recommendation systems?"}, {"input": "Generative image models conditioned on text now support high-fidelity, diverse and controllable image synthesis (Nichol et al., 2022; Ramesh et al., 2021; 2022; Saharia et al., 2022; 2021a; Yu et al., 2022; Saharia et al., 2021b). These quality improvements have come from large aligned image-text datasets (Schuhmann et al., 2022) and scalable generative model architectures. Diffusion models are particularly effective at learning high-quality image generators with a stable and scalable denoising objective (Ho et al., 2020; Sohl-Dickstein et al., 2015; Song et al., 2021). Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data (Chen et al., 2020; Ho et al., 2022; Kong et al., 2021). In this work, we develop techniques to transfer pretrained 2D image-text diffusion models to 3D object synthesis, without any 3D data (see Figure 1). Though 2D image generation is widely applicable, simulators and digital media like video games and movies demand thousands of detailed 3D assets to populate rich interactive environments. 3D assets are currently designed by hand in modeling software like Blender and Maya3D, a process requiring a great deal of time and expertise. Text-to-3D generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.\n3D generative models can be trained on explicit representations of structure like voxels (Wu et al., 2016; Chen et al., 2018) and point clouds (Yang et al., 2019; Cai et al., 2020; Zhou et al., 2021), but the 3D data needed is relatively scarce compared to plentiful 2D images. Our approach learns 3D structure using only a 2D diffusion model trained on images, and sidesteps this issue. GANs can learn controllable 3D generators from photographs of a single object category, by placing an adversarial loss on 2D image renderings of the output 3D object or scene (Henzler et al., 2019; Nguyen-Phuoc et al., 2019; Or-El et al., 2022). Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text.\nNeural Radiance Fields, or NeRF (Mildenhall et al., 2020) are an approach towards inverse rendering in which a volumetric raytracer is combined with a neural mapping from spatial coordinates to color and volumetric density. NeRF has become a critical tool for neural inverse rendering (Tewari et al., 2022). Originally, NeRF was found to work well for \u201cclassic\u201d 3D reconstruction tasks: many images of a scene are provided as input to a model, and a NeRF is optimized to recover the geometry of that specific scene, which allows for novel views of that scene from unobserved angles to be synthesized.\nMany 3D generative approaches have found success in incorporating NeRF-like models in the generative process (Schwarz et al., 2020; Chan et al., 2021b;a; Gu et al., 2021; Liu et al., 2022). One such approach is Dream Fields (Jain et al., 2022), which uses the frozen image-text joint embedding models from CLIP (Radford et al., 2021) and an optimization-based approach to train NeRFs. This work showed that pretrained 2D image-text models may be used for 3D synthesis, though 3D objects produced by this approach tend to lack realism and accuracy. CLIP has been used to guide other approaches based on voxel grids and meshes (Sanghi et al., 2022; Jetchev, 2021; Wang et al., 2022).\nWe adopt a similar approach to Dream Fields, but replace CLIP with a loss derived from distillation of a 2D diffusion model. Our loss is based on probabilty density distillation, minimizing the KL divergence between a family of Gaussian distribution with shared means based on the forward process of diffusion and the score functions learned by the pretrained diffusion model. The resulting Score Distillation Sampling (SDS) method enables sampling via optimization in differentiable image parameterizations. By combining SDS with a NeRF variant tailored to this 3D generation task, DreamFusion generates high-fidelity coherent 3D objects and scenes for a diverse set of user-provided text prompts.", "output": "1. **Introduce Your Topic**:\n    - Your introduction does an excellent job of setting the stage for the research. You not only mention the topic of generative image models but also indicate the specific problem you are addressing\u2014transferring pretrained 2D image-text diffusion models to 3D object synthesis. The introduction provides a rich backdrop of various technologies and techniques, such as GANs and Neural Radiance Fields (NeRF), that are relevant to the research.\n\n2. **Opening Hook**:\n    - The opening sentence is quite compelling, providing an overview of the state-of-the-art in image-text generative models and their capabilities. This establishes the relevance and timeliness of the topic, engaging the reader effectively.\n\n3. **Relevance and Importance**:\n    - The introduction efficiently tackles the question of why this work is essential. You touch upon practical applications, such as the requirement of 3D assets in simulators, video games, and movies. This brings a real-world relevance to the paper. Furthermore, you set your work apart by stating how it addresses the limitations of existing methods."}]}
{"aspect_id": 2, "aspect_guideline": "## Describe the background\n### Describe Background information\nAfter you\u2019ve caught your reader\u2019s attention, specify a bit more, providing context and narrowing down your topic.\n\nProvide only the most relevant background information. The introduction isn\u2019t the place to get too in-depth; if more background is essential to your paper, it can appear in the body.\n\n### Describe previous research\nFor a paper describing original research, you\u2019ll need to provide an overview of the most relevant research that has already been conducted. This is a sort of miniature literature review\u2014a sketch of the current state of research into your topic, boiled down to a few sentences.\n\nThis should be informed by genuine engagement with the literature. Your search can be less extensive than in a full literature review, but a clear sense of the relevant research is crucial to inform your own work.\n\nBegin by establishing the kinds of research that have been done, and end with limitations or gaps in the research that you intend to respond to.", "criteria": "1. **Describe the Background**: Does the paper successfully provide context and narrow down the topic by offering the most relevant background information without getting too in-depth?\n\n2. **Describe Previous Research**: Is there a well-constructed overview of relevant prior research that serves as a mini literature review, and does this overview inform the current work effectively?", "demonstrations": [{"input": "This work introduces Unified and Progressive Pruning (UPop), a novel approach for model compression in vision-language Transformers. UPop operates in a continuous optimization space to discover a multitude of multimodal subnets, thereby yielding efficient deployment in various applications.\nWhile there are various other compression techniques, this paper ventures forth with UPop as a novel and adaptive approach. We dispense with an extended discussion on background details and prior literature.\nReal-world tasks often require information to be processed from multiple modalities. A straightforward design for multimodal compression would involve the isolated compression of each modality, but this leads to inefficiencies. As a solution, UPop proposes a unified search strategy across different modalities and structures. We also find that a two-stage compression paradigm has limitations; hence, UPop integrates the search and retraining phases.\nOur contributions are as follows:\n- We introduce UPop, a new paradigm for compressing vision-language Transformers, which conducts its search in a continuous optimization space.\n- We present the idea of Unified Pruning, which allows for adaptive compression ratio assignment among different components.\n- We verify UPop\u2019s applicability and robustness across various tasks and datasets, including its extension to unimodal tasks like image classification.", "output": "1. **Describe the Background**:\n    - While your introduction successfully identifies the specific problem of model compression in vision-language Transformers and introduces your novel approach, Unified and Progressive Pruning (UPop), it lacks adequate contextualization. The paper fails to convey why this area of study is crucial, what real-world problems it addresses, or the implications of solving (or not solving) this issue. For readers unfamiliar with the topic, the introduction doesn't provide enough background to appreciate the significance of your work.\n\n2. **Describe Previous Research**:\n    - You make a passing mention of \"various other compression techniques,\" giving the reader a hint that your approach aims to be novel and perhaps more adaptive. However, this comes at the cost of neglecting a 'mini' literature review, a staple in scholarly introductions. The paper misses the opportunity to outline what existing solutions have been tried, their successes or shortcomings, and how your work either fits into or advances beyond current knowledge. The absence of this context fails to establish the gap your research aims to fill and the limitations of existing approaches, making it less clear why your work is necessary."}, {"input": "Self-supervised learning (SSL) of image representations has shown significant progress in the last few years (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Grill et al., 2020; Lee et al., 2021b; Caron et al., 2020; Zbontar et al., 2021; Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021b; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021), approaching, and sometime even surpassing, the performance of supervised baselines on many downstream tasks. Most recent approaches are based on the joint-embedding framework with a siamese network architecture (Bromley et al., 1994) which are divided into two main categories, contrastive and non-contrastive methods. Contrastive methods bring together embeddings of different views of the same image while pushing away the embeddings from different images. Non-contrastive methods also attract embeddings of views from the same image but remove the need for explicit negative pairs, either by architectural design (Grill et al., 2020; Chen & He, 2020) or by regularization of the covariance of the embeddings (Zbontar et al., 2021; Bardes et al., 2021; Li et al., 2022b).\nWhile contrastive and non-contrastive approaches seem very different and have been described as such (Zbontar et al., 2021; Bardes et al., 2021; Ermolov et al., 2021; Grill et al., 2020), we propose to take a closer look at the similarities between the two, both from a theoretical and empirical point of view and show that there exists a close relationship between them. We focus on covariance regularization-based non-contrastive methods (Zbontar et al., 2021; Ermolov et al., 2021; Bardes et al., 2021) and demonstrate that these methods can be seen as contrastive between the dimensions of the embeddings instead of contrastive between the samples. We, therefore, introduce the term dimension-contrastive methods which we believe is better suited for them, and refer to the original contrastive methods as sample-contrastive methods. To show the similarities between the two, we define contrastive and non-contrastive criteria based on the Frobenius norm of the Gram and covariance matrices of the embeddings, respectively, and show the equivalence between the two under assumptions on the normalization of the embeddings. We then relate popular methods to these criteria, highlighting the links between them and further motivating the use of the sample-contrastive and dimension-contrastive nomenclature. Finally, we introduce variations of an existing dimension-contrastive method (VICReg), and a sample-contrastive one (SimCLR). This allows us to verify this equivalence empirically and improve both VICReg and SimCLR through this lens.\nOur contributions can be summarized as follows:\n- We make a significant effort to unify several SOTA sample-contrastive and dimension-contrastive methods and show that empirical performance gaps can be closed completely. By pinpointing its source, we consolidate our understanding of SSL methods.\n- We introduce two criteria that serve as representatives for sample- and dimension-contrastive methods. We show that they are equivalent for doubly normalized embeddings, and then relate popular methods to them, highlighting their theoretical similarities.\n- We introduce methods that interpolate between VICReg and SimCLR to study the practical impact of precise components of their loss functions. This allows us to validate empirically our theoretical result by isolating the sample- and dimension-contrastive nature of methods.\n- Motivated by the equivalence, we show that advantages attributed to one family can be transferred to the other. We improve SimCLR\u2019s performance to match VICReg\u2019s, and improve VICReg to make it as robust to embedding dimension as SimCLR.", "output": "1. **Describe the Background**:\n    - Your introduction excels in laying out the background in the field of Self-supervised learning (SSL), providing a detailed sketch of the development and current state of contrastive and non-contrastive methods. Your citations create a temporal map, effectively narrowing down the topic and highlighting its current relevance. However, your introduction currently stops short of connecting these methods to real-world applications or problems they solve. A sentence or two about why SSL, and especially these methods, have a broader societal or industrial impact could greatly enhance the accessibility and appeal of your paper to a wider audience.\n\n2. **Describe Previous Research**:\n    - This introduction shines in its 'mini' literature review. The paper lists a series of references, from Chen et al., 2020a to Zhou et al., 2022b, indicating a deep and current understanding of the SSL landscape. Additionally, your classification of existing approaches into contrastive and non-contrastive methods provides a structured overview that helps inform the reader about the diverging paths within SSL. You go a step further by identifying specific techniques like joint-embedding frameworks and siamese network architecture, making it clear what types of research have been previously undertaken."}, {"input": "Speech editing has applications in a myriad of domains requiring nuanced vocal characteristics and elevated demands for speech naturalness, such as the realm of social media video production, gaming environments, and the film dubbing industry. Utilitarian voice editing utilities (Derry, 2012) enable various modulations like pitch bending, denoising, amplitude alterations, and elementary cut, copy, paste operations on waveforms. While competent, these tools exhibit certain limitations particularly when modifications to the existing audio transcript become necessary. In cases where unanticipated lexical elements surface, editors are relegated to the labor-intensive task of re-recording the particular voice clips and fusing them into the extant audio track. Such practices engender issues related to environmental noise disparities and inconsistencies in speaker attributes like volume, pitch, and rhythm, which culminate in a less than satisfactory auditory experience.\nIt is not an exaggeration to state that almost all research in the field has moved toward leveraging artificial intelligence and neural networks for optimizing and automating the process. Notably, digital signal processing techniques (Moulines & Charpentier, 1990; Morise et al., 2016; Kawahara, 2006) have been foundational, but innovations from Morrison et al. (2021) and Su et al. (2020) have illuminated new pathways in the neural network realm. They have focused on prosody prediction and sound quality enhancement techniques, respectively.\nYet another strand of scholarly endeavors has delved into Text-to-Speech (TTS) synthesis for filling the lexical gaps in the audio. Here, Jin et al. (2017), Tan et al. (2021), Wang et al. (2022), Bai et al. (2022), and Borsos et al. (2022) have offered intriguing insights and promising prototypes. Specifically, these researchers employed partial inference techniques, which\u2014although effective to a certain extent\u2014engender two categories of challenges:\n1. Challenges related to the inevitable artificiality and boundary discontinuity that result from grafting synthesized segments onto the original waveform.\n2. The failure to account for the potential shifts in tone and prosody when the corresponding transcript undergoes modification.\nThe manuscript at hand posits a cross-utterance conditioned coherent speech editing system that employs a variational autoencoder. By utilizing masked training, this system reconstructs the non-edited sections of the original audio, thus ameliorating many of the issues tied to partial inference approaches.\nFurthermore, the system optimizes resource utilization, as exemplified in Figure 1, which provides a graphical comparison of the efficiencies of the entire versus partial inference mechanisms. This is to say that our system, through the lens of Figure 1, appears as a more streamlined entity.\nAlso, to reconcile the produced audio with both the original characteristics and the post-edition semantic context, the variational autoencoder is designed to be context-sensitive, receiving inputs based on semantic understanding and original audio features.\nOur trials indicate a substantial improvement over traditional partial inference systems, showcasing greater audio fidelity and coherency, as substantiated by both subjective and objective metrics assessed over challenging datasets.\nThe organization of this paper reflects the typical structure observed in academic articles. We start by delineating the underpinnings of non-autoregressive TTS systems and masked training methodologies in Section 2. The subsequent section elucidates the proposed system\u2019s architecture. Sections 4, 5, and 6 successively outline the experimental setup, results, and conclusion.", "output": "1. **Describe the Background**:\n    - Your introduction commendably addresses the applications and relevance of speech editing, placing its importance firmly in sectors like social media video production, gaming, and the film dubbing industry. You aptly identify the limitations of existing utilitarian voice editing utilities, such as the need for re-recording and environmental noise disparities. These issues not only set up the problem your paper seeks to solve but also serve to narrow the focus of your topic effectively. However, the introduction seems to overindulge in detailing these limitations. For instance, the discussion about issues like \"environmental noise disparities\" and \"inconsistencies in speaker attributes\" could be simplified or moved to the main body of the paper. An overly detailed background can make the introduction seem dense and could potentially lose the reader's engagement.\n\n2. **Describe Previous Research**:\n    - You have undertaken a comprehensive literature review, covering a broad swath of methodologies from traditional digital signal processing techniques (citing Moulines & Charpentier, 1990; Morise et al., 2016; Kawahara, 2006) to newer neural network-based methods (highlighting Morrison et al., 2021; Su et al., 2020). This offers an in-depth look into the historical development and current state of the field, which is valuable. While thoroughness is usually a virtue, it would be beneficial to keep the introduction more concise. You could focus only on the seminal works or those most directly relevant to your research questions. Concluding this section by crisply identifying the limitations in previous studies that your research aims to overcome could sharpen your introduction's focus and make it more aligned with conventional academic writing guidelines."}, {"input": "We investigate the unreasonable effectiveness of stochastic gradient descent (SGD) algorithms on the high-dimensional non-convex optimization problems of deep learning. In particular,\n- 1. Why does SGD thrive in optimizing high-dimensional non-convex deep learning loss landscapes despite being noticeably less robust in other non-convex optimization settings, like policy learning (Ainsworth et al., 2021), trajectory optimization (Kelly, 2017), and recommender systems (Kang et al., 2016)?\n- 2. What are all the local minima? When linearly interpolating between initialization and final trained weights, why does the loss smoothly and monotonically decrease (Goodfellow & Vinyals, 2015; Frankle, 2020; Lucas et al., 2021; Vlaar & Frankle, 2021)?\n- 3. How can two independently trained models with different random initializations and data batch orders inevitably achieve nearly identical performance? Furthermore, why do their training loss curves often look identical?\nWe posit that these phenomena point to the existence of some yet uncharacterized invariance(s) in the training dynamics causing independent training runs to exhibit similar characteristics. HechtNielsen (1990) noted the permutation symmetries of hidden units in neural networks; briefly, one can swap any two units of a hidden layer in a network and \u2013 assuming weights are adjusted accordingly \u2013 network functionality will not change. Recently, Benton et al. (2021) demonstrated that SGD solutions form a connected volume of low loss and Entezari et al. (2021) conjectured that this volume is convex modulo permutation symmetries.\nConjecture 1 (Permutation invariance, informal (Entezari et al., 2021)). Most SGD solutions belong to a set whose elements can be permuted so that no barrier (as in Definition 2.2) exists on the linear interpolation between any two permuted elements.\nWe refer to such solutions as being linearly mode connected (LMC) (Frankle et al., 2020), an extension of mode connectivity (Garipov et al., 2018; Draxler et al., 2018). If true, Conjecture 1 will both materially expand our understanding of how SGD works in the context of deep learning and offer a credible explanation for the preceding phenomena, in particular.\nContributions. In this paper, we attempt to uncover what invariances may be responsible for the phenomena cited above and the unreasonable effectiveness of SGD in deep learning. We make the following contributions:\n- 1. Matching methods. We propose three algorithms, grounded in concepts and techniques from combinatorial optimization, to align the weights of two independently trained models. Where appropriate, we prove hardness results for these problems and propose approximation algorithms. Our fastest method identifies permutations in mere seconds on current hardware.\n- 2. Relationship to optimization algorithms. We demonstrate by means of counterexample that linear mode connectivity is an emergent property of training procedures, not of model architectures. We connect this result to prior work on the implicit biases of SGD.\n- 3. Experiments, including zero-barrier LMC for ResNets. Empirically, we explore the existence of linear mode connectivity modulo permutation symmetries in experiments across MLPs, CNNs, and ResNets trained on MNIST, CIFAR-10, and CIFAR-100. We contribute the first-ever demonstration of zero-barrier LMC between two independently trained ResNets. We explore the relationship between LMC and model width as well as training time. Finally, we show evidence of our methods\u2019 ability to combine models trained on independent datasets into a merged model that outperforms both input models in terms of test loss (but not accuracy) and is no more expensive in compute or memory than either input model.", "output": "1. **Describe the Background**:\n    - The introduction for this paper offers a clear and comprehensive outline of the problems being tackled, specifically the puzzling effectiveness of stochastic gradient descent (SGD) in high-dimensional, non-convex optimization problems commonly found in deep learning. By explicitly stating the key questions the paper seeks to answer, the authors do an excellent job of highlighting the gaps or limitations in existing knowledge. However, the paper could provide more context about why these questions are important. For example, why understanding SGD's effectiveness in deep learning is important or timely. Is this a roadblock in scaling machine learning models? Does it have implications for computational efficiency or even for our understanding of learning and intelligence? Such information would provide the uninitiated reader a sense of the real-world implications of your investigation.\n\n2. **Describe Previous Research**:\n    - The introduction does a commendable job of incorporating existing research to frame its own questions and hypotheses. It cites various studies across different applications and even includes previous conjectures. This provides a 'mini' literature review that does well to contextualize the current work. It both acknowledges the work that has been done and clearly points out what is yet to be understood, thereby identifying the research gap this paper aims to fill."}]}
{"aspect_id": 3, "aspect_guideline": "## Establish your research problem\nThe next step is to clarify how your own research fits in and what problem it addresses.\n\nIn an empirical research paper, try to lead into the problem on the basis of your discussion of the literature. Think in terms of these questions:\n\n- What research gap is your work intended to fill?\n- What limitations in previous work does it address?\n- What contribution to knowledge does it make?\n- You can make the connection between your problem and the existing research using phrases like the following.\n\n| Phrase | Connection |\n|------|-----------------------|\n| Although x has been studied in detail, insufficient attention has been paid to y. | You will address a previously overlooked aspect of your topic. |\n| The implications of x study deserve to be explored further. | You will build on something suggested by a previous study, exploring it in greater depth. |\n| It is generally assumed that x. However, this paper suggests that y \u2026 | You will depart from the consensus on your topic, establishing a new position. |", "criteria": "1. **Research Gap**: Assess if the paper explicitly identifies a gap in the existing research that it aims to fill. The identification should be based on a thorough discussion of existing literature.\n\n2. **Limitations in Previous Work**: Check whether the paper clearly points out the limitations or shortcomings in previous research that it intends to address. This should provide a rationale for the new research.\n\n3. **Contribution to Knowledge**: Evaluate if the paper clearly articulates what new insights or information it brings to the field. The contribution should be distinct and valuable in the context of existing research.", "demonstrations": [{"input": "Cross-Entropy (CE) objective is often utilized for deep neural networks (DNNs) in classification tasks. For this research, we present a mechanism termed \"Outputnorm\" that moderates certain outcomes in the use of CE objectives.\nWhile some may consider that different objectives and normalization methods exist in machine learning, this paper focuses on the Outputnorm layer without delving into the need for its creation.\nFirstly, the introduction of Outputnorm counters what we refer to as \"Model Output Blow-up effect.\" This phenomenon, which occurs during the use of CE objectives, can manifest as counterproductive growth in model output dimensions. Our approach of employing an Outputnorm layer can ameliorate such effects by altering the output logits prior to calculating the cross-entropy loss.\nOur contributions can be summarized as follows:\n- We introduce the Outputnorm layer as an element in machine learning.\n- We present a technical and analytical discourse on how Outputnorm operates.", "output": "1. **Research Gap**:\n    - The introduction falls short in articulating the research gap it aims to address. There is no explicit mention of how \"Outputnorm\" fills a void in the existing literature or what specifically necessitates its introduction. The absence of this element leaves readers guessing why this work is important and how it advances the field.\n\n2. **Limitations in Previous Work**:\n    - Your introduction does not discuss or cite any limitations in the current body of work that your research intends to remedy. The omission of this critical aspect makes it unclear why \"Outputnorm\" is a necessary or valuable addition to the realm of machine learning algorithms, especially with respect to the Cross-Entropy objective. Without this context, it is challenging to justify the importance of the research.\n\n3. **Contribution to Knowledge**:\n    - While you outline what you consider to be the \"key contributions\" of the paper, the introduction does not make it explicit how these contributions add to or modify existing knowledge. Specifically, it is unclear how introducing the \"Outputnorm layer\" moves the field forward or provides a unique solution to a problem that has been recognized but not yet addressed. The paper would benefit from a clearer articulation of its novel contributions in the context of existing research."}, {"input": "Dense prediction tasks constitute a fundamental class of computer vision problems, where the goal is to learn a mapping from an input image to a pixel-wise annotated label. Examples include semantic segmentation, depth estimation, edge detection, and keypoint detection, to name a few (Zamir et al., 2018; Cai & Pu, 2019). While supervised methods achieved remarkable progress, they require a substantial amount of manually annotated pixel-wise labels, leading to a massive and often prohibitive per-task labeling cost (Kang et al., 2019; Liu et al., 2020; Ouali et al., 2020). Prior work involving transfer and multi-task learning have made efforts to generally relieve the burden, but they often assume that relations between tasks are known in advance, and still require a fairly large amount of labeled images of the task of interest (e.g., thousands) (Zamir et al., 2018; Standley et al., 2020; O Pinheiro et al., 2020; Wang et al., 2021). This motivates us to seek a few-shot learning solution that can universally learn arbitrary dense prediction tasks from a few (e.g., ten) labeled images.\nHowever, existing few-shot learning methods for computer vision are specifically targeted to solve a restricted set of tasks, such as classification, object detection, and semantic segmentation (Vinyals et al., 2016; Kang et al., 2019; Min et al., 2021). As a result, they often exploit prior knowledge and assumptions specific to these tasks in designing model architecture and training procedure, therefore not suited for generalizing to arbitrary dense prediction tasks (Snell et al., 2017; Fan et al., 2022; Iqbal et al., 2022; Hong et al., 2022). To our knowledge, no prior work in few-shot learning provided approaches to solve arbitrary dense prediction tasks in a universal manner.\nWe argue that designing a universal few-shot learner for arbitrary dense prediction tasks must meet the following desiderata. First, the learner must have a unified architecture that can handle arbitrary tasks by design, and share most of the parameters across tasks so that it can acquire generalizable knowledge for few-shot learning of arbitrary unseen tasks. Second, the learner should flexibly adapt its prediction mechanism to solve diverse tasks of unseen semantics, while being efficient enough to prevent over-fitting. Designing such a learner is highly challenging, as it should be general and unified while being able to flexibly adapt to any unseen task without over-fitting few examples.\nIn this work, we propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. We draw inspiration from the cognitive process of analogy making (Mitchell, 2021); given a few examples of a new task, humans can quickly understand how to relate input and output based on a similarity between examples (i.e., assign similar outputs to similar inputs), while flexibly changing the notion of similarity to the given context. In VTM, we implement analogymaking for dense prediction as patch-level non-parametric matching, where the model learns the similarity in image patches that captures the similarity in label patches. Given a few labeled examples of a novel task, it first adapts its similarity that describes the given examples well, then predicts the labels of an unseen image by combining the label patches of the examples based on image patch similarity. Despite the simplicity, the model has a unified architecture for arbitrary dense prediction tasks since the matching algorithm encapsulates all tasks and label structures (e.g., continuous or discrete) by nature. Also, we introduce only a small amount of task-specific parameters, which makes our model robust to over-fitting as well as flexible.\nOur contributions are as follows. (1) For the first time to our knowledge, we propose and tackle the problem of universal few-shot learning of arbitrary dense prediction tasks. We formulate the problem as episodic meta-learning and identify two key desiderata of the learner \u2013 unified architecture and adaptation mechanism. (2) We propose Visual Token Matching (VTM), a novel universal fewshot learner for dense prediction tasks. It employs non-parametric matching on tokenized image and label embeddings, which flexibly adapts to unseen tasks using a tiny amount of task-specific parameters. (3) We implement VTM as a powerful hierarchical encoder-decoder architecture, where token matching is performed at multiple feature hierarchies using attention mechanism. We employ ViT image and label encoders (Dosovitskiy et al., 2020) and a convolutional decoder (Ranftl et al., 2021), which seamlessly works with our algorithm. (4) We demonstrate VTM on a challenging variant of Taskonomy dataset (Zamir et al., 2018) and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive to or outperforms fully supervised baselines given extremely few examples (0.1%), sometimes using only 10 labeled images (< 0.004%).", "output": "1. **Research Gap**:\n    - Your introduction does an excellent job in this aspect. You clearly identify a research gap by stating that existing few-shot learning methods are restricted to a specific set of tasks and are not universally applicable to arbitrary dense prediction tasks. This provides the reader with a clear understanding of what is missing in the current body of knowledge and why your work is needed.\n\n2. **Limitations in Previous Work**:\n    - The introduction discusses the limitations of existing few-shot learning methods that are specific to certain tasks and outlines the challenges they face in generalizing to arbitrary dense prediction tasks. You provide concrete citations to substantiate these points. This section successfully sets the stage for your research question and the need for a universal few-shot learner, making it clear why this research is essential.\n\n3. **Contribution to Knowledge**:\n    - You make your contributions explicit and tie them back to the identified research gap and limitations in prior work. Not only do you introduce a new learning approach, Visual Token Matching (VTM), but you also define the challenges it aims to overcome, providing the model's architecture details and its application. You make it clear how this work advances the field, adding not just a novel method but also a new problem formulation for universal few-shot learning."}, {"input": "It's widely known that in various machine learning endeavors, there is an amalgamation of data from numerous sources. This is especially evident in domains such as healthcare, where the convergence of data is imperative. This pooling together of data is beneficial for obvious reasons, such as broadening the scope and improving the comprehensiveness of the dataset. A point to note is that among the various sources, some may not be as reliable as others, presenting data discrepancies in terms of quality, noise, or even inaccuracies in labeling.\nIn our current world of machine learning, models are routinely trained with datasets from different origins. But the reliability of these data sources is not always taken into account. Herein, we introduce a method that bears semblance to the manner in which children discern information from a multitude of sources. The essence of our approach is the assertion that the plasticity of model weights can potentially hinge on the trustworthiness of data sources. To make this viable, our method operates by preserving a weight distribution over various data sources, which in turn influence gradient updates during back-propagation. It is worth mentioning that the scope of our work is predominantly centered on challenges arising from data corruption and label issues, putting aside, for now, any considerations regarding missing data.", "output": "1. **Research Gap**:\n    - The introduction points out that models in machine learning are often trained with datasets from various origins and suggests that the reliability of these sources is often overlooked. This statement starts to identify a research gap, but it could be far more compelling if backed by a sufficient context to argue that this issue is under-studied or under-addressed in existing research.\n\n2. **Limitations in Previous Work**:\n    - The introduction fails to identify limitations or shortcomings in previous research that the paper aims to address. While it mentions that models are routinely trained with datasets from different origins, it does not provide specific examples or citations to establish what issues or limitations currently exist in the field concerning the reliability of multi-source data. Thus, the absence of this element makes the research problem less compelling.\n\n3. **Contribution to Knowledge**:\n    - While the paper introduces a new method, it does not make it clear how this method advances the field or offers a solution to an identified problem. It lacks an explicit discussion on how this approach is different or more beneficial than existing techniques. As a result, the reader may find it difficult to ascertain the significance of this contribution to the existing body of knowledge."}, {"input": "Symmetries allow machine learning models to generalize properties of one data point to the properties of an entire class of data points. A model that captures translational symmetry, for example, will have the same output for an image and a version of the same image shifted a half pixel to the left or right. If a classification model produces dramatically different predictions as a result of translation by half a pixel or rotation by a few degrees it is likely misaligned with physical reality. Equivariance provides a formal notion of consistency under transformation. A function is equivariant if symmetries in the input space are preserved in the output space.\nBaking equivariance into models through architecture design has led to breakthrough performance across many data modalities, including images (Cohen & Welling, 2016; Veeling et al., 2018), proteins (Jumper et al., 2021) and atom force fields (Batzner et al., 2022; Frey et al., 2022). In computer vision, translation equivariance has historically been regarded as a particularly compelling property of convolutional neural networks (CNNs) (LeCun et al., 1995). Imposing equivariance restricts the size of the hypothesis space, reducing the complexity of the learning problem and improving generalization (Goodfellow et al., 2016).\nIn most neural networks classifiers, however, true equivariance has been challenging to achieve, and many works have shown that model outputs can change dramatically for small changes in the input space (Azulay & Weiss, 2018; Engstrom et al., 2018; Vasconcelos et al., 2021; Ribeiro & Sch\u00f6n, 2021). Several authors have significantly improved the equivariance properties of CNNs with architectural changes inspired by careful signal processing (Zhang, 2019; Karras et al., 2021), but non-architectural mechanisms for encouraging equivariance, such as data augmentations, continue to be necessary for good generalization performance (Wightman et al., 2021).\nThe increased prominence of non-convolutional architectures, such as vision transformers (ViTs) and mixer models, simultaneously demonstrates that explicitly encoding architectural biases for equivariance is not necessary for good generalization in image classification, as ViT models perform on-par with or better than their convolutional counterparts with sufficient data and well-chosen augmentations (Dosovitskiy et al., 2020; Tolstikhin et al., 2021). Given the success of large flexible architectures anddata augmentations, it is unclear what clear practical advantages are provided by explicit architectural constraints over learning equivariances from the data and augmentations. Resolving these questions systemically requires a unified equivariance metric and large-scale evaluation.\nIn what follows, we introduce the Lie derivative as a tool for measuring the equivariance of neural networks under continuous transformations. The local equivariance error (LEE), constructed with the Lie derivative, makes it possible to compare equivariance across models and to analyze the contribution of each layer of a model to its overall equivariance. Using LEE, we conduct a large-scale analysis of hundreds of image classification models. The breadth of this study allows us to uncover a novel connection between equivariance and model generalization, and the surprising result that ViTs are often more equivariant than their convolutional counterparts after training. To explain this result, we use the layer-wise decomposition of LEE to demonstrate how common building block layers shared across ViTs and CNNs, such as pointwise non-linearities, frequently give rise to aliasing and violations of equivariance.\nWe make our code publicly available at https://github.com/ngruver/lie-deriv.", "output": "1. **Research Gap**:\n    - This introduction does a great job in outlining the research gap. It mentions that while equivariance has been beneficial in various machine learning tasks, achieving true equivariance in neural network classifiers remains challenging. The paper proposes to explore this issue by introducing the Lie derivative as a tool for measuring equivariance in neural networks.\n\n2. **Limitations in Previous Work**:\n    - The introduction clearly identifies limitations in existing approaches. It mentions that despite efforts to improve the equivariance properties of CNNs through architectural changes, non-architectural mechanisms like data augmentations are still necessary for good performance. Additionally, it points out the contrast in the field concerning the need for explicitly encoding architectural biases for equivariance, especially when other models like ViTs perform similarly or better without such constraints. This provides a robust rationale for the study.\n\n3. **Contribution to Knowledge**:\n    - The paper outlines several key contributions, such as the introduction of a new metric (the local equivariance error, or LEE) for measuring equivariance and a large-scale analysis of image classification models. These contributions are contextualized well, indicating how they advance our understanding of equivariance in neural networks. The paper even suggests that it will uncover a novel connection between equivariance and model generalization, setting the stage for significant contributions to the field."}]}
{"aspect_id": 4, "aspect_guideline": "## Specify your objective(s)\nNow you\u2019ll get into the specifics of what you intend to find out or express in your research paper. An empirical paper generally poses a research question (sometimes with a hypothesis as to the answer).\n\n### Research question and hypothesis\nThe research question is the question you want to answer in an empirical research paper.\n\nPresent your research question clearly and directly, with a minimum of discussion at this point. The rest of the paper will be taken up with discussing and investigating this question; here you just need to express it.\n\nA research question can be framed either directly or indirectly.\n\nIf your research involved testing hypotheses, these should be stated along with your research question. They are usually presented in the past tense, since the hypothesis will already have been tested by the time you are writing up your paper.", "criteria": "1. **Specify Your Objectives**: Evaluate whether the paper clearly outlines the specific objectives it intends to achieve through the research. This includes the focus and the scope of what the paper aims to find out or express.\n\n2. **Research Question and/or Hypothesis**: Check if the paper includes a well-articulated research question or hypothesis. Assess the clarity and directness with which the research question or hypothesis is presented, and whether it is framed appropriately for the paper's empirical nature.", "demonstrations": [{"input": "Natural Language Processing (NLP) has made significant strides in the recent past, particularly in the realm of unsupervised sentence representation learning. Several renowned scholars have delved into this area, including Hill et al. (2016) and Le & Mikolov (2014), aiming to understand and enhance the effectiveness of these representations for various applications. There is no denying the importance of these representations, especially when we look at their relevance in scenarios where resources are limited, or the computational demand is high, such as zero-shot semantic similarity comparisons and large-scale document retrieval.\nBERT-based pre-trained language models (PLMs), as highlighted by Devlin et al. (2019), have emerged as powerful tools in this context. Yet, a closer look reveals certain intriguing patterns. The sentence representations derived from these BERT-based models, for instance, exhibit certain peculiar behaviors in vector space as noticed by Ethayarajh (2019). An attempt to navigate through this maze has been the introduction of contrastive learning, which is rooted in the idea of refining the sentence representations, as explored by Chen et al. (2020) and He et al. (2020). One might wonder about the underpinnings of this technique. It largely banks on leveraging various perturbations to create semantically related examples from original sentences.\nNow, the act of perturbing itself is an art, and here's where things get even more fascinating. While one could argue that heuristic methods dominate this space, there's an undercurrent of challenges associated with such methods. The issue is twofold: on one hand, these heuristic perturbations can be somewhat blind to the larger goals, potentially hampering optimal outcomes or inducing overfitting. On the other hand, PLMs, being intricately designed, have their sensitivities, and even minor perturbations can lead to significant changes in representations.\nIn light of the aforementioned observations, there exists a certain academic discourse around devising a contrastive learning framework equipped with a learnable perturbation strategy, which hypothetically could open doors to more nuanced and adaptive sentence representation learning. There are whispers of a weakening mechanism to mitigate token and feature representation changes, alongside a gradient-based algorithm to optimize these perturbations. One could say that this is an endeavor to refine the traditional approach of perturbation by dialing down on the magnitude of changes, ensuring a more holistic and informed augmentation process.\nEncapsulating these thoughts, we bring to the table a concept\u2014dubbed as L2P-CSR. This is essentially a canvas where learning meets perturbation, all woven together for contrastive learning of unsupervised sentence representations. There are layers to this framework: starting with the use of probability matrices to perturb the PLM, following up with an algorithmic adjustment based on intermediate sentence representations, and culminating in the augmentation of contrasting sentence representations. Preliminary findings suggest an edge over some of the existing models, particularly in semantic textual similarity tasks.", "output": "1. **Specify Your Objectives**:\n    - Your introduction provides an impressive and thorough overview of Natural Language Processing (NLP), particularly in the context of unsupervised sentence representation learning. It covers seminal works like Hill et al. (2016) and Le & Mikolov (2014), and moves into the advancements brought by BERT-based models, as cited from Devlin et al. (2019). However, your specific objectives get lost amidst this comprehensive background. For instance, it's only towards the end that the reader learns about \"L2P-CSR,\" which seems to be the primary focus of your research. Given the level of detail and nuance you've presented, a more direct and explicit articulation of your objectives\u2014perhaps right before or after introducing the concept of \"L2P-CSR\"\u2014would significantly enhance the reader's comprehension of your paper's primary goals.\n\n2. **Research Question and/or Hypothesis**:\n    - While you've adeptly highlighted intriguing aspects in the field, such as the peculiar behaviors in vector space of BERT-based models cited from Ethayarajh (2019), and the challenges surrounding heuristic methods of perturbation, you haven't translated these insights into a clear research question or hypothesis. These observations indicate gaps or challenges in the field, and they naturally lead the reader to expect a focused question or hypothesis that your research aims to address. However, this is missing. Given that the paper is empirical in nature, directly stating a research question\u2014for example, \"How does a learnable perturbation strategy within a contrastive learning framework affect the quality of sentence representations?\"\u2014or a specific hypothesis\u2014\"We hypothesize that implementing a learnable perturbation strategy will result in more adaptive and nuanced sentence representations\"\u2014would provide a structured path for the reader to follow through the remainder of your paper."}, {"input": "The expanding interest in the area of reward learning stems from the concern that it is difficult (or even impossible) to specify what we actually want AI agents to optimize, when it comes to increasingly complex, real-world tasks (Ziebart et al., 2009; Muelling et al., 2017). At the core of reward learning is the idea that human behavior serves as evidence about the underlying desired objective. Research on inferring rewards typically uses noisy-rationality as a model for human behavior: the human will take higher value actions with higher probability. It has enjoyed great success in a variety of reward inference applications (Ziebart et al., 2008; Vasquez et al., 2014; Wulfmeier et al., 2015), but researchers have also started to come up against its limitations (Reddy et al., 2018). This is not surprising, given decades of research in behavioral economics that has identified a deluge of systematic biases people have when making decisions on how to act, like myopia/hyperbolic discounting (Gr\u00fcne-Yanoff, 2015), optimism bias (Sharot et al., 2007), prospect theory (Kahneman & Tversky, 2013), and many more (Thompson, 1999; Do et al., 2008). Hence, the noisy-rationality model has become a complication in many reward learning tasks AI researchers are interested in. For instance, in shared autonomy (Javdani et al., 2015), a human operating a robotic arm may behave suboptimally due to being unfamiliar with the control interface or the robot\u2019s dynamics, leading to the robot inferring the wrong goal (Reddy et al., 2014; Chan et al., 2021).\nRecent work in reward learning attempts to go beyond noisy rationality and consider more accurate models of human behavior, by for instance looking at biases as variations on the Bellman update (Chan et al., 2021), modeling the human\u2019s false beliefs (Reddy et al., 2018), or learning their suboptimal perception process (Reddy et al., 2020). And while we might be getting closer, we will realistically never have a perfect model of human behavior. This raises an obvious question: Does the human model need to be perfect in order for reward inference to be successful? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, especially as it applies to value alignment: we will never have perfect models, and we will therefore never have guarantees that the agent does not do something catastrophically bad with respect to what people actually value. On the other hand, if we can show that as our models improve, we have a guarantee that reward accuracy also improves, then there is hope: though modeling human behavior is difficult, we know that improving such models will make AI agents more aligned with us.\nThe main goal of this work is to study whether we can bound the error in inferred reward parameters by some function of the distance between the assumed and true human model, specifically the KL divergence between the two models. We study this question both theoretically and empirically. Our first result is a negative answer: we show that given a finite dataset of demonstrations, it is possible to hypothesize a true human model, under which the dataset is most likely to be generated, that is \"close\" to the assumed model, but results in arbitrarily large error in the reward we would infer via maximum likelihood estimation (MLE). However, we argue that though this negative scenario can arise, it is unlikely to occur in practice. This is because the result relies on an adversarial construction of the human model, and though the dataset is most likely, it is not necessarily representative of datasets sampled by the model. Given this, our main result is thus a reason for hope: we identify mild assumptions on the true human behavior, under which we can actually bound the error in inferred reward parameters linearly by the error of the human model. Thus, if these assumptions hold, refining the human model will monotonically improve the accuracy of the learned reward. We also show how this bound simplifies for particular biases like false internal dynamics or myopia.\nEmpirically, we also show a similar, optimistic message about reward learning, using both diagnostic gridworld domains (Fu et al., 2019b), as well as the Lunar Lander game, which involves continuous control over a continuous state space. First, we verify that under various simulated biases, when the conditions on the human model are likely to be satisfied, small divergences in human models do not lead to large reward errors. Second, using real human demonstration data, we derive a natural human bias and demonstrate that the same finding holds even with real humans. Overall, our results suggest an optimistic perspective on the framework of reward learning, and that efforts in improving human models will further enhance the quality of the inferred rewards.", "output": "1. **Specify Your Objectives**:\n    - Your introduction does an excellent job of framing the current state of knowledge in reward learning, emphasizing the relevance and importance of modeling human behavior. The transition to the specific objectives of the paper is smooth and builds upon the foundation established in the early part of the introduction. The introduction specifies the study's main goal: to understand the link between errors in the human behavior model and errors in inferred reward parameters. The clarity in presenting this objective aids in setting the reader's expectations for the subsequent content.\n\n2. **Research Question and/or Hypothesis**:\n    - Your introduction is well-structured in presenting the research question: \"Does the human model need to be perfect in order for reward inference to be successful?\" This is further detailed with potential outcomes, allowing the reader to appreciate the significance and depth of the research question. The hypothesis is also provided, aiming to determine if the error in inferred reward parameters can be bounded by the distance between the assumed and true human model. The details on how you will study this question both theoretically and empirically, as well as the preliminary results, enhance the comprehensiveness of your introduction."}, {"input": "Machine learning models, trained within a centralized architecture, often require clients to share their data with a central server. This model has been criticized for issues including privacy concerns, limitations on mobile bandwidth, and the high cost of training on centralized servers. While Federated Learning (FL) has been proposed as a solution to these concerns, various challenges remain, such as non-IID data distribution, communication costs, and data privacy. The interpretability of federated learning algorithms also continues to be a topic of ongoing discourse.\nRecent applications in fields such as economics forecasting, activity recognition, and healthcare have used data collected from sensors, often referred to as multivariate time series (MTS). Different approaches exist for classifying MTS data, from distance-based algorithms to feature-based methods. Deep learning methods, particularly Convolutional Neural Networks (CNNs), have shown promise in this domain but lack transparency in their decision-making processes.\nIn light of these considerations, this paper explores the realm of federated learning in the context of multivariate time series. We introduce Federated LeArning Multivariate timE Series to Graph (FLAMES2Graph), an innovative framework aimed at addressing some of the noted challenges. This framework utilizes CNNs to isolate what we refer to as Multivariate Highly Activated Periods (MHAPs), which are then subjected to clustering. A graph-based representation, developed from the clustered MHAPs, is shared between clients and a central server.", "output": "1. **Specify Your Objectives**:\n    - The introduction gives a comprehensive overview of the field, presenting the problems and solutions associated with centralized learning, and how Federated Learning seeks to address these. The transition to multivariate time series (MTS) data and its relation to the federated learning context is also provided. However, the clear objectives of your study, which should ideally highlight what you aim to achieve with the FLAMES2Graph framework, are intermingled with other details. Making the objectives more explicit would provide the reader with a clearer roadmap of the paper's purpose.\n\n2. **Research Question and/or Hypothesis**:\n    - While the introduction alludes to various challenges and suggests that FLAMES2Graph addresses some of them, there is no direct formulation of a research question or hypothesis. The reader is left to infer the study's main inquiries from the provided information. Incorporating a straightforward question or hypothesis can help steer the readers' expectations and make the paper's purpose more evident."}, {"input": "Comparing the prediction performance of a deep neural network with the state-of-the-art (SOTA) performance is a common approach to validating advances brought by a new proposal in machine learning research. By definition, SOTA performance such as error or 1\u2212AUC1 monotonically decreases over time, but there is a fundamental limitation in the prediction performance that a machine learning model can achieve. Hence, it is important to figure out how close the current SOTA performance is to the underlying best performance achievable (Theisen et al., 2021). For example, Henighan et al. (2020) studied the scaling law (Kaplan et al., 2020) of Transformers (Vaswani et al., 2017) by distinguishing the reducible and irreducible errors.\nIn classification problems, one way to characterize the irreducible part is the Bayes error (Cover & Hart, 1967; Fukunaga, 1990), which is the best achievable expected error with any measurable function. The Bayes error will become zero in the special case where class distributions are completely separable, but in practice, we usually deal with complex distributions with some class overlap. Natural images tend to have a lower Bayes error (e.g., 0.21% test error on MNIST (LeCun et al., 1998) in Wan et al. (2013)), while medical images tend to have a higher Bayes error, since even medical technologists can disagree on the ground truth label (Sasada et al., 2018). If the current model\u2019s performance has already achieved the Bayes error, it is meaningless to aim for further error improvement. We may even find out that the model\u2019s error has exceeded the Bayes error. This may imply test set overfitting (Recht et al., 2018) is taking place. Knowing the Bayes error will be helpful to avoid such common pitfalls.\nEstimating the Bayes error has been a topic of interest in the research community (Fukunaga & Hostetler, 1975; Fukunaga, 1990; Devijver, 1985; Berisha et al., 2016; Noshad et al., 2019; Michelucci et al., 2021; Theisen et al., 2021). To the best of our knowledge, all previous papers have proposed ways to estimate the Bayes error from a dataset consisting of pairs of instances and their hard labels. When instances and hard labels are available, one can also train a supervised classifier, which is known to approach the Bayes classifier (that achieves the Bayes error) with sufficient training data provided that the model is correctly specified.\nThis is an interesting research problem from the point of view of Vapnik\u2019s principle2 (Vapnik, 2000) since we can derive the Bayes error from the Bayes classifier (and the underlying distribution) while we cannot recover the Bayes classifier from the knowledge of the Bayes error, which is just a scalar. How can we take full advantage of this property? While the Bayes error is usually defined as the best achievable expected error with any measurable function, it is known to be equivalent to the expectation of the minimum of class-posteriors with respect to classes for binary classification. Inspired by Vapnik\u2019s principle, our main idea is to skip the intermediate step of learning a function model, and we directly approximate the minimum of the class-posteriors by using soft labels (corresponding to the class probability) or uncertainty labels (corresponding to the class uncertainty). 3\nOur proposed method has two benefits. Firstly, our method is model-free. Since we do not learn a model, we can escape the curse of dimensionality, while dealing with high-dimensional instances would cause issues such as overfitting if we were to train a model. High dimensionality may cause performance deterioration for other Bayes error estimation methods (Berisha et al., 2016; Noshad et al., 2019) due to divergence estimation. We experimentally show how our method can more accurately estimate the Bayes error than baselines that utilize instances and soft labels. Our modelfree method is also extremely fast since we do not have any hyperparameters to tune nor a function model to train.\nThe second benefit is a more practical one: our method is completely instance-free. Suppose our final goal is to estimate the Bayes error instead of training a classifier. In that case, we do not need to collect instance-label pairs, and it may be less costly to collect soft/uncertainty labels without instances. Dealing with instances can cause privacy issues, and it can be expensive due to data storage costs especially when they are high-dimensional or can come in large quantities. It may lead to security costs to protect instances from a data breach. As an example of an instance-free scenario, we can consider doctors who are diagnosing patients by inspecting symptoms and asking questions, without explicitly collecting or storing the patients\u2019 data in the database. In this scenario, the hospital will only have the decisions and confidence of doctors, which can be used as soft labels.\nThe contributions of the paper is as follows. We first propose a direct way to estimate the Bayes error from soft (or uncertainty) labels without a model nor instances. We show that our estimator is unbiased and consistent. In practice, collecting soft/uncertainty labels can be difficult since the labelling process can become noisy. We propose a modified estimator that is still unbiased and consistent even when the soft labels are contaminated with zero-mean noise. We also show that our approach can be applied to other classification problems, such as weakly supervised learning (Sugiyama et al., 2022). Finally, we show the proposed methods\u2019 behavior through various experiments. Our results suggest that recently proposed deep networks such as the Vision Transformer (Dosovitskiy et al., 2021) has reached or is about to reach the Bayes error for benchmark datasets, such as CIFAR-10H (Peterson et al., 2019) and Fashion-MNIST-H (which is a new dataset we present; explained in Sec. 5.3). We also demonstrate how our proposed method can be used to estimate the Bayes error for academic conferences such as ICLR, by regarding them as an accept/reject binary classification problem.", "output": "1. **Specify Your Objectives**:\n    - This introduction is highly comprehensive and does an excellent job of setting the stage, but it struggles a bit in immediately conveying the primary objectives of the paper. You mention various aspects of Bayes error and its importance, but it takes some time to get to the crux of what the paper aims to achieve. While the objectives are ultimately presented, summarizing them earlier could provide the reader with a clearer roadmap.\n\n2. **Research Question and/or Hypothesis**:\n    - Your introduction outlines a rich context around the problem of Bayes error estimation, effectively setting the stage for a research question. You point to existing limitations in Bayes error estimation methods, as evidenced in the sentence, \"High dimensionality may cause performance deterioration for other Bayes error estimation methods.\" However, the introduction falls short of explicitly stating a research question or a hypothesis. Condensing the identified gaps and expected advantages of your method into a clear, direct research question would be beneficial. For example, you could explicitly ask, \"How can the Bayes error be more accurately estimated without relying on hard labels and without succumbing to issues like high dimensionality?\" Similarly, turning your implied hypothesis\u2014namely that your approach bypasses the curse of dimensionality and is quicker\u2014into an explicit statement would offer the reader a concrete idea of what you intend to prove or disprove through your research."}]}
{"aspect_id": 5, "aspect_guideline": "## Map out your paper\nThe final part of the introduction is often dedicated to a brief overview of the rest of the paper.\n\nIn a paper structured using the standard scientific \u201cintroduction, methods, results, discussion\u201d format, this isn\u2019t always necessary. But if your paper is structured in a less predictable way, it\u2019s important to describe the shape of it for the reader.\n\nIf included, the overview should be concise, direct, and written in the present tense.", "criteria": "1. **Map out your Paper**: Evaluate whether the introduction of the paper ends with a brief, clear, and direct overview of the rest of the paper to guide the reader, and check if this overview is written in the present tense.", "demonstrations": [{"input": "Event Causality Identification (ECI) seeks to identify the causal relation between two events in text. For example, as shown in Figure 1, in the sentence \u201cThe strong 6.1-magnitude quake left hundreds more injured ...\u201d, the ECI model should identify the causality between \u201cquake\u201d and \u201cinjured\u201d. ECI presents the causality structure of text, which is beneficial to a wide range of applications in natural language processing (NLP), including future event forecasting (Hashimoto, 2019), machine reading comprehension (Berant et al., 2014), and question answering (Oh et al., 2016).\nECI is composed of two parts: sentence-level ECI (SECI) (Liu et al., 2020; Zuo et al., 2021a) which aims to identify the intra-sentence event causality, and document-level ECI (DECI) (Gao et al., 2019) which aims to identify the inter-sentence event causality. One of the great challenges of ECI is how to identify the implicit causal relations, which are always expressed with multiple sentences and without clear causal cues (Cao et al., 2021). To address this problem, recent studies construct event graphs and apply graph neural networks as the reasoning module to infer the implicit event causality (Tran Phu & Nguyen, 2021; Chen et al., 2022). For example, ERGO (Chen et al., 2022), the recent state-of-the-art (SOTA) method, uses a graph transformer to enable event interaction, and reasons on an event relational graph (ERG), where each node represents an event pair and contains its relational information. There is an edge between two nodes only if they share one event.\nAlthough recently proposed reasoning models have achieved some success in ECI, their reasoning process is unnatural, as they reason intra- and inter-sentence event causality in the same time. We observe that most intra-sentence event causality is easy to identify with explicit causality cues, while inter-sentence event causality is more implicit and needs to be inferred from intra-sentence event causality. Take Figure 1 as an example, the causality of intra-sentence event pair \u201c(quake, injured)\u201d could be identified easily with the causality indicator \u201cleft\u201d. Based on the intra-sentence event causality and coreference relation \u201c(quake, earthquake)\u201d, we can propagate the causality via the coreference chain and infer that the event pair \u201c(earthquake, injured)\u201d also has causality.\nIn addition to the unnatural reasoning process, the graph neural networks used as the reasoning model also need improvement. For example, ERGO (Chen et al., 2022) simply aggregates the representations of neighborhood nodes (i.e., event pairs), ignoring the reasoning chains among these neighbors. In Figure 1, when the node of \u201c(earthquake, injured)\u201d is the target node to be reasoned, its two neighbors form a premise node pair if they contain the same event that the target node does not contain, e.g., nodes of \u201c(quake, injured)\u201d and \u201c(quake, earthquake)\u201d. Then the causality of the target node could be reasoned via the following reasoning chain: Cause(quake, injured) \u2227 Coreference(earthquake, quake) \u2192 Cause(earthquake, injured). The reasoning model should regard the premise node pair as a whole part to aggregate neighbors at a reasoning chain level.\nTo address the two problems mentioned above, we propose a novel Progressive Graph Pairwise Attention Network (PPAT) for reasoning event causality on the Sentence boundary Event Relational Graph (SERG). Same as ERG, each node of SERG denotes an event pair, and two nodes that share one event have two directed edges connecting with each other. Specially, the intra-sentence nodes only connect with the intra-sentence nodes in SERG. Figure 1 shows an example. The intrasentence node (in blue) does not have edges directed from inter-sentence nodes (in green), while inter-sentence nodes can aggregate information from the intra-sentence node via directed edges. Two basic ideas of SERG are: (i) only two nodes that share an event can have direct influence on each other. (ii) intra-sentence causality reasoning does not depend on inter-sentence nodes.\nPPAT provides effective global reasoning upon three aspects: (1) Progressive reasoning strategy: PPAT reasons progressively on SERG, as it first predicts the intra-sentence causality, and then reasons the inter-sentence causality based on the previous SECI prediction. The progressive reasoning strategy takes the dependence of inter-sentence causality on intra-sentence causality into consideration. (2) Pairwise attention: PPAT applies a novel graph pairwise attention network, which aggregates neighbors at a reasoning chain level instead of node level. A reasoning chain corresponds to a premise node pair. Pairwise attention can introduce interaction between the target node and its premise node pairs, thus attending to the possible reasoning chains and inferring the target causality. (3) Causality-guided training strategy: Since node representations on each layer of PPAT will be served as auxiliary information for reasoning on the next layer, it is important for every layer of PPAT to learn causality-related node representations, so we apply an additional loss to provide causality supervision on every layer and assist PPAT to have better reasoning performance.", "output": "1. **Map out your Paper**:\n    - Your introduction lacks a brief overview that maps out the rest of the paper, which would be useful given the complexity of the topic. Adding this in the present tense would guide the reader better."}, {"input": "Vision and language models (VLMs) have demonstrated high performance on dozens of wellestablished benchmarks (Radford et al., 2021; Li et al., 2022; Singh et al., 2022; Alayrac et al., 2022; Wang et al., 2022a;b; Zhai et al., 2022). Yet it is unclear whether performance on these benchmarks indicates rich compositional understanding of either text or images. For example, does CLIP distinguish between \u201cthe horse is eating the grass\u201d and \u201cthe grass is eating the horse\u201d? Natural scenes are complex, composed of many objects and attributes, in relationships with one another. While there have been important efforts to test compositional representations of objects, attributes, and relations (Thrush et al., 2022), such efforts are based on small sets of hand-crafted examples, often combined with testing many other types of knowledge. This makes it hard to evaluate the role of relational and attributional knowledge in isolation and lacks the statistical power to quantify how well VLMs perform on granular subtypes of compositions. Here, we provide a large-scale test bed to evaluate VLMs\u2019 attribution, relation, and order understanding. Using the test bed we create, we find significant deficiencies: many models fail to perform beyond chance level at simple tasks requiring compositional understanding.\nMany VLMs are pretrained and tested on large datasets with complex scenes and detailed captions with rich compositional structure. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. In the recent literature, the dominant VLM training paradigm is image-text contrastive pretraining (Jia et al., 2021; Radford et al., 2021; Zhang et al., 2020) over these large pretraining datasets. Contrastive pretraining optimizes for the task of image-text retrieval, and naturally many VLMs are tested in the retrieval task. In this work, we provide an analysis of retrieval, as an evaluation and objective. We propose experiments to analyze how these models are evaluated and trained, to understand the underlying issues.\nOur main contributions are three-fold:\n- 1. Introducing the Attribution, Relation, and Order benchmark (ARO) for fine-grained evaluation of VLMs\u2019 relation, attribution, and order understanding. We present four new tasks: Visual Genome Attributions and Visual Genome Relations, to test the understanding of objects\u2019 attributes and relations in complex natural scenes; and COCO Order and Flickr30k Order, to test the models\u2019 ability to identify the correct ordering of the words in a caption (Section 2). Using these evaluations, we show that state-of-the-art VLMs fail to represent simple relations such as \u201cto the right of\u201d and \u201cbehind\u201d, fail to represent the attributive difference between \u201cthe black jacket and the blue sky\u201d versus \u201cthe blue jacket and the black sky\u201d, and fail to represent the difference between correct and permuted captions. We provide fine-grained insights into the types of attributions and relations that models most frequently fail to understand.\n- 2. A critique of retrieval and contrastive pretraining. Given we find VLMs exhibit poor compositional understanding, why have these issues not surfaced in many previous evaluations? Existing retrieval datasets are equipped with complex scenes and detailed descriptions as captions, typically full of rich compositional structure. Intriguingly, the models can perform well on retrieval without having a good compositional understanding. Our experiments (Section 3) show that models can achieve a high performance on retrieval even when the order and composition cues are removed from captions or images. Hence, it is natural that models with compositional deficiencies can still perform well on the standard evaluations. This suggests that standard retrieval tasks are limited in their ability to assess compositional understanding of the model, further motivating the need for our comprehensive ARO benchmark. Since contrastive pretraining optimizes for retrieval, our findings also show that models can perform well on contrastive pretraining without learning compositional information. Given our results, we argue that not learning the compositional information is a valid shortcut strategy (Geirhos et al., 2020), and VLMs have little incentive to learn to encode compositionality during contrastive pretraining.\n- 3. Composition-aware hard negatives can go a long way. We propose a simple fix: mining of composition-aware hard negatives (Section 4). First, we introduce hard negatives consisting of the nearest neighboring images into each batch, to force models to represent fine-grained differences between very similar scenes. Second, we introduce hard negative captions into each batch, consisting of the true captions with word order perturbed, to force models to distinguish between correct and incorrect order. Finally, we show that this simple finetuning modification provides significant improvements in model understanding of attributes and relations.", "output": "1. **Map out your Paper**:\n    - The introduction effectively maps out the rest of the paper. The authors briefly explain the problem and then present the main contributions they will cover, all in the present tense. This layout provides a clear guide for the reader on what to expect."}, {"input": "Recent advancements in large-scale representation learning have significantly improved downstream performance on virtually every modality, from images Aghajanyan et al. (2022); Bao et al. (2021); He et al. (2022) to text Liu et al. (2019); Lewis et al. (2020) to speech Conneau et al. (2020); Radford et al.. Meanwhile, the domain of molecular representations has only seen a fraction of the scale of these other domains. In this paper, we show that pre-training with masked language modeling objective on a large collection of molecules allows us to learn generalizable representations that perform well on a wide range of computational chemistry benchmarks.\nWe train MolBART, a general purpose, pre-trained generative masked language model for molecules trained with an order of magnitude more compute than previous pre-trained molecular representations over all available molecules (over 1.7 billion molecules).\nTo validate the efficacy of our representations, we do in-depth evaluations across a large amount of classification, regression, and generation tasks achieving a new state of the art on ten tasks. We then explore the representations from a domain-expert perspective by comparing a series of attribution interpretability methods with structural alerts, a rule-based system developed by chemists to explain specific molecular properties. In particular, for a model trained to predict the results of a commonly used toxicity test, the Integrated Gradients method usually highlights atoms known to be part of the substructures responsible for toxicity.\nFurthermore, our pre-trained model, in a completely unsupervised fashion, learned representations for specific tasks, i.e., certain neurons correlated highly with downstream tasks. For specific tasks, we see that a linear combination of frozen ten neurons is enough to reach 90% of the full fine-tuning performance.", "output": "1. **Map out your Paper**:\n    - The introduction falls short in providing a brief, direct overview that maps out the rest of the paper. Given the complexity of the topic and the multiple areas the paper covers (training of MolBART, in-depth evaluations, etc.), an explicit roadmap would help guide the reader. Such an overview should ideally be written in the present tense."}, {"input": "Recent advances in camera-only 3D detection have alleviated monocular ambiguities by leveraging a short history. Despite their improvements, these outdoor works neglect the majority of past observations, limiting their temporal fusion to a few frames in a short 2-3 second window. These long-term past observations are critical for better depth estimation, which has been demonstrated through oracle experiments (Wang et al., 2021b; Jing et al., 2022) as the main bottleneck of cameraonly pipelines due to their lack of explicit depth measurements.\nAlthough existing methods aggregate temporal features differently, in essence, these works all consider regions in 3D space and consider image features corresponding to these hypothesis locations from multiple timesteps. Then, they use this temporal information to determine the occupancy of or the existence of an object at those regions. As such, these works are instances of temporal stereo matching. To quantify the quality of multi-view (temporal) depth estimation possible in these methods, we define localization potential at a 3D location as the magnitude of the change in the source-view projection induced by a change in depth in the reference view. As shown in Figure 1, a larger localization potential causes depth hypotheses (Yao et al., 2018) for a reference view pixel to be projected further apart, giving them more distinct source view features. Then, the correct depth hypothesis with a stronger match with the source view feature can more easily suppress incorrect depth hypotheses with clearly unrelated features, allowing for more accurate depth estimation.\nWe evaluate the localization potential in driving scenarios and find that only using a few recent frames heavily limits the localization potential, and thus the depth estimation potential, of existing methods. Distinct from the intuition in both indoor works, which select frames with above a minimum translation and rotation (Hou et al., 2019; Sun et al., 2021), and outdoor works, which often empirically select a single historical frame (Huang & Huang, 2022; Wang et al., 2022c; Liu et al., 2022b), we find that the optimal rotation and temporal difference between the reference and source frame varies significantly over different pixels, depths, cameras, and ego-motion. Hence, it is necessary to utilize many timesteps over a long history for each pixel and depth to have access to a setup that maximizes its localization potential. Further, we find that localization potential is not only decreased by fewer timesteps but is also hurt by the lower image feature resolution used in existing methods. Both factors significantly hinder the benefits of temporal fusion in prior works.\nWe verify our theoretical analysis by designing a model that naturally follows from our findings. Although existing methods\u2019 usage of low-resolution image feature maps for multi-view stereo limits matching quality, our proposed long-term temporal fusion\u2019s dramatic increase in localization potential can offset this limitation. Our model adopts the coarse but efficient low-resolution feature maps and leverages a 16-frame BEV cost volume. We find that such a framework already outperforms prior-arts, highlighting the significant gap in utilizing temporal information in existing literature. We extend our model by further exploiting short-term temporal fusion with an efficient sampling module, replacing monocular depth priors in the 16-frame BEV cost volume with a two-view depth prior. This time offsetting the temporal decrease in localization potential with an increase in feature map resolution, we observe a further boost in performance, demonstrating that short-term and long-term temporal fusion are highly complementary. Our main contributions are as follows:\n- We define localization potential to measure the ease of multi-view depth estimation and use it to theoretically and empirically demonstrate that the optimal rotation and temporal difference between reference and source cameras for multi-view stereo varies significantly over pixels and depths. This runs contrary to intuition in existing works that impose a minimum view change threshold or empirically search for a single past frame to fuse.\n- We verify our theoretical analysis by designing a model, SOLOFusion, that leverages both ShOrt-term, high-resolution and LOng-term, low-resolution temporal stereo for depth estimation. Critically, we are the first, to the best of our knowledge, to balance the impacts of spatial resolution and temporal difference on localization potential and use it to design an efficient but strong temporal multi-view 3D detector in the autonomous driving task.\n- Our framework significantly outperforms state-of-the-art methods in utilizing temporal information, demonstrating considerable improvement in mAP and mATE over a strong nontemporal baseline as shown in Figure 2. SOLOFusion achieves first on the nuScenes test set and outperforms previous best art by 5.2% mAP and 3.7% NDS on the validation set.", "output": "1. **Map out your Paper**:\n    - This introduction does a good job of outlining the paper's structure and content. The authors list their main contributions in a clear and organized manner, informing the reader of what to expect in the rest of the paper. The overview is in the present tense, aligning well with the provided guidelines."}]}
